
@book{aberson_applied_2019,
  title = {Applied {{Power Analysis}} for the {{Behavioral Sciences}}},
  shorttitle = {Applied {{Power Analysis}} for the {{Behavioral Sciences}}},
  author = {Aberson, Christopher L.},
  date = {2019-02-04},
  edition = {2},
  publisher = {{Routledge}},
  location = {{New York}},
  abstract = {Applied Power Analysis for the Behavioral Sciences is a practical "how-to" guide to conducting statistical power analyses for psychology and related fields. The book provides a guide to conducting analyses that is appropriate for researchers and students, including those with limited quantitative backgrounds. With practical use in mind, the text provides detailed coverage of topics such as how to estimate expected effect sizes and power analyses for complex designs. The topical coverage of the text, an applied approach, in-depth coverage of popular statistical procedures, and a focus on conducting analyses using R make the text a unique contribution to the power literature. To facilitate application and usability, the text includes ready-to-use R code developed for the text. An accompanying R package called pwr2ppl (available at https://github.com/chrisaberson/pwr2ppl) provides tools for conducting power analyses across each topic covered in the text.},
  isbn = {978-1-138-04456-2},
  langid = {english},
  pagetotal = {214},
  annotation = {00000}
}

@article{albers_credible_2018,
  title = {Credible {{Confidence}}: {{A Pragmatic View}} on the {{Frequentist}} vs {{Bayesian Debate}}},
  shorttitle = {Credible {{Confidence}}},
  author = {Albers, Casper J. and Kiers, Henk A. L. and van Ravenzwaaij, Don},
  date = {2018-08-24},
  journaltitle = {Collabra: Psychology},
  volume = {4},
  number = {1},
  pages = {31},
  publisher = {{The Regents of the University of California}},
  issn = {2474-7394},
  doi = {10.1525/collabra.149},
  url = {http://www.collabra.org/articles/10.1525/collabra.149/},
  urldate = {2020-07-07},
  abstract = {Article: Credible Confidence: A Pragmatic View on the Frequentist vs Bayesian Debate},
  issue = {1},
  langid = {english}
}

@article{albers_when_2018,
  title = {When Power Analyses Based on Pilot Data Are Biased: {{Inaccurate}} Effect Size Estimators and Follow-up Bias},
  shorttitle = {When Power Analyses Based on Pilot Data Are Biased},
  author = {Albers, Casper J. and Lakens, Daniël},
  date = {2018},
  journaltitle = {Journal of Experimental Social Psychology},
  shortjournal = {Journal of Experimental Social Psychology},
  volume = {74},
  pages = {187--195},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2017.09.004},
  url = {http://www.sciencedirect.com/science/article/pii/S002210311630230X},
  urldate = {2017-10-16},
  abstract = {When designing a study, the planned sample size is often based on power analyses. One way to choose an effect size for power analyses is by relying on pilot data. A-priori power analyses are only accurate when the effect size estimate is accurate. In this paper we highlight two sources of bias when performing a-priori power analyses for between-subject designs based on pilot data. First, we examine how the choice of the effect size index (η2, ω2 and ε2) affects the sample size and power of the main study. Based on our observations, we recommend against the use of η2 in a-priori power analyses. Second, we examine how the maximum sample size researchers are willing to collect in a main study (e.g. due to time or financial constraints) leads to overestimated effect size estimates in the studies that are performed. Determining the required sample size exclusively based on the effect size estimates from pilot data, and following up on pilot studies only when the sample size estimate for the main study is considered feasible, creates what we term follow-up bias. We explain how follow-up bias leads to underpowered main studies. Our simulations show that designing main studies based on effect sizes estimated from small pilot studies does not yield desired levels of power due to accuracy bias and follow-up bias, even when publication bias is not an issue. We urge researchers to consider alternative approaches to determining the sample size of their studies, and discuss several options.},
  keywords = {Effect size,Epsilon-squared,Eta-squared,Follow-up bias,Omega-squared,Power analysis}
}

@article{allison_power_1997,
  title = {Power and Money: {{Designing}} Statistically Powerful Studies While Minimizing Financial Costs},
  shorttitle = {Power and Money},
  author = {Allison, David B. and Allison, Ronald L. and Faith, Myles S. and Paultre, Furcy and Pi-Sunyer, F. Xavier},
  date = {1997},
  journaltitle = {Psychological Methods},
  volume = {2},
  number = {1},
  pages = {20--33},
  issn = {1939-1463(Electronic),1082-989X(Print)},
  doi = {10.1037/1082-989X.2.1.20},
  abstract = {Adequate statistical power is increasingly demanded in research designs. However, obtaining adequate research funding is increasingly difficult. This places researchers in a difficult position. In response, the authors advocate an approach to designing studies that considers statistical power and financial concerns simultaneously. Their purpose is twofold: (a) to introduce the general paradigm of cost optimization in the context of power analysis and (b) to present techniques for such optimization. Techniques are presented in the context of a randomized clinical trial. The authors consider (a) selecting optimal cutpoints for subject screening tests; (b) optimally allocating subjects to different treatment conditions; (c) choosing between obtaining more subjects or taking more replicate measurements; and (d) using prerandomization covariates. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Cost Containment,Experimentation,Experimenters,Statistical Power}
}

@article{anderson_addressing_2017,
  title = {Addressing the “{{Replication Crisis}}”: {{Using Original Studies}} to {{Design Replication Studies}} with {{Appropriate Statistical Power}}},
  shorttitle = {Addressing the “{{Replication Crisis}}”},
  author = {Anderson, Samantha F. and Maxwell, Scott E.},
  date = {2017-03-07},
  journaltitle = {Multivariate Behavioral Research},
  pages = {1--20},
  issn = {0027-3171, 1532-7906},
  doi = {10.1080/00273171.2017.1289361},
  url = {https://www.tandfonline.com/doi/full/10.1080/00273171.2017.1289361},
  urldate = {2017-05-01},
  langid = {english}
}

@incollection{anderson_group_2014,
  title = {Group {{Sequential Design}} in {{R}}},
  booktitle = {Clinical {{Trial Biostatistics}} and {{Biopharmaceutical Applications}}},
  author = {Anderson, Keaven M.},
  date = {2014},
  pages = {179--209},
  publisher = {{CRC Press}},
  location = {{New York}},
  isbn = {978-1-4822-1218-1}
}

@article{anderson_sample-size_2017,
  title = {Sample-Size Planning for More Accurate Statistical Power: {{A}} Method Adjusting Sample Effect Sizes for Publication Bias and Uncertainty},
  shorttitle = {Sample-Size Planning for More Accurate Statistical Power},
  author = {Anderson, Samantha F. and Kelley, Ken and Maxwell, Scott E.},
  date = {2017},
  journaltitle = {Psychological science},
  volume = {28},
  number = {11},
  pages = {1547--1562},
  publisher = {{Sage Publications Sage CA: Los Angeles, CA}},
  doi = {10.1177/0956797617723724}
}

@article{appelbaum_journal_2018,
  title = {Journal Article Reporting Standards for Quantitative Research in Psychology: {{The APA Publications}} and {{Communications Board}} Task Force Report.},
  shorttitle = {Journal Article Reporting Standards for Quantitative Research in Psychology},
  author = {Appelbaum, Mark and Cooper, Harris and Kline, Rex B. and Mayo-Wilson, Evan and Nezu, Arthur M. and Rao, Stephen M.},
  date = {2018-01-18},
  journaltitle = {American Psychologist},
  volume = {73},
  number = {1},
  pages = {3},
  publisher = {{US: American Psychological Association}},
  issn = {1935-990X},
  doi = {10.1037/amp0000191},
  url = {https://psycnet.apa.org/fulltext/2018-00750-002.pdf},
  urldate = {2020-07-25}
}

@article{armitage_repeated_1969,
  title = {Repeated Significance Tests on Accumulating Data},
  author = {Armitage, Peter and McPherson, C. K. and Rowe, B. C.},
  date = {1969},
  journaltitle = {Journal of the Royal Statistical Society: Series A (General)},
  volume = {132},
  number = {2},
  pages = {235--244},
  publisher = {{Wiley Online Library}}
}

@article{arslan_how_2019,
  title = {How to {{Automatically Document Data With}} the Codebook {{Package}} to {{Facilitate Data Reuse}}},
  author = {Arslan, Ruben C.},
  date = {2019-05-16},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  shortjournal = {Advances in Methods and Practices in Psychological Science},
  pages = {2515245919838783},
  issn = {2515-2459},
  doi = {10.1177/2515245919838783},
  url = {https://doi.org/10.1177/2515245919838783},
  urldate = {2019-05-22},
  abstract = {Data documentation in psychology lags behind not only many other disciplines, but also basic standards of usefulness. Psychological scientists often prefer to invest the time and effort that would be necessary to document existing data well in other duties, such as writing and collecting more data. Codebooks therefore tend to be unstandardized and stored in proprietary formats, and they are rarely properly indexed in search engines. This means that rich data sets are sometimes used only once—by their creators—and left to disappear into oblivion. Even if they can find an existing data set, researchers are unlikely to publish analyses based on it if they cannot be confident that they understand it well enough. My codebook package makes it easier to generate rich metadata in human- and machine-readable codebooks. It uses metadata from existing sources and automates some tedious tasks, such as documenting psychological scales and reliabilities, summarizing descriptive statistics, and identifying patterns of missingness. The codebook R package and Web app make it possible to generate a rich codebook in a few minutes and just three clicks. Over time, its use could lead to psychological data becoming findable, accessible, interoperable, and reusable, thereby reducing research waste and benefiting both its users and the scientific community as a whole.},
  langid = {english}
}

@book{babbage_reflections_1830,
  title = {Reflections on the {{Decline}} of {{Science}} in {{England}}: {{And}} on {{Some}} of {{Its Causes}}},
  shorttitle = {Reflections on the {{Decline}} of {{Science}} in {{England}}},
  author = {Babbage, Charles},
  date = {1830},
  publisher = {{B. Fellowes}},
  url = {http://archive.org/details/reflectionsonde00mollgoog},
  urldate = {2017-10-26},
  abstract = {Book digitized by Google and uploaded to the Internet Archive by user tpb.},
  editora = {{unknown library}},
  editoratype = {collaborator},
  langid = {english},
  pagetotal = {309}
}

@article{bacchetti_current_2010,
  title = {Current Sample Size Conventions: {{Flaws}}, Harms, and Alternatives},
  shorttitle = {Current Sample Size Conventions},
  author = {Bacchetti, Peter},
  date = {2010-03-22},
  journaltitle = {BMC Medicine},
  shortjournal = {BMC Medicine},
  volume = {8},
  number = {1},
  pages = {17},
  issn = {1741-7015},
  doi = {10.1186/1741-7015-8-17},
  url = {https://doi.org/10.1186/1741-7015-8-17},
  urldate = {2020-12-31},
  abstract = {The belief remains widespread that medical research studies must have statistical power of at least 80\% in order to be scientifically sound, and peer reviewers often question whether power is high enough.},
  keywords = {Current Convention,Inadequate Sample Size,Information Method,Marginal Return,Sample Size Planning}
}

@article{bacchetti_simple_2008,
  title = {Simple, {{Defensible Sample Sizes Based}} on {{Cost Efficiency}}},
  author = {Bacchetti, Peter and McCulloch, Charles E. and Segal, Mark R.},
  date = {2008},
  journaltitle = {Biometrics},
  volume = {64},
  number = {2},
  pages = {577--585},
  issn = {1541-0420},
  doi = {10.1111/j.1541-0420.2008.01004_1.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2008.01004_1.x},
  urldate = {2020-12-31},
  abstract = {The conventional approach of choosing sample size to provide 80\% or greater power ignores the cost implications of different sample size choices. Costs, however, are often impossible for investigators and funders to ignore in actual practice. Here, we propose and justify a new approach for choosing sample size based on cost efficiency, the ratio of a study's projected scientific and/or practical value to its total cost. By showing that a study's projected value exhibits diminishing marginal returns as a function of increasing sample size for a wide variety of definitions of study value, we are able to develop two simple choices that can be defended as more cost efficient than any larger sample size. The first is to choose the sample size that minimizes the average cost per subject. The second is to choose sample size to minimize total cost divided by the square root of sample size. This latter method is theoretically more justifiable for innovative studies, but also performs reasonably well and has some justification in other cases. For example, if projected study value is assumed to be proportional to power at a specific alternative and total cost is a linear function of sample size, then this approach is guaranteed either to produce more than 90\% power or to be more cost efficient than any sample size that does. These methods are easy to implement, based on reliable inputs, and well justified, so they should be regarded as acceptable alternatives to current conventional approaches.},
  langid = {english},
  keywords = {Innovation,Peer review,Power,Research funding,Study design},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1541-0420.2008.01004\_1.x}
}

@article{baguley_standardized_2009,
  title = {Standardized or Simple Effect Size: {{What}} Should Be Reported?},
  shorttitle = {Standardized or Simple Effect Size},
  author = {Baguley, Thom},
  date = {2009-08-01},
  journaltitle = {British Journal of Psychology},
  volume = {100},
  number = {3},
  pages = {603--617},
  issn = {2044-8295},
  doi = {10.1348/000712608X377117},
  url = {http://onlinelibrary.wiley.com/doi/10.1348/000712608X377117/abstract},
  urldate = {2018-02-13},
  abstract = {It is regarded as best practice for psychologists to report effect size when disseminating quantitative research findings. Reporting of effect size in the psychological literature is patchy – though this may be changing – and when reported it is far from clear that appropriate effect size statistics are employed. This paper considers the practice of reporting point estimates of standardized effect size and explores factors such as reliability, range restriction and differences in design that distort standardized effect size unless suitable corrections are employed. For most purposes simple (unstandardized) effect size is more robust and versatile than standardized effect size. Guidelines for deciding what effect size metric to use and how to report it are outlined. Foremost among these are: (i) a preference for simple effect size over standardized effect size, and (ii) the use of confidence intervals to indicate a plausible range of values the effect might take. Deciding on the appropriate effect size statistic to report always requires careful thought and should be influenced by the goals of the researcher, the context of the research and the potential needs of readers.},
  langid = {english}
}

@article{baguley_understanding_2004,
  title = {Understanding Statistical Power in the Context of Applied Research},
  author = {Baguley, Thom},
  date = {2004-03-01},
  journaltitle = {Applied Ergonomics},
  shortjournal = {Applied Ergonomics},
  volume = {35},
  number = {2},
  pages = {73--80},
  issn = {0003-6870},
  doi = {10.1016/j.apergo.2004.01.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0003687004000195},
  urldate = {2020-12-28},
  abstract = {Estimates of statistical power are widely used in applied research for purposes such as sample size calculations. This paper reviews the benefits of power and sample size estimation and considers several problems with the use of power calculations in applied research that result from misunderstandings or misapplications of statistical power. These problems include the use of retrospective power calculations and standardized measures of effect size. Methods of increasing the power of proposed research that do not involve merely increasing sample size (such as reduction in measurement error, increasing ‘dose’ of the independent variable and optimizing the design) are noted. It is concluded that applied researchers should consider a broader range of factors (other than sample size) that influence statistical power, and that the use of standardized measures of effect size should be avoided (except as intermediate stages in prospective power or sample size calculations).},
  langid = {english},
  keywords = {Applied research,Experimental design,Statistical power}
}

@article{bakker_recommendations_2020,
  title = {Recommendations in Pre-Registrations and Internal Review Board Proposals Promote Formal Power Analyses but Do Not Increase Sample Size},
  author = {Bakker, Marjan and Veldkamp, Coosje L. S. and van den Akker, Olmo R. and van Assen, Marcel A. L. M. and Crompvoets, Elise and Ong, How Hwee and Wicherts, Jelte M.},
  date = {2020-07-31},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {15},
  number = {7},
  pages = {e0236079},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0236079},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0236079},
  urldate = {2020-08-04},
  abstract = {In this preregistered study, we investigated whether the statistical power of a study is higher when researchers are asked to make a formal power analysis before collecting data. We compared the sample size descriptions from two sources: (i) a sample of pre-registrations created according to the guidelines for the Center for Open Science Preregistration Challenge (PCRs) and a sample of institutional review board (IRB) proposals from Tilburg School of Behavior and Social Sciences, which both include a recommendation to do a formal power analysis, and (ii) a sample of pre-registrations created according to the guidelines for Open Science Framework Standard Pre-Data Collection Registrations (SPRs) in which no guidance on sample size planning is given. We found that PCRs and IRBs (72\%) more often included sample size decisions based on power analyses than the SPRs (45\%). However, this did not result in larger planned sample sizes. The determined sample size of the PCRs and IRB proposals (Md = 90.50) was not higher than the determined sample size of the SPRs (Md = 126.00; W = 3389.5, p = 0.936). Typically, power analyses in the registrations were conducted with G*power, assuming a medium effect size, α = .05 and a power of .80. Only 20\% of the power analyses contained enough information to fully reproduce the results and only 62\% of these power analyses pertained to the main hypothesis test in the pre-registration. Therefore, we see ample room for improvements in the quality of the registrations and we offer several recommendations to do so.},
  langid = {english},
  keywords = {Analysis of variance,Computer software,Linear regression analysis,Metaanalysis,Open science,Psychology,Research ethics,Social sciences}
}

@book{bausell_power_2002,
  title = {Power {{Analysis}} for {{Experimental Research}}: {{A Practical Guide}} for the {{Biological}}, {{Medical}} and {{Social Sciences}}},
  shorttitle = {Power {{Analysis}} for {{Experimental Research}}},
  author = {Bausell, R. Barker and Li, Yu-Fang},
  date = {2002-09-19},
  edition = {1st edition},
  publisher = {{Cambridge University Press}},
  abstract = {Power analysis is an essential tool for determining whether a statistically significant result can be expected in a scientific experiment prior to the experiment being performed. Many funding agencies and institutional review boards now require power analyses to be carried out before they will approve experiments, particularly where they involve the use of human subjects. This comprehensive, yet accessible, book provides practising researchers with step-by-step instructions for conducting power/sample size analyses, assuming only basic prior knowledge of summary statistics and the normal distribution. It contains a unified approach to statistical power analysis, with numerous easy-to-use tables to guide the reader without the need for further calculations or statistical expertise. This will be an indispensable text for researchers and graduates in the medical and biological sciences needing to apply power analysis in the design of their experiments.},
  langid = {english},
  pagetotal = {376}
}

@book{bausell_power_2002-1,
  title = {Power Analysis for Experimental Research: A Practical Guide for the Biological, Medical and Social Sciences},
  shorttitle = {Power Analysis for Experimental Research},
  author = {Bausell, R. Barker and Li, Yu-Fang},
  date = {2002},
  publisher = {{Cambridge University Press}}
}

@article{bayarri_rejection_2016,
  title = {Rejection Odds and Rejection Ratios: {{A}} Proposal for Statistical Practice in Testing Hypotheses},
  shorttitle = {Rejection Odds and Rejection Ratios},
  author = {Bayarri, M.J. and Benjamin, Daniel J. and Berger, James O. and Sellke, Thomas M.},
  date = {2016-06},
  journaltitle = {Journal of Mathematical Psychology},
  volume = {72},
  pages = {90--103},
  issn = {00222496},
  doi = {10.1016/j.jmp.2015.12.007},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S002224961600002X},
  urldate = {2017-09-19},
  langid = {english},
  keywords = {Bayes factors,Bayesian,Frequentist,Odds}
}

@article{bem_feeling_2011,
  title = {Feeling the Future: Experimental Evidence for Anomalous Retroactive Influences on Cognition and Affect},
  shorttitle = {Feeling the Future},
  author = {Bem, Daryl J.},
  date = {2011-03},
  journaltitle = {Journal of Personality and Social Psychology},
  shortjournal = {J Pers Soc Psychol},
  volume = {100},
  number = {3},
  eprint = {21280961},
  eprinttype = {pmid},
  pages = {407--425},
  issn = {1939-1315},
  doi = {10.1037/a0021524},
  abstract = {The term psi denotes anomalous processes of information or energy transfer that are currently unexplained in terms of known physical or biological mechanisms. Two variants of psi are precognition (conscious cognitive awareness) and premonition (affective apprehension) of a future event that could not otherwise be anticipated through any known inferential process. Precognition and premonition are themselves special cases of a more general phenomenon: the anomalous retroactive influence of some future event on an individual's current responses, whether those responses are conscious or nonconscious, cognitive or affective. This article reports 9 experiments, involving more than 1,000 participants, that test for retroactive influence by "time-reversing" well-established psychological effects so that the individual's responses are obtained before the putatively causal stimulus events occur. Data are presented for 4 time-reversed effects: precognitive approach to erotic stimuli and precognitive avoidance of negative stimuli; retroactive priming; retroactive habituation; and retroactive facilitation of recall. The mean effect size (d) in psi performance across all 9 experiments was 0.22, and all but one of the experiments yielded statistically significant results. The individual-difference variable of stimulus seeking, a component of extraversion, was significantly correlated with psi performance in 5 of the experiments, with participants who scored above the midpoint on a scale of stimulus seeking achieving a mean effect size of 0.43. Skepticism about psi, issues of replication, and theories of psi are also discussed.},
  langid = {english},
  keywords = {Affect,Awareness,Boredom,Cognition,Erotica,Escape Reaction,Female,Habituation; Psychophysiologic,Humans,Male,Mental Recall,Parapsychology,Subliminal Stimulation,Time Factors}
}

@article{bem_feeling_2015,
  title = {Feeling the Future: {{A}} Meta-Analysis of 90 Experiments on the Anomalous Anticipation of Random Future Events},
  shorttitle = {Feeling the Future},
  author = {Bem, Daryl and Tressoldi, Patrizio and Rabeyron, Thomas and Duggan, Michael},
  date = {2015-10-30},
  journaltitle = {F1000Research},
  issn = {2046-1402},
  doi = {10.12688/f1000research.7177.1},
  url = {http://f1000research.com/articles/4-1188/v1},
  urldate = {2016-06-24},
  langid = {english}
}

@article{bem_must_2011,
  title = {Must Psychologists Change the Way They Analyze Their Data?},
  author = {Bem, Daryl J. and Utts, Jessica and Johnson, Wesley O.},
  date = {2011-10},
  journaltitle = {Journal of Personality and Social Psychology},
  shortjournal = {J Pers Soc Psychol},
  volume = {101},
  number = {4},
  eprint = {21928916},
  eprinttype = {pmid},
  pages = {716--719},
  issn = {1939-1315},
  doi = {10.1037/a0024777},
  abstract = {Wagenmakers, Wetzels, Borsboom, and van der Maas (2011) argued that psychologists should replace the familiar "frequentist" statistical analyses of their data with bayesian analyses. To illustrate their argument, they reanalyzed a set of psi experiments published recently in this journal by Bem (2011), maintaining that, contrary to his conclusion, his data do not yield evidence in favor of the psi hypothesis. We argue that they have incorrectly selected an unrealistic prior distribution for their analysis and that a bayesian analysis using a more reasonable distribution yields strong evidence in favor of the psi hypothesis. More generally, we argue that there are advantages to bayesian analyses that merit their increased use in the future. However, as Wagenmakers et al.'s analysis inadvertently revealed, they contain hidden traps that must be better understood before being more widely substituted for the familiar frequentist analyses currently employed by most research psychologists.},
  langid = {english},
  keywords = {Data Interpretation; Statistical,Humans,Psychology}
}

@book{berkeley_defence_1735,
  title = {A Defence of Free-Thinking in Mathematics, in Answer to a Pamphlet of {{Philalethes Cantabrigiensis}} Entitled {{Geometry No Friend}} to {{Infidelity}}. {{Also}} an Appendix Concerning Mr. {{Walton}}'s {{Vindication}} of the Principles of Fluxions against the Objections Contained in {{The}} Analyst. {{By}} the Author of {{The}} Minute Philosopher},
  author = {Berkeley, George},
  date = {1735},
  volume = {3},
  url = {https://www.maths.tcd.ie/pub/HistMath/People/Berkeley/Defence/Defence.html}
}

@book{berkeley_defence_1735-1,
  title = {A Defence of Free-Thinking in Mathematics, in Answer to a Pamphlet of {{Philalethes Cantabrigiensis}} Entitled {{Geometry No Friend}} to {{Infidelity}}. {{Also}} an Appendix Concerning Mr. {{Walton}}'s {{Vindication}} of the Principles of Fluxions against the Objections Contained in {{The}} Analyst. {{By}} the Author of {{The}} Minute Philosopher},
  author = {Berkeley, George},
  date = {1735},
  volume = {3},
  url = {https://www.maths.tcd.ie/pub/HistMath/People/Berkeley/Defence/Defence.html}
}

@article{bigby_understanding_2014,
  title = {Understanding and Evaluating Systematic Reviews and Meta-Analyses},
  author = {Bigby, Michael},
  date = {2014-03-01},
  journaltitle = {Indian Journal of Dermatology},
  volume = {59},
  number = {2},
  eprint = {24700930},
  eprinttype = {pmid},
  pages = {134},
  issn = {0019-5154},
  doi = {10.4103/0019-5154.127671},
  url = {http://www.e-ijd.org/article.asp?issn=0019-5154;year=2014;volume=59;issue=2;spage=134;epage=139;aulast=Bigby;type=0},
  urldate = {2019-07-13},
  abstract = {A systematic review is a summary of existing evidence that answers a specific clinical question, contains a thorough, unbiased search of the relevant literature, explicit criteria for assessing studies and structured presentation of the results. A systematic review that incorporates quantitative pooling of similar studies to produce an overall summary of treatment effects is a meta-analysis. A systematic review should have clear, focused clinical objectives containing four elements expressed through the acronym PICO ( \textbf{P}atient, group of \textbf{p}atients, or \textbf{p}roblem, an \textbf{I}ntervention, a \textbf{C}omparison intervention and specific \textbf{O} utcomes). Explicit and thorough search of the literature is a pre-requisite of any good systematic review. Reviews should have pre-defined explicit criteria for what studies would be included and the analysis should include only those studies that fit the inclusion criteria. The quality (risk of bias) of the primary studies should be critically appraised. Particularly the role of publication and language bias should be acknowledged and addressed by the review, whenever possible. Structured reporting of the results with quantitative pooling of the data must be attempted, whenever appropriate. The review should include interpretation of the data, including implications for clinical practice and further research. Overall, the current quality of reporting of systematic reviews remains highly variable.},
  langid = {english}
}

@incollection{blume_likelihood_2011,
  title = {Likelihood and Its {{Evidential Framework}}},
  booktitle = {Philosophy of {{Statistics}}},
  author = {Blume, Jeffrey D.},
  date = {2011},
  pages = {493--511},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-444-51862-0.50014-9},
  url = {http://linkinghub.elsevier.com/retrieve/pii/B9780444518620500149},
  urldate = {2018-05-18},
  isbn = {978-0-444-51862-0},
  langid = {english}
}

@book{borenstein_introduction_2009,
  title = {Introduction to Meta-Analysis},
  editor = {Borenstein, Michael},
  date = {2009},
  publisher = {{John Wiley \& Sons}},
  location = {{Chichester, U.K}},
  abstract = {This text provides a concise and clearly presented discussion of all the elements in a meta-analysis. It is illustrated with worked examples throughout, with visual explanations, using screenshots from Excel spreadsheets and computer programs such as Comprehensive Meta-Analysis (CMA) or Strata},
  isbn = {978-0-470-05724-7},
  pagetotal = {421},
  keywords = {Meta-analysis,Meta-Analysis as Topic}
}

@article{bosco_correlational_2015,
  title = {Correlational Effect Size Benchmarks},
  author = {Bosco, Frank A. and Aguinis, Herman and Singh, Kulraj and Field, James G. and Pierce, Charles A.},
  date = {2015-03},
  journaltitle = {The Journal of Applied Psychology},
  shortjournal = {J Appl Psychol},
  volume = {100},
  number = {2},
  eprint = {25314367},
  eprinttype = {pmid},
  pages = {431--449},
  issn = {1939-1854},
  doi = {10.1037/a0038047},
  abstract = {Effect size information is essential for the scientific enterprise and plays an increasingly central role in the scientific process. We extracted 147,328 correlations and developed a hierarchical taxonomy of variables reported in Journal of Applied Psychology and Personnel Psychology from 1980 to 2010 to produce empirical effect size benchmarks at the omnibus level, for 20 common research domains, and for an even finer grained level of generality. Results indicate that the usual interpretation and classification of effect sizes as small, medium, and large bear almost no resemblance to findings in the field, because distributions of effect sizes exhibit tertile partitions at values approximately one-half to one-third those intuited by Cohen (1988). Our results offer information that can be used for research planning and design purposes, such as producing better informed non-nil hypotheses and estimating statistical power and planning sample size accordingly. We also offer information useful for understanding the relative importance of the effect sizes found in a particular study in relationship to others and which research domains have advanced more or less, given that larger effect sizes indicate a better understanding of a phenomenon. Also, our study offers information about research domains for which the investigation of moderating effects may be more fruitful and provide information that is likely to facilitate the implementation of Bayesian analysis. Finally, our study offers information that practitioners can use to evaluate the relative effectiveness of various types of interventions.},
  langid = {english},
  keywords = {Behavioral Research,Benchmarking,Data Interpretation; Statistical,Humans}
}

@article{brown_errors_1983,
  title = {Errors, {{Types I}} and {{II}}},
  author = {Brown, George W.},
  date = {1983-06-01},
  journaltitle = {American Journal of Diseases of Children},
  shortjournal = {American Journal of Diseases of Children},
  volume = {137},
  number = {6},
  pages = {586--591},
  issn = {0002-922X},
  doi = {10.1001/archpedi.1983.02140320062014},
  url = {https://doi.org/10.1001/archpedi.1983.02140320062014},
  urldate = {2020-12-16},
  abstract = {• The practicing physician and the clinical investigator regularly confront therapeutic trials, diagnostic tests, and other hypothesis-testing situations. The clinical literature increasingly displays statistical notations and concepts related to decision making in medicine. For these reasons, the physician is obligated to have some familiarity with the principles behind the null hypothesis, Type I and II errors, statistical power, and related elements of hypothesis testing.(Am J Dis Child 1983;137:586-591)}
}

@article{brown_grim_2017,
  title = {The {{GRIM Test}}: {{A Simple Technique Detects Numerous Anomalies}} in the {{Reporting}} of {{Results}} in {{Psychology}}},
  shorttitle = {The {{GRIM Test}}},
  author = {Brown, Nicholas J. L. and Heathers, James A. J.},
  date = {2017-05-01},
  journaltitle = {Social Psychological and Personality Science},
  shortjournal = {Social Psychological and Personality Science},
  volume = {8},
  number = {4},
  pages = {363--369},
  issn = {1948-5506},
  doi = {10.1177/1948550616673876},
  url = {https://doi.org/10.1177/1948550616673876},
  urldate = {2019-07-13},
  abstract = {We present a simple mathematical technique that we call granularity-related inconsistency of means (GRIM) for verifying the summary statistics of research reports in psychology. This technique evaluates whether the reported means of integer data such as Likert-type scales are consistent with the given sample size and number of items. We tested this technique with a sample of 260 recent empirical articles in leading journals. Of the articles that we could test with the GRIM technique (N = 71), around half (N = 36) appeared to contain at least one inconsistent mean, and more than 20\% (N = 16) contained multiple such inconsistencies. We requested the data sets corresponding to 21 of these articles, receiving positive responses in 9 cases. We confirmed the presence of at least one reporting error in all cases, with three articles requiring extensive corrections. The implications for the reliability and replicability of empirical psychology are discussed.},
  langid = {english}
}

@article{brysbaert_how_2019,
  title = {How Many Participants Do We Have to Include in Properly Powered Experiments? {{A}} Tutorial of Power Analysis with Reference Tables},
  shorttitle = {How Many Participants Do We Have to Include in Properly Powered Experiments?},
  author = {Brysbaert, Marc},
  date = {2019-07-19},
  journaltitle = {Journal of Cognition},
  volume = {2},
  number = {1},
  pages = {16},
  issn = {2514-4820},
  doi = {10.5334/joc.72},
  url = {http://www.journalofcognition.org/articles/10.5334/joc.72/},
  urldate = {2019-07-19},
  abstract = {Article: How many participants do we have to include in properly powered experiments?  A tutorial of power analysis with reference tables},
  langid = {english}
}

@article{brysbaert_how_2019-1,
  title = {How Many Words Do We Read per Minute? {{A}} Review and Meta-Analysis of Reading Rate},
  shorttitle = {How Many Words Do We Read per Minute?},
  author = {Brysbaert, Marc},
  date = {2019},
  journaltitle = {Journal of Memory and Language},
  volume = {109},
  pages = {104047},
  publisher = {{Elsevier}}
}

@article{brysbaert_power_2018,
  title = {Power {{Analysis}} and {{Effect Size}} in {{Mixed Effects Models}}: {{A Tutorial}}},
  shorttitle = {Power {{Analysis}} and {{Effect Size}} in {{Mixed Effects Models}}},
  author = {Brysbaert, Marc and Stevens, Michaël},
  date = {2018-01-12},
  journaltitle = {Journal of Cognition},
  volume = {1},
  number = {1},
  issn = {2514-4820},
  doi = {10.5334/joc.10},
  url = {http://www.journalofcognition.org/article/10.5334/joc.10/},
  urldate = {2018-09-07},
  abstract = {Article: Power Analysis and Effect Size in Mixed Effects Models: A Tutorial},
  langid = {english},
  annotation = {00012}
}

@software{buchanan_mote_2017,
  title = {{{MOTE}}: {{Effect Size}} and {{Confidence Interval Calculator}}.},
  author = {Buchanan, Erin M. and Scofield, J and Valentine, K. D.},
  date = {2017},
  version = {0.0.0.9100.}
}

@article{button_power_2013,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  shorttitle = {Power Failure},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munafò, Marcus R.},
  date = {2013-04-10},
  journaltitle = {Nature Reviews Neuroscience},
  volume = {14},
  number = {5},
  pages = {365--376},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn3475},
  url = {http://www.nature.com/doifinder/10.1038/nrn3475},
  urldate = {2015-11-30}
}

@article{carter_correcting_2019,
  title = {Correcting for {{Bias}} in {{Psychology}}: {{A Comparison}} of {{Meta-Analytic Methods}}},
  shorttitle = {Correcting for {{Bias}} in {{Psychology}}},
  author = {Carter, Evan C. and Schönbrodt, Felix D. and Gervais, Will M. and Hilgard, Joseph},
  date = {2019-06-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  shortjournal = {Advances in Methods and Practices in Psychological Science},
  volume = {2},
  number = {2},
  pages = {115--144},
  issn = {2515-2459},
  doi = {10.1177/2515245919847196},
  url = {https://doi.org/10.1177/2515245919847196},
  urldate = {2019-07-11},
  abstract = {Publication bias and questionable research practices in primary research can lead to badly overestimated effects in meta-analysis. Methodologists have proposed a variety of statistical approaches to correct for such overestimation. However, it is not clear which methods work best for data typically seen in psychology. Here, we present a comprehensive simulation study in which we examined how some of the most promising meta-analytic methods perform on data that might realistically be produced by research in psychology. We simulated several levels of questionable research practices, publication bias, and heterogeneity, and used study sample sizes empirically derived from the literature. Our results clearly indicated that no single meta-analytic method consistently outperformed all the others. Therefore, we recommend that meta-analysts in psychology focus on sensitivity analyses—that is, report on a variety of methods, consider the conditions under which these methods fail (as indicated by simulation studies such as ours), and then report how conclusions might change depending on which conditions are most plausible. Moreover, given the dependence of meta-analytic methods on untestable assumptions, we strongly recommend that researchers in psychology continue their efforts to improve the primary literature and conduct large-scale, preregistered replications. We provide detailed results and simulation code at https://osf.io/rf3ys and interactive figures at http://www.shinyapps.org/apps/metaExplorer/.},
  langid = {english}
}

@article{cascio_open_1983,
  title = {Open a {{New Window}} in {{Rational Research Planning}}: {{Adjust Alpha}} to {{Maximize Statistical Power}}},
  shorttitle = {Open a {{New Window}} in {{Rational Research Planning}}},
  author = {Cascio, Wayne F. and Zedeck, Sheldon},
  date = {1983},
  journaltitle = {Personnel Psychology},
  volume = {36},
  number = {3},
  pages = {517--526},
  issn = {1744-6570},
  doi = {10.1111/j.1744-6570.1983.tb02233.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1744-6570.1983.tb02233.x},
  urldate = {2020-12-16},
  abstract = {Alternative strategies for optimizing statistical power in applied psychological research are considered. Increasing sample size and combining predictors in order to yield a useful effect size are well-known tactics for increasing power. A third approach, increasing alpha, is rarely used because of zealous adherence to convention. There are two related aspects in setting the alpha level. First, the relative seriousness of Type I and Type II errors must be considered. This assessment must then be qualified and redetermined after taking into account the prior probability that an effect exists. Procedures that make these processes objective are demonstrated. When sample size and effect size are both fixed, increasing alpha may be the only feasible strategy for maximizing power. It is concluded that a priori power analysis should be a major consideration in any test of an hypothesis, and that alpha level adjustment should be viewed as a useful strategy for increasing power.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1744-6570.1983.tb02233.x}
}

@book{chang_adaptive_2016,
  title = {Adaptive {{Design Theory}} and {{Implementation Using SAS}} and {{R}}},
  author = {Chang, Mark},
  date = {2016-10-14},
  edition = {2nd edition},
  publisher = {{Chapman and Hall/CRC}},
  abstract = {Get Up to Speed on Many Types of Adaptive Designs  Since the publication of the first edition, there have been remarkable advances in the methodology and application of adaptive trials. Incorporating many of these new developments, Adaptive Design Theory and Implementation Using SAS and R, Second Edition offers a detailed framework to understand the use of various adaptive design methods in clinical trials.  New to the Second Edition  Twelve new chapters covering blinded and semi-blinded sample size reestimation design, pick-the-winners design, biomarker-informed adaptive design, Bayesian designs, adaptive multiregional trial design, SAS and R for group sequential design, and much more More analytical methods for K-stage adaptive designs, multiple-endpoint adaptive design, survival modeling, and adaptive treatment switching New material on sequential parallel designs with rerandomization and the skeleton approach in adaptive dose-escalation trials Twenty new SAS macros and R functions Enhanced end-of-chapter problems that give readers hands-on practice addressing issues encountered in designing real-life adaptive trials  Covering even more adaptive designs, this book provides biostatisticians, clinical scientists, and regulatory reviewers with up-to-date details on this innovative area in pharmaceutical research and development. Practitioners will be able to improve the efficiency of their trial design, thereby reducing the time and cost of drug development.},
  isbn = {978-1-138-03423-5},
  langid = {english},
  pagetotal = {706}
}

@article{cho_is_2013,
  title = {Is Two-Tailed Testing for Directional Research Hypotheses Tests Legitimate?},
  author = {Cho, Hyun-Chul and Abe, Shuzo},
  date = {2013-09},
  journaltitle = {Journal of Business Research},
  shortjournal = {Journal of Business Research},
  series = {Advancing {{Research Methods}} in {{Marketing}}},
  volume = {66},
  number = {9},
  pages = {1261--1266},
  issn = {0148-2963},
  doi = {10.1016/j.jbusres.2012.02.023},
  url = {http://www.sciencedirect.com/science/article/pii/S0148296312000550},
  urldate = {2016-09-02},
  abstract = {This paper demonstrates that there is currently a widespread misuse of two-tailed testing for directional research hypotheses tests. One probable reason for this overuse of two-tailed testing is the seemingly valid beliefs that two-tailed testing is more conservative and safer than one-tailed testing. However, the authors examine the legitimacy of this notion and find it to be flawed. A second and more fundamental cause of the current problem is the pervasive oversight in making a clear distinction between the research hypothesis and the statistical hypothesis. Based upon the explicated, sound relationship between the research and statistical hypotheses, the authors propose a new scheme of hypothesis classification to facilitate and clarify the proper use of statistical hypothesis testing in empirical research.},
  keywords = {hypothesis testing,one-tailed testing,Research hypothesis in existential form,Research hypothesis in non-existential form,Statistical hypothesis,two-tailed testing}
}

@article{cohen_earth_1994,
  title = {The Earth Is Round (p {$<$} .05).},
  author = {Cohen, Jacob},
  date = {1994},
  journaltitle = {American Psychologist},
  volume = {49},
  number = {12},
  pages = {997--1003},
  issn = {1935-990X, 0003-066X},
  doi = {10.1037/0003-066X.49.12.997},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0003-066X.49.12.997},
  urldate = {2017-06-05},
  langid = {english}
}

@article{cohen_earth_1995,
  title = {The Earth Is Round ( p\hspace{0.6em}{$<$}\hspace{0.6em}.05): {{Rejoinder}}},
  shorttitle = {The Earth Is Round ( p\hspace{0.6em}{$<$}\hspace{0.6em}.05)},
  author = {Cohen, Jacob},
  date = {1995-12},
  journaltitle = {American Psychologist},
  volume = {50},
  number = {12},
  pages = {1103},
  issn = {0003-066X},
  doi = {http://dx.doi.org/10.1037/0003-066X.50.12.1103},
  url = {https://search.proquest.com/docview/614334790/abstract/11A04DC87A49418DPQ/1},
  urldate = {2018-03-26},
  langid = {english},
  keywords = {Null Hypothesis Testing (major)}
}

@article{cohen_statistical_1965,
  title = {Some Statistical Issues in Psychological Research},
  author = {Cohen, Jacob},
  date = {1965},
  journaltitle = {Handbook of clinical psychology},
  pages = {95--121}
}

@book{cohen_statistical_1988,
  title = {Statistical Power Analysis for the Behavioral Sciences},
  author = {Cohen, Jacob},
  date = {1988},
  edition = {2nd ed},
  publisher = {{L. Erlbaum Associates}},
  location = {{Hillsdale, N.J}},
  isbn = {978-0-8058-0283-2},
  pagetotal = {567},
  keywords = {Probabilities,Social sciences,Statistical methods,Statistical power analysis}
}

@article{cohen_things_1990,
  title = {Things {{I}} Have Learned (so Far)},
  author = {Cohen, Jacob},
  date = {1990},
  journaltitle = {American Psychologist},
  volume = {45},
  number = {12},
  pages = {1304--1312},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1935-990X(Electronic),0003-066X(Print)},
  doi = {10.1037/0003-066X.45.12.1304},
  abstract = {This is an account of what I have learned (so far) about the application of statistics to psychology and the other sociobiomedical sciences. It includes the principles "less is more" (fewer variables, more highly targeted issues, sharp rounding off), "simple is better" (graphic representation, unit weighting for linear composites), and "some things you learn aren't so." I have learned to avoid the many misconceptions that surround Fisherian null hypothesis testing. I have also learned the importance of power analysis and the determination of just how big (rather than how statistically significant) are the effects that we study. Finally, I have learned that there is no royal road to statistical induction, that the informed judgment of the investigator is the crucial element in the interpretation of data, and that things take time. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Psychology,Social Sciences,Statistics}
}

@article{colquhoun_reproducibility_2017,
  title = {The {{Reproducibility Of Research And The Misinterpretation Of P Values}}},
  author = {Colquhoun, David},
  date = {2017-08-07},
  journaltitle = {bioRxiv},
  pages = {144337},
  doi = {10.1101/144337},
  url = {http://www.biorxiv.org/content/early/2017/08/07/144337},
  urldate = {2017-08-19},
  abstract = {We wish to answer this question If you observe a "significant" P value after doing a single unbiased experiment, what is the probability that your result is a false positive?. The weak evidence provided by P values between 0.01 and 0.05 is explored by exact calculations of false positive risks. When you observe P = 0.05, the odds in favour of there being a real effect (given by the likelihood ratio) are about 3:1. This is far weaker evidence than the odds of 19 to 1 that might, wrongly, be inferred from the P value. And if you want to limit the false positive risk to 5 \%, you would have to assume that you were 87\% sure that there was a real effect before the experiment was done. If you observe P = 0.001 in a well-powered experiment, it gives a likelihood ratio of almost 100:1 odds on there being a real effect. That would usually be regarded as conclusive, But the false positive risk would still be 8\% if the prior probability of a real effect was only 0.1. And, in this case, if you wanted to achieve a false positive risk of 5\% you would need to observe P = 0.00045. It is recommended that the terms "significant" and "non-significant" should never be used. Rather, P values should be supplemented by specifying the prior probability that would be needed to produce a specified (e.g. 5\%) false positive risk. It may also be helpful to specify the minimum false positive risk associated with the observed P value. Despite decades of warnings, many areas of science still insist on labelling a result of P {$<$} 0.05 as "significant". This practice must account for a substantial part of the lack of reproducibility in some areas of science. And this is before you get to the many other well-known problems, like multiple comparisons, lack of randomisation and P-hacking. Science is endangered by statistical misunderstanding, and by university presidents and research funders who impose perverse incentives on scientists.},
  langid = {english}
}

@article{cook_assessing_2014,
  title = {Assessing Methods to Specify the Target Difference for a Randomised Controlled Trial: {{DELTA}} ({{Difference ELicitation}} in {{TriAls}}) Review},
  shorttitle = {Assessing Methods to Specify the Target Difference for a Randomised Controlled Trial},
  author = {Cook, Jonathan and Hislop, Jennifer and Adewuyi, Temitope and Harrild, Kirsten and Altman, Douglas and Ramsay, Craig and Fraser, Cynthia and Buckley, Brian and Fayers, Peter and Harvey, Ian and Briggs, Andrew and Norrie, John and Fergusson, Dean and Ford, Ian and Vale, Luke},
  date = {2014-01},
  journaltitle = {Health Technology Assessment},
  volume = {18},
  number = {28},
  issn = {1366-5278, 2046-4924},
  doi = {10.3310/hta18280},
  url = {https://www.journalslibrary.nihr.ac.uk/hta/hta18280/},
  urldate = {2020-12-11},
  langid = {english}
}

@article{cook_choosing_2017,
  title = {Choosing the Target Difference ('effect Size') for a Randomised Controlled Trial - {{DELTA2}} Guidance Protocol},
  author = {Cook, Jonathan A. and Julious, Steven A. and Sones, William and Rothwell, Joanne C. and Ramsay, Craig R. and Hampson, Lisa V. and Emsley, Richard and Walters, Stephen J. and Hewitt, Catherine and Bland, Martin and Fergusson, Dean A. and Berlin, Jesse A. and Altman, Doug and Vale, Luke D.},
  date = {2017-06-12},
  journaltitle = {Trials},
  shortjournal = {Trials},
  volume = {18},
  number = {1},
  eprint = {28606102},
  eprinttype = {pmid},
  pages = {271},
  issn = {1745-6215},
  doi = {10.1186/s13063-017-1969-5},
  abstract = {BACKGROUND: A key step in the design of a randomised controlled trial (RCT) is the estimation of the number of participants needed. By far the most common approach is to specify a target difference and then estimate the corresponding sample size; this sample size is chosen to provide reassurance that the trial will have high statistical power to detect such a difference between the randomised groups (at the planned statistical significance level). The sample size has many implications for the conduct of the study, as well as carrying scientific and ethical aspects to its choice. Despite the critical role of the target difference for the primary outcome in the design of an RCT, the manner in which it is determined has received little attention. This article reports the protocol of the Difference ELicitation in TriAls (DELTA2) project, which will produce guidance on the specification and reporting of the target difference for the primary outcome in a sample size calculation for RCTs. METHODS/DESIGN: The DELTA2 project has five components: systematic literature reviews of recent methodological developments (stage 1) and existing funder guidance (stage 2); a Delphi study (stage 3); a 2-day consensus meeting bringing together researchers, funders and patient representatives, as well as one-off engagement sessions at relevant stakeholder meetings (stage 4); and the preparation and dissemination of a guidance document (stage 5). DISCUSSION: Specification of the target difference for the primary outcome is a key component of the design of an RCT. There is a need for better guidance for researchers and funders regarding specification and reporting of this aspect of trial design. The aim of this project is to produce consensus based guidance for researchers and funders.},
  langid = {english},
  pmcid = {PMC5469157},
  keywords = {Clinically important difference,Consensus,Delphi Technique,Effect size,Endpoint Determination,Guidance,Humans,Pilot study,Randomised controlled trial,Randomized Controlled Trials as Topic,Research Design,Sample size,Sample Size,Target difference}
}

@book{cooper_reporting_2020,
  title = {Reporting Quantitative Research in Psychology: {{How}} to Meet {{APA Style Journal Article Reporting Standards}} (2nd Ed.).},
  shorttitle = {Reporting Quantitative Research in Psychology},
  author = {Cooper, Harris},
  date = {2020},
  publisher = {{American Psychological Association}},
  location = {{Washington}},
  doi = {10.1037/0000178-000},
  url = {http://content.apa.org/books/16150-000},
  urldate = {2020-07-25},
  isbn = {978-1-4338-3283-3 978-1-4338-3342-7},
  langid = {english}
}

@article{copay_understanding_2007,
  title = {Understanding the Minimum Clinically Important Difference: A Review of Concepts and Methods},
  shorttitle = {Understanding the Minimum Clinically Important Difference},
  author = {Copay, Anne G. and Subach, Brian R. and Glassman, Steven D. and Polly, David W. and Schuler, Thomas C.},
  date = {2007},
  journaltitle = {The Spine Journal},
  volume = {7},
  number = {5},
  pages = {541--546},
  doi = {10.1016/j.spinee.2007.01.008},
  url = {http://www.sciencedirect.com/science/article/pii/S1529943007000526},
  urldate = {2017-07-21}
}

@article{correll_avoid_2020,
  title = {Avoid {{Cohen}}’s ‘{{Small}}’, ‘{{Medium}}’, and ‘{{Large}}’ for {{Power Analysis}}},
  author = {Correll, Joshua and Mellinger, Christopher and McClelland, Gary H. and Judd, Charles M.},
  date = {2020-03-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {24},
  number = {3},
  pages = {200--207},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2019.12.009},
  url = {http://www.sciencedirect.com/science/article/pii/S1364661319302979},
  urldate = {2020-08-11},
  abstract = {One of the most difficult and important decisions in power analysis involves specifying an effect size. Researchers frequently employ definitions of small, medium, and large that were proposed by Jacob Cohen. These definitions are problematic for two reasons. First, they are arbitrary, based on non-scientific criteria. Second, they are inconsistent, changing dramatically and illogically as a function of the statistical test a researcher plans to use (e.g., t-test versus regression). These problems may be unknown to many researchers, but they have a huge impact on power analyses. Estimates of the required n may be inappropriately doubled or cut in half. For power analyses to have any meaning, these definitions of effect size should be avoided.},
  langid = {english},
  keywords = {effect size,research design,research methods}
}

@manual{cousineau_superb_2019,
  type = {manual},
  title = {Superb: {{Computes}} Standard Error and Confidence Interval of Means under Various Designs and Sampling Schemes},
  author = {Cousineau, Denis and Chiasson, Felix},
  date = {2019}
}

@article{cox_problems_1958,
  title = {Some {{Problems Connected}} with {{Statistical Inference}}},
  author = {Cox, D. R.},
  date = {1958-06},
  journaltitle = {Annals of Mathematical Statistics},
  shortjournal = {Ann. Math. Statist.},
  volume = {29},
  number = {2},
  pages = {357--372},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177706618},
  url = {https://projecteuclid.org/euclid.aoms/1177706618},
  urldate = {2020-05-21},
  abstract = {Project Euclid - mathematics and statistics online},
  langid = {english},
  mrnumber = {MR94890},
  zmnumber = {0088.11702}
}

@article{cumming_confidence_2006,
  title = {Confidence Intervals and Replication: {{Where}} Will the next Mean Fall?},
  shorttitle = {Confidence Intervals and Replication},
  author = {Cumming, Geoff and Maillardet, Robert},
  date = {2006},
  journaltitle = {Psychological Methods},
  volume = {11},
  number = {3},
  pages = {217--227},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.11.3.217},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/1082-989X.11.3.217},
  urldate = {2015-11-30},
  langid = {english}
}

@book{cumming_introduction_2016,
  title = {Introduction to the {{New Statistics}}: {{Estimation}}, {{Open Science}}, and {{Beyond}}},
  shorttitle = {Introduction to the {{New Statistics}}},
  author = {Cumming, Geoff and Calin-Jageman, Robert},
  date = {2016-10-04},
  eprint = {KR8xDQAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Routledge}},
  abstract = {This is the first introductory statistics text to use an estimation approach from the start to help readers understand effect sizes, confidence intervals (CIs), and meta-analysis (‘the new statistics’). It is also the first text to explain the new and exciting Open Science practices, which encourage replication and enhance the trustworthiness of research. In addition, the book explains NHST fully so students can understand published research. Numerous real research examples are used throughout. The book uses today’s most effective learning strategies and promotes critical thinking, comprehension, and retention, to deepen users’ understanding of statistics and modern research methods. The free ESCI (Exploratory Software for Confidence Intervals) software makes concepts visually vivid, and provides calculation and graphing facilities. The book can be used with or without ESCI.  Other highlights include: - Coverage of both estimation and NHST approaches, and how to easily translate between the two.  - Some exercises use ESCI to analyze data and create graphs including CIs, for best understanding of estimation methods.  -Videos of the authors describing key concepts and demonstrating use of ESCI provide an engaging learning tool for traditional or flipped classrooms. -In-chapter exercises and quizzes with related commentary allow students to learn by doing, and to monitor their progress. -End-of-chapter exercises and commentary, many using real data, give practice for using the new statistics to analyze data, as well as for applying research judgment in realistic contexts.  -Don’t fool yourself tips help students avoid common errors.  -Red Flags highlight the meaning of "significance" and what p values actually mean.  -Chapter outlines, defined key terms, sidebars of key points, and summarized take-home messages provide a study tool at exam time.  -http://www.routledge.com/cw/cumming offers for students: ESCI downloads; data sets; key term flashcards; tips for using SPSS for analyzing data; and videos. For instructors it offers: tips for teaching the new statistics and Open Science; additional homework exercises; assessment items; answer keys for homework and assessment items; and downloadable text images; and PowerPoint lecture slides.  Intended for introduction to statistics, data analysis, or quantitative methods courses in psychology, education, and other social and health sciences, researchers interested in understanding the new statistics will also appreciate this book. No familiarity with introductory statistics is assumed.},
  isbn = {978-1-317-48337-3},
  langid = {english},
  pagetotal = {595},
  keywords = {Business & Economics / Statistics,Education / Statistics,Medical / Biostatistics,Psychology / Statistics}
}

@article{cumming_new_2014,
  title = {The {{New Statistics}}: {{Why}} and {{How}}},
  shorttitle = {The {{New Statistics}}},
  author = {Cumming, G.},
  date = {2014-01-01},
  journaltitle = {Psychological Science},
  volume = {25},
  number = {1},
  pages = {7--29},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797613504966},
  url = {http://pss.sagepub.com/lookup/doi/10.1177/0956797613504966},
  urldate = {2015-11-30},
  langid = {english}
}

@book{cumming_understanding_2013,
  title = {Understanding the New Statistics: {{Effect}} Sizes, Confidence Intervals, and Meta-Analysis},
  shorttitle = {Understanding the New Statistics},
  author = {Cumming, Geoff},
  date = {2013},
  publisher = {{Routledge}},
  url = {https://books.google.nl/books?hl=nl&lr=&id=1W6laNc7Xt8C&oi=fnd&pg=PR1&dq=cumming+new+statistics&ots=PuKTSHb51T&sig=U-_k1y1YyREjLxP-FMfw4ood9OI},
  urldate = {2016-09-07}
}

@article{danziger_extraneous_2011,
  title = {Extraneous Factors in Judicial Decisions},
  author = {Danziger, S. and Levav, J. and Avnaim-Pesso, L.},
  date = {2011-04-26},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {108},
  number = {17},
  pages = {6889--6892},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/PNAS.1018033108},
  url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1018033108},
  urldate = {2018-07-24},
  langid = {english},
  annotation = {00711}
}

@book{de_groot_methodology_1969,
  title = {Methodology},
  author = {de Groot, Adrianus Dingeman},
  options = {useprefix=true},
  date = {1969},
  volume = {6},
  publisher = {{Mouton \& Co.}},
  location = {{The Hague}}
}

@online{de_vrieze_meta-analyses_2018,
  title = {Meta-Analyses Were Supposed to End Scientific Debates. {{Often}}, They Only Cause More Controversy},
  author = {de Vrieze, Jop and {2018} and Pm, 4:15},
  options = {useprefix=true},
  date = {2018-09-18T09:23:18-04:00},
  url = {https://www.sciencemag.org/news/2018/09/meta-analyses-were-supposed-end-scientific-debates-often-they-only-cause-more},
  urldate = {2019-08-04},
  abstract = {Compiling the evidence from dozens of studies doesn't always bring clarity},
  langid = {english},
  organization = {{Science | AAAS}}
}

@report{debruine_understanding_2019,
  type = {preprint},
  title = {Understanding Mixed Effects Models through Data Simulation},
  author = {DeBruine, Lisa Marie and Barr, Dale J.},
  date = {2019-06-02},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/xp5cy},
  url = {https://osf.io/xp5cy},
  urldate = {2019-12-03},
  abstract = {Experimental designs that sample both subjects and stimuli from a larger population need to account for random effects of both subjects and stimuli using mixed effects models. However, much of this research is analyzed using ANOVA on aggregated responses because researchers are not confident specifying and interpreting mixed effects models. The tutorial will explain how to simulate data with random effects structure and analyse the data using linear mixed effects regression (with the lme4 R package), with a focus on interpreting the output in light of the simulated parameters. Data simulation can not only enhance understanding of how these models work, but also enables researchers to perform power calculations for complex designs.}
}

@article{delacre_why_2017,
  title = {Why {{Psychologists Should}} by {{Default Use Welch}}’s {\emph{t}}-Test {{Instead}} of {{Student}}’s {\emph{t}}-Test},
  author = {Delacre, Marie and Lakens, Daniël and Leys, Christophe},
  date = {2017},
  journaltitle = {International Review of Social Psychology},
  volume = {30},
  number = {1},
  issn = {2119-4130},
  doi = {10.5334/irsp.82},
  url = {http://www.rips-irsp.com/articles/10.5334/irsp.82/},
  urldate = {2017-04-08},
  abstract = {When comparing two independent groups, psychology researchers commonly use Student’s t-tests. Assumptions of normality and homogeneity of variance underlie this test. More often than not, when these conditions are not met, Student’s t-test can be severely biased and lead to invalid statistical inferences. Moreover, we argue that the assumption of equal variances will seldom hold in psychological research, and choosing between Student’s t-test and Welch’s t-test based on the outcomes of a test of the equality of variances often fails to provide an appropriate answer. We show that the Welch’s t-test provides a better control of Type 1 error rates when the assumption of homogeneity of variance is not met, and it loses little robustness compared to Student’s t-test when the assumptions are met. We argue that Welch’s t-test should be used as a default strategy.},
  langid = {english},
  keywords = {homogeneity of variance,Homoscedasticity,Levene’s test,statistical power,Student’s t-test,type 1 error,type 2 error,Welch’s t-test}
}

@article{detsky_using_1990,
  title = {Using Cost-Effectiveness Analysis to Improve the Efficiency of Allocating Funds to Clinical Trials},
  author = {Detsky, Allan S.},
  date = {1990},
  journaltitle = {Statistics in Medicine},
  volume = {9},
  number = {1-2},
  pages = {173--184},
  issn = {1097-0258},
  doi = {10.1002/sim.4780090124},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4780090124},
  urldate = {2020-12-31},
  abstract = {This study applied a cost-effectiveness model to seven randomized trials. The model demonstrates the effect of design choices made in the planning stages of a clinical trial on the costs and benefits derived from conducting the trial. The study focused on one parameter used to calculate sample size: the minimum clinically important difference in event rates between control and experimental therapies. The study shows that the model can be operationalized to these trials. A computerized software package and manual has been developed to simplify the calculations. While there was some variation in the incremental cost-effectiveness ratios across the seven trials in this study, all ratios may be below the funding threshold. This analytical technique can be used to demonstrate explicitly the resource consequences of the design of randomized trials and perhaps to set funding priorities.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.4780090124}
}

@book{dienes_understanding_2008,
  title = {Understanding Psychology as a Science: {{An}} Introduction to Scientific and Statistical Inference},
  shorttitle = {Understanding Psychology as a Science},
  author = {Dienes, Zoltan},
  date = {2008},
  publisher = {{Palgrave Macmillan}}
}

@article{dienes_using_2014,
  title = {Using {{Bayes}} to Get the Most out of Non-Significant Results},
  author = {Dienes, Zoltan},
  date = {2014},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {5},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2014.00781},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00781/full},
  urldate = {2021-01-02},
  abstract = {No scientific conclusion follows automatically from a statistically non-significant result, yet people routinely use non-significant results to guide conclusions about the status of theories (or the effectiveness of practices). To know whether a non-significant result counts against a theory, or if it just indicates data insensitivity, researchers must use one of: power, intervals (such as confidence or credibility intervals), or else an indicator of the relative evidence for one theory over another, such as a Bayes factor. I argue Bayes factors allow theory to be linked to data in a way that overcomes the weaknesses of the other approaches. Specifically, Bayes factors use the data themselves to determine their sensitivity in distinguishing theories (unlike power), and they make use of those aspects of a theory’s predictions that are often easiest to specify (unlike power and intervals, which require specifying the minimal interesting value in order to address theory). Bayes factors provide a coherent approach to determining whether non-significant results support a null hypothesis over a theory, or whether the data are just insensitive. They allow accepting and rejecting the null hypothesis to be put on an equal footing. Concrete examples are provided to indicate the range of application of a simple online Bayes calculator, which reveal both the strengths and weaknesses of Bayes factors.},
  langid = {english},
  keywords = {Bayes factor,confidence interval,credibility interval,Significance testing,statistical inference}
}

@article{dodge_method_1929,
  title = {A {{Method}} of {{Sampling Inspection}}},
  author = {Dodge, H. F. and Romig, H. G.},
  date = {1929-10-01},
  journaltitle = {Bell System Technical Journal},
  volume = {8},
  number = {4},
  pages = {613--631},
  issn = {1538-7305},
  doi = {10.1002/j.1538-7305.1929.tb01240.x},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/j.1538-7305.1929.tb01240.x/abstract},
  urldate = {2017-06-29},
  abstract = {This paper outlines some of the general considerations which must be taken into account in setting up any practical sampling inspection plan. An economical method of inspection is developed in detail for the case where the purpose of the inspection is to determine the acceptability of discrete lots of a product submitted by a producer. By employing probability theory, the method places a definite barrier in the path of material of defective quality and gives this protection to the consumer with a minimum of inspection expense.},
  langid = {english}
}

@article{eckermann_value_2010,
  title = {The {{Value}} of {{Value}} of {{Information}}},
  author = {Eckermann, Simon and Karnon, Jon and Willan, Andrew R.},
  date = {2010-09-01},
  journaltitle = {PharmacoEconomics},
  shortjournal = {Pharmacoeconomics},
  volume = {28},
  number = {9},
  pages = {699--709},
  issn = {1179-2027},
  doi = {10.2165/11537370-000000000-00000},
  url = {https://doi.org/10.2165/11537370-000000000-00000},
  urldate = {2020-06-27},
  abstract = {Value of information (VOI) methods have been proposed as a systematic approach to inform optimal research design and prioritization. Four related questions arise that VOI methods could address. (i) Is further research for a health technology assessment (HTA) potentially worthwhile? (ii) Is the cost of a given research design less than its expected value? (iii) What is the optimal research design for an HTA? (iv) How can research funding be best prioritized across alternative HTAs?},
  langid = {english}
}

@article{erdfelder_gpower_1996,
  title = {{{GPOWER}}: {{A}} General Power Analysis Program},
  shorttitle = {{{GPOWER}}},
  author = {Erdfelder, Edgar and Faul, Franz and Buchner, Axel},
  date = {1996-03-01},
  journaltitle = {Behavior Research Methods, Instruments, \& Computers},
  shortjournal = {Behavior Research Methods, Instruments, \& Computers},
  volume = {28},
  number = {1},
  pages = {1--11},
  issn = {1532-5970},
  doi = {10.3758/BF03203630},
  url = {https://doi.org/10.3758/BF03203630},
  urldate = {2020-08-04},
  abstract = {GPOWER is a completely interactive, menu-driven program for IBM-compatible and Apple Macintosh personal computers. It performs high-precision statistical power analyses for the most common statistical tests in behavioral research, that is,t tests,F tests, andχ2 tests. GPOWER computes (1) power values for given sample sizes, effect sizes andα levels (post hoc power analyses); (2) sample sizes for given effect sizes,α levels, and power values (a priori power analyses); and (3)α andβ values for given sample sizes, effect sizes, andβ/α ratios (compromise power analyses). The program may be used to display graphically the relation between any two of the relevant variables, and it offers the opportunity to compute the effect size measures from basic parameters defining the alternative hypothesis. This article delineates reasons for the development of GPOWER and describes the program’s capabilities and handling.},
  langid = {english}
}

@article{faul_gpower_2007,
  title = {{{GPower}} 3: {{A}} Flexible Statistical Power Analysis Program for the Social, Behavioral, and Biomedical Sciences},
  shorttitle = {G*{{Power}} 3},
  author = {Faul, Franz and Erdfelder, Edgar and Lang, Albert-Georg and Buchner, Axel},
  date = {2007-05},
  journaltitle = {Behavior Research Methods},
  volume = {39},
  number = {2},
  pages = {175--191},
  issn = {1554-351X, 1554-3528},
  doi = {10.3758/BF03193146},
  url = {http://www.springerlink.com/index/10.3758/BF03193146},
  urldate = {2018-05-24},
  langid = {english}
}

@article{ferguson_vast_2012,
  title = {A Vast Graveyard of Undead Theories Publication Bias and Psychological Science’s Aversion to the Null},
  author = {Ferguson, Christopher J. and Heene, Moritz},
  date = {2012},
  journaltitle = {Perspectives on Psychological Science},
  volume = {7},
  number = {6},
  pages = {555--561},
  url = {http://pps.sagepub.com/content/7/6/555.short},
  urldate = {2015-12-11}
}

@article{ferron_power_1996,
  title = {The {{Power}} of {{Randomization Tests}} for {{Single-Case Phase Designs}}},
  author = {Ferron, John and Onghena, Patrick},
  date = {1996-04-01},
  journaltitle = {The Journal of Experimental Education},
  volume = {64},
  number = {3},
  pages = {231--239},
  publisher = {{Routledge}},
  issn = {0022-0973},
  doi = {10.1080/00220973.1996.9943805},
  url = {https://doi.org/10.1080/00220973.1996.9943805},
  urldate = {2021-02-07},
  abstract = {Monte Carlo methods were used to estimate the power of randomization tests used with single-case designs involving the random assignment of treatments to phases. The design studied involved 2 treatments and 6 phases. The power was studied for 6 standardized effect sizes (0, .2, .5, .8, 1.1, and 1.4), 4 levels of autocorrelation (1st order autocorrelation coefficients of -.3, 0, .3, and .6), and 5 different phase lengths (4, 5, 6, 7, and 8 observations). Power was estimated for each condition by simulating 10,000 experiments. The results showed an adequate level of power ({$>$} .80) when effect sizes were large (1.1 and 1.4), phase lengths exceeded 5, and autocorrelation was not negative.},
  annotation = {\_eprint: https://doi.org/10.1080/00220973.1996.9943805}
}

@book{feyerabend_against_1993,
  title = {Against Method},
  author = {Feyerabend, Paul},
  date = {1993},
  edition = {3rd ed},
  publisher = {{Verso}},
  location = {{London ; New York}},
  isbn = {978-0-86091-481-5 978-0-86091-646-8},
  pagetotal = {279},
  keywords = {Methodology,Philosophy,Rationalism,Science}
}

@article{fiedler_questionable_2015,
  title = {Questionable {{Research Practices Revisited}}},
  author = {Fiedler, Klaus and Schwarz, Norbert},
  date = {2015-10-19},
  journaltitle = {Social Psychological and Personality Science},
  shortjournal = {Social Psychological and Personality Science},
  pages = {1948550615612150},
  issn = {1948-5506, 1948-5514},
  doi = {10.1177/1948550615612150},
  url = {http://spp.sagepub.com/content/early/2015/10/19/1948550615612150},
  urldate = {2015-12-11},
  abstract = {The current discussion of questionable research practices (QRPs) is meant to improve the quality of science. It is, however, important to conduct QRP studies with the same scrutiny as all research. We note problems with overestimates of QRP prevalence and the survey methods used in the frequently cited study by John, Loewenstein, and Prelec. In a survey of German psychologists, we decomposed QRP prevalence into its two multiplicative components, proportion of scientists who ever committed a behavior and, if so, how frequently they repeated this behavior across all their research. The resulting prevalence estimates are lower by order of magnitudes. We conclude that inflated prevalence estimates, due to problematic interpretation of survey data, can create a descriptive norm (QRP is normal) that can counteract the injunctive norm to minimize QRPs and unwantedly damage the image of behavioral sciences, which are essential to dealing with many societal problems.},
  langid = {english},
  keywords = {ethics/morality,language,research methods,research practices,survey methodology}
}

@article{fiedler_tools_2004,
  title = {Tools, Toys, Truisms, and Theories: {{Some}} Thoughts on the Creative Cycle of Theory Formation},
  shorttitle = {Tools, Toys, Truisms, and Theories},
  author = {Fiedler, Klaus},
  date = {2004},
  journaltitle = {Personality and Social Psychology Review},
  volume = {8},
  number = {2},
  pages = {123--131},
  url = {http://psr.sagepub.com/content/8/2/123.short},
  urldate = {2016-09-07}
}

@article{field_minimizing_2004,
  title = {Minimizing the Cost of Environmental Management Decisions by Optimizing Statistical Thresholds},
  author = {Field, Scott A. and Tyre, Andrew J. and Jonzén, Niclas and Rhodes, Jonathan R. and Possingham, Hugh P.},
  date = {2004-08-01},
  journaltitle = {Ecology Letters},
  volume = {7},
  number = {8},
  pages = {669--675},
  issn = {1461-0248},
  doi = {10.1111/j.1461-0248.2004.00625.x},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1461-0248.2004.00625.x/abstract},
  urldate = {2017-08-24},
  abstract = {Environmental management decisions are prone to expensive mistakes if they are triggered by hypothesis tests using the conventional Type I error rate (α) of 0.05. We derive optimal α-levels for decision-making by minimizing a cost function that specifies the overall cost of monitoring and management. When managing an economically valuable koala population, it shows that a decision based on α~=~0.05 carries an expected cost over \$5~million greater than the optimal decision. For a species of such value, there is never any benefit in guarding against the spurious detection of declines and therefore management should proceed directly to recovery action. This result holds in most circumstances where the species’ value substantially exceeds its recovery costs. For species of lower economic value, we show that the conventional α-level of 0.05 rarely approximates the optimal decision-making threshold. This analysis supports calls for reversing the statistical ‘burden of proof’ in environmental decision-making when the cost of Type II errors is relatively high.},
  langid = {english},
  keywords = {Koala,management,optimal monitoring,statistical power,Statistical Significance,type I error,type II error}
}

@book{fisher_design_1935,
  title = {The Design of Experiments},
  author = {Fisher, Ronald Aylmer},
  date = {1935},
  publisher = {{Oliver And Boyd; Edinburgh; London}}
}

@book{fisher_statistical_1956,
  title = {Statistical Methods and Scientific Inference},
  author = {Fisher, Ronald A.},
  date = {1956},
  volume = {viii},
  publisher = {{Hafner Publishing Co.}},
  location = {{Oxford, England}},
  abstract = {An explicit statement of the logical nature of statistical reasoning that has been implicitly required in the development and use of statistical techniques in the making of uncertain inferences and in the design of experiments. Included is a consideration of the concept of mathematical probability; a comparison of fiducial and confidence intervals; a comparison of the logic of tests of significance with the acceptance decision approach; and a discussion of the principles of prediction and estimation.},
  pagetotal = {175}
}

@article{fortin_big_2013,
  title = {Big Science vs. Little Science: How Scientific Impact Scales with Funding},
  shorttitle = {Big Science vs. Little Science},
  author = {Fortin, Jean-Michel and Currie, David J.},
  date = {2013},
  url = {http://dx.plos.org/10.1371/journal.pone.0065263},
  urldate = {2015-12-12}
}

@article{fraley_n-pact_2014,
  title = {The {{N-Pact Factor}}: {{Evaluating}} the {{Quality}} of {{Empirical Journals}} with {{Respect}} to {{Sample Size}} and {{Statistical Power}}},
  shorttitle = {The {{N-Pact Factor}}},
  author = {Fraley, R. Chris and Vazire, Simine},
  date = {2014-10-08},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {9},
  number = {10},
  pages = {e109019},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0109019},
  url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0109019},
  urldate = {2016-04-01},
  abstract = {The authors evaluate the quality of research reported in major journals in social-personality psychology by ranking those journals with respect to their  N -pact Factors (NF)—the statistical power of the empirical studies they publish to detect typical effect sizes. Power is a particularly important attribute for evaluating research quality because, relative to studies that have low power, studies that have high power are more likely to (a) to provide accurate estimates of effects, (b) to produce literatures with low false positive rates, and (c) to lead to replicable findings. The authors show that the average sample size in social-personality research is 104 and that the power to detect the typical effect size in the field is approximately 50\%. Moreover, they show that there is considerable variation among journals in sample sizes and power of the studies they publish, with some journals consistently publishing higher power studies than others. The authors hope that these rankings will be of use to authors who are choosing where to submit their best work, provide hiring and promotion committees with a superior way of quantifying journal quality, and encourage competition among journals to improve their NF rankings.},
  keywords = {Personality,Personality differences,Psychology,Research Design,Research integrity,Research quality assessment,Social psychology,Social research}
}

@article{francis_frequency_2014,
  title = {The Frequency of Excess Success for Articles in {{Psychological Science}}},
  author = {Francis, Gregory},
  date = {2014-03-18},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {21},
  number = {5},
  pages = {1180--1187},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-014-0601-x},
  url = {http://link.springer.com/article/10.3758/s13423-014-0601-x},
  urldate = {2015-12-11},
  abstract = {Recent controversies have questioned the quality of scientific practice in the field of psychology, but these concerns are often based on anecdotes and seemingly isolated cases. To gain a broader perspective, this article applies an objective test for excess success to a large set of articles published in the journal Psychological Science between 2009 and 2012. When empirical studies succeed at a rate much higher than is appropriate for the estimated effects and sample sizes, readers should suspect that unsuccessful findings have been suppressed, the experiments or analyses were improper, or the theory does not properly account for the data. In total, problems appeared for 82 \% (36 out of 44) of the articles in Psychological Science that had four or more experiments and could be analyzed.},
  langid = {english},
  keywords = {Cognitive psychology,Probabilistic reasoning,Statistical inference,Statistics}
}

@article{francis_too_2012,
  title = {Too Good to Be True: {{Publication}} Bias in Two Prominent Studies from Experimental Psychology},
  shorttitle = {Too Good to Be True},
  author = {Francis, Gregory},
  date = {2012},
  journaltitle = {Psychonomic bulletin \& review},
  volume = {19},
  number = {2},
  pages = {151--156}
}

@article{franco_publication_2014,
  title = {Publication Bias in the Social Sciences: {{Unlocking}} the File Drawer},
  shorttitle = {Publication Bias in the Social Sciences},
  author = {Franco, Annie and Malhotra, Neil and Simonovits, Gabor},
  date = {2014},
  journaltitle = {Science},
  volume = {345},
  number = {6203},
  pages = {1502--1505},
  doi = {10.1126/SCIENCE.1255484},
  annotation = {00377}
}

@article{friede_sample_2006,
  title = {Sample Size Recalculation in Internal Pilot Study Designs: A Review},
  shorttitle = {Sample Size Recalculation in Internal Pilot Study Designs},
  author = {Friede, Tim and Kieser, Meinhard},
  date = {2006},
  journaltitle = {Biometrical Journal: Journal of Mathematical Methods in Biosciences},
  volume = {48},
  number = {4},
  pages = {537--555},
  publisher = {{Wiley Online Library}},
  doi = {10.1002/bimj.200510238}
}

@article{fugard_supporting_2015,
  title = {Supporting Thinking on Sample Sizes for Thematic Analyses: A Quantitative Tool},
  shorttitle = {Supporting Thinking on Sample Sizes for Thematic Analyses},
  author = {Fugard, Andrew J. B. and Potts, Henry W. W.},
  date = {2015-11-02},
  journaltitle = {International Journal of Social Research Methodology},
  volume = {18},
  number = {6},
  pages = {669--684},
  publisher = {{Routledge}},
  issn = {1364-5579},
  doi = {10.1080/13645579.2015.1005453},
  url = {https://doi.org/10.1080/13645579.2015.1005453},
  urldate = {2020-08-26},
  abstract = {Thematic analysis is frequently used to analyse qualitative data in psychology, healthcare, social research and beyond. An important stage in planning a study is determining how large a sample size may be required, however current guidelines for thematic analysis are varied, ranging from around 2 to over 400 and it is unclear how to choose a value from the space in between. Some guidance can also not be applied prospectively. This paper introduces a tool to help users think about what would be a useful sample size for their particular context when investigating patterns across participants. The calculation depends on (a) the expected population theme prevalence of the least prevalent theme, derived either from prior knowledge or based on the prevalence of the rarest themes considered worth uncovering, e.g. 1 in 10, 1 in 100; (b) the number of desired instances of the theme; and (c) the power of the study. An adequately powered study will have a high likelihood of finding sufficient themes of the desired prevalence. This calculation can then be used alongside other considerations. We illustrate how to use the method to calculate sample size before starting a study and achieved power given a sample size, providing tables of answers and code for use in the free software, R. Sample sizes are comparable to those found in the literature, for example to have 80\% power to detect two instances of a theme with 10\% prevalence, 29 participants are required. Increasing power, increasing the number of instances or decreasing prevalence increases the sample size needed. We do not propose this as a ritualistic requirement for study design, but rather as a pragmatic supporting tool to help plan studies using thematic analysis.},
  keywords = {Corrigendum,power analysis,sample size determination,thematic analysis},
  annotation = {\_eprint: https://doi.org/10.1080/13645579.2015.1005453}
}

@article{funder_evaluating_2019,
  title = {Evaluating Effect Size in Psychological Research: {{Sense}} and Nonsense},
  shorttitle = {Evaluating Effect Size in Psychological Research},
  author = {Funder, David C. and Ozer, Daniel J.},
  date = {2019},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  volume = {2},
  number = {2},
  pages = {156--168},
  publisher = {{Sage Publications Sage CA: Los Angeles, CA}},
  doi = {10.1177/2515245919847202}
}

@article{glockner_irrational_2016,
  title = {The Irrational Hungry Judge Effect Revisited: {{Simulations}} Reveal That the Magnitude of the Effect Is Overestimated},
  shorttitle = {The Irrational Hungry Judge Effect Revisited},
  author = {Glöckner, Andreas},
  date = {2016},
  journaltitle = {Judgment and Decision Making},
  volume = {11},
  number = {6},
  pages = {601--610}
}

@article{good_bayesnon-bayes_1992,
  title = {The {{Bayes}}/{{Non-Bayes Compromise}}: {{A Brief Review}}},
  shorttitle = {The {{Bayes}}/{{Non-Bayes Compromise}}},
  author = {Good, I. J.},
  date = {1992-09},
  journaltitle = {Journal of the American Statistical Association},
  volume = {87},
  number = {419},
  eprint = {2290192},
  eprinttype = {jstor},
  pages = {597--606},
  issn = {01621459},
  doi = {10.2307/2290192}
}

@article{good_c140_1982,
  title = {C140. {{Standardized}} Tail-Area Probabilities},
  author = {Good, I. J.},
  date = {1982-12-01},
  journaltitle = {Journal of Statistical Computation and Simulation},
  volume = {16},
  number = {1},
  pages = {65--66},
  issn = {0094-9655},
  doi = {10.1080/00949658208810607},
  url = {https://doi.org/10.1080/00949658208810607},
  urldate = {2018-03-29}
}

@article{green_how_1991,
  title = {How {{Many Subjects Does It Take To Do A Regression Analysis}}},
  author = {Green, S. B.},
  date = {1991-07-01},
  journaltitle = {Multivariate Behavioral Research},
  shortjournal = {Multivariate Behav Res},
  volume = {26},
  number = {3},
  eprint = {26776715},
  eprinttype = {pmid},
  pages = {499--510},
  issn = {0027-3171},
  doi = {10.1207/s15327906mbr2603_7},
  abstract = {Numerous rules-of-thumb have been suggested for determining the minimum number of subjects required to conduct multiple regression analyses. These rules-of-thumb are evaluated by comparing their results against those based on power analyses for tests of hypotheses of multiple and partial correlations. The results did not support the use of rules-of-thumb that simply specify some constant (e.g., 100 subjects) as the minimum number of subjects or a minimum ratio of number of subjects (N) to number of predictors (m). Some support was obtained for a rule-of-thumb that N ≥ 50 + 8 m for the multiple correlation and N ≥104 + m for the partial correlation. However, the rule-of-thumb for the multiple correlation yields values too large for N when m ≥ 7, and both rules-of-thumb assume all studies have a medium-size relationship between criterion and predictors. Accordingly, a slightly more complex rule-of thumb is introduced that estimates minimum sample size as function of effect size as well as the number of predictors. It is argued that researchers should use methods to determine sample size that incorporate effect size.},
  langid = {english}
}

@article{green_simr_2016,
  title = {{{SIMR}}: An {{R}} Package for Power Analysis of Generalized Linear Mixed Models by Simulation},
  shorttitle = {{{SIMR}}},
  author = {Green, Peter and MacLeod, Catriona J.},
  date = {2016},
  journaltitle = {Methods in Ecology and Evolution},
  volume = {7},
  number = {4},
  pages = {493--498},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.12504},
  url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12504},
  urldate = {2019-03-22},
  abstract = {The r package simr allows users to calculate power for generalized linear mixed models from the lme4 package. The power calculations are based on Monte Carlo simulations. It includes tools for (i) running a power analysis for a given model and design; and (ii) calculating power curves to assess trade-offs between power and sample size. This paper presents a tutorial using a simple example of count data with mixed effects (with structure representative of environmental monitoring data) to guide the user along a gentle learning curve, adding only a few commands or options at a time.},
  langid = {english},
  keywords = {experimental design,glmm,Monte Carlo,random effects,sample size,type II error},
  annotation = {00148}
}

@article{greenland_statistical_2016,
  title = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power: A Guide to Misinterpretations},
  shorttitle = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power},
  author = {Greenland, Sander and Senn, Stephen J. and Rothman, Kenneth J. and Carlin, John B. and Poole, Charles and Goodman, Steven N. and Altman, Douglas G.},
  date = {2016-04},
  journaltitle = {European Journal of Epidemiology},
  volume = {31},
  number = {4},
  pages = {337--350},
  issn = {0393-2990, 1573-7284},
  doi = {10.1007/s10654-016-0149-3},
  url = {http://link.springer.com/10.1007/s10654-016-0149-3},
  urldate = {2016-07-02},
  langid = {english}
}

@article{greenwald_consequences_1975,
  title = {Consequences of Prejudice against the Null Hypothesis.},
  author = {Greenwald, Anthony G.},
  date = {1975},
  journaltitle = {Psychological Bulletin},
  volume = {82},
  number = {1},
  pages = {1},
  url = {http://psycnet.apa.org/journals/bul/82/1/1/},
  urldate = {2015-12-23}
}

@online{grunwald_safe_2019,
  title = {Safe {{Testing}}},
  author = {Grünwald, Peter and de Heide, Rianne and Koolen, Wouter},
  options = {useprefix=true},
  date = {2019-06-18},
  eprint = {1906.07801},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1906.07801},
  urldate = {2019-10-21},
  abstract = {We present a new theory of hypothesis testing. The main concept is the S-value, a notion of evidence which, unlike p-values, allows for effortlessly combining evidence from several tests, even in the common scenario where the decision to perform a new test depends on the previous test outcome: safe tests based on S-values generally preserve Type-I error guarantees under such "optional continuation". S-values exist for completely general testing problems with composite null and alternatives. Their prime interpretation is in terms of gambling or investing, each S-value corresponding to a particular investment. Surprisingly, optimal "GROW" S-values, which lead to fastest capital growth, are fully characterized by the joint information projection (JIPr) between the set of all Bayes marginal distributions on H0 and H1. Thus, optimal S-values also have an interpretation as Bayes factors, with priors given by the JIPr. We illustrate the theory using two classical testing scenarios: the one-sample t-test and the 2x2 contingency table. In the t-test setting, GROW s-values correspond to adopting the right Haar prior on the variance, like in Jeffreys' Bayesian t-test. However, unlike Jeffreys', the "default" safe t-test puts a discrete 2-point prior on the effect size, leading to better behavior in terms of statistical power. Sharing Fisherian, Neymanian and Jeffreys-Bayesian interpretations, S-values and safe tests may provide a methodology acceptable to adherents of all three schools.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Methodology}
}

@article{gupta_intention_2011,
  title = {Intention-to-Treat Concept: {{A}} Review},
  shorttitle = {Intention-to-Treat Concept},
  author = {Gupta, Sandeep K.},
  date = {2011},
  journaltitle = {Perspectives in Clinical Research},
  shortjournal = {Perspect Clin Res},
  volume = {2},
  number = {3},
  eprint = {21897887},
  eprinttype = {pmid},
  pages = {109--112},
  issn = {2229-3485},
  doi = {10.4103/2229-3485.83221},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3159210/},
  urldate = {2020-12-15},
  abstract = {Randomized controlled trials often suffer from two major complications, i.e., noncompliance and missing outcomes. One potential solution to this problem is a statistical concept called intention-to-treat (ITT) analysis. ITT analysis includes every subject who is randomized according to randomized treatment assignment. It ignores noncompliance, protocol deviations, withdrawal, and anything that happens after randomization. ITT analysis maintains prognostic balance generated from the original random treatment allocation. In ITT analysis, estimate of treatment effect is generally conservative. A better application of the ITT approach is possible if complete outcome data are available for all randomized subjects. Per-protocol population is defined as a subset of the ITT population who completed the study without any major protocol violations.},
  pmcid = {PMC3159210}
}

@article{hacking_experimentation_1982,
  title = {Experimentation and {{Scientific Realism}}},
  author = {Hacking, Ian},
  date = {1982},
  journaltitle = {Philosophical Topics},
  volume = {13},
  number = {1},
  eprint = {43153910},
  eprinttype = {jstor},
  pages = {71--87},
  issn = {0276-2080},
  doi = {10/fz8ftm},
  annotation = {00312}
}

@article{hallahan_statistical_1996,
  title = {Statistical Power: {{Concepts}}, Procedures, and Applications},
  shorttitle = {Statistical Power},
  author = {Hallahan, Mark and Rosenthal, Robert},
  date = {1996-05-01},
  journaltitle = {Behaviour Research and Therapy},
  shortjournal = {Behaviour Research and Therapy},
  volume = {34},
  number = {5},
  pages = {489--499},
  issn = {0005-7967},
  doi = {10.1016/0005-7967(95)00082-8},
  url = {http://www.sciencedirect.com/science/article/pii/0005796795000828},
  urldate = {2020-12-16},
  abstract = {This paper discusses the concept of statistical power and its application to psychological research. Power, the probability that a significance test will produce a significant result when the null hypothesis is false, often is neglected with potentially serious consequences. The concept of power should be considered as part of planning and interpreting research. This article provides explication of the concept of power and suggestions for researchers to increase the power of their investigations.},
  langid = {english}
}

@article{halpern_continuing_2002,
  title = {The Continuing Unethical Conduct of Underpowered Clinical Trials},
  author = {Halpern, Scott D. and Karlawish, Jason HT and Berlin, Jesse A.},
  date = {2002},
  journaltitle = {Jama},
  volume = {288},
  number = {3},
  pages = {358--362},
  publisher = {{American Medical Association}},
  doi = {doi:10.1001/jama.288.3.358}
}

@article{halpern_sample_2001,
  title = {The Sample Size for a Clinical Trial: {{A Bayesian}} Decision Theoretic Approach},
  shorttitle = {The Sample Size for a Clinical Trial},
  author = {Halpern, Jerry and Brown Jr, Byron Wm and Hornberger, John},
  date = {2001},
  journaltitle = {Statistics in Medicine},
  volume = {20},
  number = {6},
  pages = {841--858},
  publisher = {{Wiley Online Library}},
  doi = {10.1002/sim.703}
}

@article{harms_making_2018,
  title = {Making 'null Effects' Informative: Statistical Techniques and Inferential Frameworks},
  shorttitle = {Making 'null Effects' Informative},
  author = {Harms, Christopher and Lakens, Daniël},
  date = {2018},
  journaltitle = {Journal of Clinical and Translational Research},
  number = {3},
  pages = {382--393},
  issn = {2424810X},
  doi = {10.18053/jctres.03.2017S2.007},
  url = {http://www.jctres.com/en/03.2017S2.007/},
  urldate = {2019-04-10},
  abstract = {Being able to interpret ‘null effects’ is important for cumulative knowledge generation in science. To draw informative conclusions from null-effects, researchers need to move beyond the incorrect interpretation of a non-significant result in a null-hypothesis significance test as evidence of the absence of an effect. We explain how to statistically evaluate null-results using equivalence tests, Bayesian estimation, and Bayes factors. A worked example demonstrates how to apply these statistical tools and interpret the results. Finally, we explain how no statistical approach can actually prove that the null-hypothesis is true, and briefly discuss the philosophical differences between statistical approaches to examine null-effects. The increasing availability of easy-to-use software and online tools to perform equivalence tests, Bayesian estimation, and calculate Bayes factors make it timely and feasible to complement or move beyond traditional null-hypothesis tests, and allow researchers to draw more informative conclusions about null-effects.},
  langid = {english}
}

@article{heath_calculating_2020,
  title = {Calculating the {{Expected Value}} of {{Sample Information}} in {{Practice}}: {{Considerations}} from 3 {{Case Studies}}:},
  shorttitle = {Calculating the {{Expected Value}} of {{Sample Information}} in {{Practice}}},
  author = {Heath, Anna and Kunst, Natalia and Jackson, Christopher and Strong, Mark and Alarid-Escudero, Fernando and Goldhaber-Fiebert, Jeremy D. and Baio, Gianluca and Menzies, Nicolas A. and Jalal, Hawre},
  date = {2020-04-16},
  journaltitle = {Medical Decision Making},
  publisher = {{SAGE PublicationsSage CA: Los Angeles, CA}},
  doi = {10.1177/0272989X20912402},
  url = {https://journals.sagepub.com/doi/10.1177/0272989X20912402?url_ver=Z39.88-2003&rfr_id=ori%3Arid%3Acrossref.org&rfr_dat=cr_pub++0pubmed},
  urldate = {2020-11-10},
  abstract = {Background. Investing efficiently in future research to improve policy decisions is an important goal. Expected value of sample information (EVSI) can be used t...},
  langid = {english}
}

@article{hedges_power_2001,
  title = {The Power of Statistical Tests in Meta-Analysis.},
  author = {Hedges, Larry V. and Pigott, Therese D.},
  date = {2001},
  journaltitle = {Psychological methods},
  volume = {6},
  number = {3},
  pages = {203--217},
  publisher = {{American Psychological Association}},
  doi = {10.1037/1082-989X.6.3.203}
}

@online{heino_legacy_2016,
  title = {The Legacy of Social Psychology},
  author = {Heino, Matti TJ},
  date = {2016-11-13T18:33:58+00:00},
  url = {https://mattiheino.com/2016/11/13/legacy-of-psychology/},
  urldate = {2019-07-19},
  abstract = {What can we learn re-examining the classic cognitive dissonance experiment?},
  langid = {english},
  organization = {{Data punk | Käyttäytymisarkkitehtuuri}}
}

@article{hilgard_maximal_2021,
  title = {Maximal Positive Controls: {{A}} Method for Estimating the Largest Plausible Effect Size},
  shorttitle = {Maximal Positive Controls},
  author = {Hilgard, Joseph},
  date = {2021-03-01},
  journaltitle = {Journal of Experimental Social Psychology},
  shortjournal = {Journal of Experimental Social Psychology},
  volume = {93},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2020.104082},
  url = {http://www.sciencedirect.com/science/article/pii/S0022103120304224},
  urldate = {2020-12-12},
  abstract = {Effect sizes in social psychology are generally not large and are limited by error variance in manipulation and measurement. Effect sizes exceeding these limits are implausible and should be viewed with skepticism. Maximal positive controls, experimental conditions that should show an obvious and predictable effect, can provide estimates of the upper limits of plausible effect sizes on a measure. In this work, maximal positive controls are conducted for three measures of aggressive cognition, and the effect sizes obtained are compared to studies found through systematic review. Questions are raised regarding the plausibility of certain reports with effect sizes comparable to, or in excess of, the effect sizes found in maximal positive controls. Maximal positive controls may provide a means to identify implausible study results at lower cost than direct replication.},
  langid = {english},
  keywords = {Aggression,Aggressive thought,Positive controls,Scientific self-correction,Violent video games}
}

@article{hill_empirical_2008,
  title = {Empirical {{Benchmarks}} for {{Interpreting Effect Sizes}} in {{Research}}},
  author = {Hill, Carolyn J. and Bloom, Howard S. and Black, Alison Rebeck and Lipsey, Mark W.},
  date = {2008},
  journaltitle = {Child Development Perspectives},
  volume = {2},
  number = {3},
  pages = {172--177},
  issn = {1750-8606},
  doi = {10.1111/j.1750-8606.2008.00061.x},
  url = {https://srcd.onlinelibrary.wiley.com/doi/abs/10.1111/j.1750-8606.2008.00061.x},
  urldate = {2020-03-08},
  abstract = {ABSTRACT— There is no universal guideline or rule of thumb for judging the practical importance or substantive significance of a standardized effect size estimate for an intervention. Instead, one must develop empirical benchmarks of comparison that reflect the nature of the intervention being evaluated, its target population, and the outcome measure or measures being used. This approach is applied to the assessment of effect size measures for educational interventions designed to improve student academic achievement. Three types of empirical benchmarks are illustrated: (a) normative expectations for growth over time in student achievement, (b) policy-relevant gaps in student achievement by demographic group or school performance, and (c) effect size results from past research for similar interventions and target populations. The findings can be used to help assess educational interventions, and the process of doing so can provide guidelines for how to develop and use such benchmarks in other fields.},
  langid = {english},
  keywords = {educational evaluation,effect size,student performance},
  annotation = {\_eprint: https://srcd.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1750-8606.2008.00061.x}
}

@article{hoenig_abuse_2001,
  title = {The Abuse of Power: The Pervasive Fallacy of Power Calculations for Data Analysis},
  shorttitle = {The Abuse of Power},
  author = {Hoenig, John M. and Heisey, Dennis M.},
  date = {2001},
  journaltitle = {The American Statistician},
  volume = {55},
  number = {1},
  pages = {19--24},
  doi = {10.1198/000313001300339897},
  url = {http://amstat.tandfonline.com/doi/abs/10.1198/000313001300339897},
  urldate = {2017-09-30}
}

@article{huedo-medina_assessing_2006,
  title = {Assessing Heterogeneity in Meta-Analysis: {{Q}} Statistic or {{I}}\$\^2\$ Index?},
  shorttitle = {Assessing Heterogeneity in Meta-Analysis},
  author = {Huedo-Medina, Tania B. and Sánchez-Meca, Julio and Marín-Martínez, Fulgencio and Botella, Juan},
  date = {2006},
  journaltitle = {Psychological methods},
  volume = {11},
  number = {2},
  pages = {193},
  url = {http://psycnet.apa.org/journals/met/11/2/193/},
  urldate = {2016-05-01},
  annotation = {00000}
}

@article{hung_behavior_1997,
  title = {The {{Behavior}} of the {{P-Value When}} the {{Alternative Hypothesis}} Is {{True}}},
  author = {Hung, H. M. James and O'Neill, Robert T. and Bauer, Peter and Kohne, Karl},
  date = {1997},
  journaltitle = {Biometrics},
  shortjournal = {Biometrics},
  volume = {53},
  number = {1},
  eprint = {2533093},
  eprinttype = {jstor},
  pages = {11--22},
  issn = {0006-341X},
  doi = {10.2307/2533093},
  abstract = {The P-value is a random variable derived from the distribution of the test statistic used to analyze a data set and to test a null hypothesis. Under the null hypothesis, the P-value based on a continuous test statistic has a uniform distribution over the interval [0, 1], regardless of the sample size of the experiment. In contrast, the distribution of the P-value under the alternative hypothesis is a function of both sample size and the true value or range of true values of the tested parameter. The characteristics, such as mean and percentiles, of the P-value distribution can give valuable insight into how the P-value behaves for a variety of parameter values and sample sizes. Potential applications of the P-value distribution under the alternative hypothesis to the design, analysis, and interpretation of results of clinical trials are considered.},
  annotation = {00148}
}

@article{jaeschke_measurement_1989,
  title = {Measurement of Health Status: {{Ascertaining}} the Minimal Clinically Important Difference},
  shorttitle = {Measurement of Health Status},
  author = {Jaeschke, Roman and Singer, Joel and Guyatt, Gordon H.},
  date = {1989-12-01},
  journaltitle = {Controlled Clinical Trials},
  shortjournal = {Controlled Clinical Trials},
  volume = {10},
  number = {4},
  eprint = {2691207},
  eprinttype = {pmid},
  pages = {407--415},
  issn = {0197-2456},
  doi = {10.1016/0197-2456(89)90005-6},
  url = {http://www.contemporaryclinicaltrials.com/article/0197-2456(89)90005-6/fulltext},
  urldate = {2018-02-09},
  langid = {english},
  keywords = {Measurement,quality of life,responsiveness},
  annotation = {02971}
}

@book{jeffreys_theory_1939,
  title = {Theory of Probability},
  author = {Jeffreys, Harold},
  date = {1939},
  series = {The {{International}} Series of Monographs on Physics},
  edition = {1st ed},
  publisher = {{Oxford University Press}},
  location = {{Oxford [Oxfordshire]: New York}},
  isbn = {978-0-19-853193-7},
  pagetotal = {459},
  keywords = {Probabilities}
}

@book{jennison_group_2000,
  title = {Group Sequential Methods with Applications to Clinical Trials},
  author = {Jennison, Christopher and Turnbull, Bruce W.},
  date = {2000},
  publisher = {{Chapman \& Hall/CRC}},
  location = {{Boca Raton}},
  isbn = {978-0-8493-0316-6},
  pagetotal = {390},
  keywords = {Clinical Trials,Decision Theory,methods,Models; Statistical,Statistical methods,Statistics}
}

@article{john_measuring_2012,
  title = {Measuring the Prevalence of Questionable Research Practices with Incentives for Truth Telling},
  author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
  date = {2012},
  journaltitle = {Psychological science},
  volume = {23},
  number = {5},
  pages = {524--532},
  url = {http://journals.sagepub.com/doi/abs/10.1177/0956797611430953},
  urldate = {2017-08-15},
  annotation = {00795}
}

@article{johnson_revised_2013,
  title = {Revised Standards for Statistical Evidence},
  author = {Johnson, V. E.},
  date = {2013-11-26},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {110},
  number = {48},
  pages = {19313--19317},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1313476110},
  url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1313476110},
  urldate = {2016-01-11},
  langid = {english},
  annotation = {00466}
}

@article{joseph_manipulation_2020,
  title = {The Manipulation of Affect: {{A}} Meta-Analysis of Affect Induction Procedures},
  shorttitle = {The Manipulation of Affect},
  author = {Joseph, Dana L. and Chan, Micaela Y. and Heintzelman, Samantha J. and Tay, Louis and Diener, Ed and Scotney, Victoria S.},
  date = {2020},
  journaltitle = {Psychological Bulletin},
  volume = {146},
  number = {4},
  pages = {355--375},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1939-1455(Electronic),0033-2909(Print)},
  doi = {10.1037/bul0000224},
  abstract = {Affect inductions have become essential for testing theories of affect and for conducting experimental research on the effects of mood and emotion. The current review takes stock of the vast body of existing literature on affect induction procedures (AIPs; also referred to as mood inductions) to evaluate the effectiveness of affect inductions as research tools and to test theories of affect (e.g., the bipolarity hypothesis, negativity bias, positivity offset, and theories of emotionality and gender) using meta-analytic data. In doing so, we seek to address whether AIPs are effective for inducing affective states, what conditions maximize their effectiveness, for which emotions they are most effective, for whom they are most effective, and whether affect induction findings can provide insight into theories of affect. A meta-analysis of 874 samples and 53,509 participants suggests that affect inductions are effective on average (δ = 1.32), but this effectiveness varies with the type of affect induction, the emotion being induced, and the gender of the participants. Further, results indicate coupled activation where the induction of positive (negative) emotions leads to a corresponding reduction in negative (positive) emotions, which provides support for the bipolar continuum of positive and negative affect. Results also revealed a negativity bias in which individuals display stronger reactions to negative stimuli than positive stimuli. A practical guide in the choice of affect induction procedures for researchers is presented and implications for emotion theory are discussed. (PsycINFO Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Emotional States,Emotionality (Personality),Emotions,Human Sex Differences,Negative Emotions,Negativism,Positive Emotions,Positivism,Theories}
}

@article{julious_sample_2004,
  title = {Sample Sizes for Clinical Trials with Normal Data},
  author = {Julious, Steven A.},
  date = {2004-06-30},
  journaltitle = {Statistics in Medicine},
  shortjournal = {Stat Med},
  volume = {23},
  number = {12},
  eprint = {15195324},
  eprinttype = {pmid},
  pages = {1921--1986},
  issn = {0277-6715},
  doi = {10.1002/sim.1783},
  abstract = {This article gives an overview of sample size calculations for parallel group and cross-over studies with Normal data. Sample size derivation is given for trials where the objective is to demonstrate: superiority, equivalence, non-inferiority, bioequivalence and estimation to a given precision, for different types I and II errors. It is demonstrated how the different trial objectives influence the null and alternative hypotheses of the trials and how these hypotheses influence the calculations. Sample size tables for the different types of trials and worked examples are given.},
  langid = {english},
  keywords = {Biometry,Cross-Over Studies,Humans,Randomized Controlled Trials as Topic,Research Design,Sample Size,Therapeutic equivalency},
  annotation = {00317}
}

@article{julious_sample_2005,
  title = {Sample Size of 12 per Group Rule of Thumb for a Pilot Study},
  author = {Julious, Steven A.},
  date = {2005},
  journaltitle = {Pharmaceutical Statistics},
  volume = {4},
  number = {4},
  pages = {287--291},
  issn = {1539-1612},
  doi = {10.1002/pst.185},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pst.185},
  urldate = {2020-12-16},
  abstract = {When designing a clinical trial an appropriate justification for the sample size should be provided in the protocol. However, there are a number of settings when undertaking a pilot trial when there is no prior information to base a sample size on. For such pilot studies the recommendation is a sample size of 12 per group. The justifications for this sample size are based on rationale about feasibility; precision about the mean and variance; and regulatory considerations. The context of the justifications are that future studies will use the information from the pilot in their design. Copyright © 2005 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {pilot study,sample size},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/pst.185}
}

@article{keefe_defining_2013,
  title = {Defining a {{Clinically Meaningful Effect}} for the {{Design}} and {{Interpretation}} of {{Randomized Controlled Trials}}},
  author = {Keefe, Richard S. E. and Kraemer, Helena C. and Epstein, Robert S. and Frank, Ellen and Haynes, Ginger and Laughren, Thomas P. and Mcnulty, James and Reed, Shelby D. and Sanchez, Juan and Leon, Andrew C.},
  date = {2013},
  journaltitle = {Innovations in Clinical Neuroscience},
  shortjournal = {Innov Clin Neurosci},
  volume = {10},
  eprint = {23882433},
  eprinttype = {pmid},
  pages = {4S-19S},
  issn = {2158-8333},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3719483/},
  urldate = {2018-03-10},
  abstract = {Objective: This article captures the proceedings of a meeting aimed at defining clinically meaningful effects for use in randomized controlled trials for psychopharmacological agents., Design: Experts from a variety of disciplines defined clinically meaningful effects from their perspectives along with viewpoints about how to design and interpret randomized controlled trials., Setting: The article offers relevant, practical, and sometimes anecdotal information about clinically meaningful effects and how to interpret them., Participants: The concept for this session was the work of co-chairs Richard Keefe and the late Andy Leon. Faculty included Richard Keefe, PhD; James McNulty, AbScB; Robert S. Epstein, MD, MS; Shelby D. Reed, PhD; Juan Sanchez, MD; Ginger Haynes, PhD; Andrew C. Leon, PhD; Helena Chmura Kraemer, PhD; Ellen Frank, PhD, and Kenneth L. Davis, MD., Results: The term clinically meaningful effect is an important aspect of designing and interpreting randomized controlled trials but can be particularly difficult in the setting of psychopharmacology where effect size may be modest, particularly over the short term, because of a strong response to placebo. Payers, regulators, patients, and clinicians have different concerns about clinically meaningful effects and may describe these terms differently. The use of moderators in success rate differences may help better delineate clinically meaningful effects., Conclusion: There is no clear consensus on a single definition for clinically meaningful differences in randomized controlled trials, and investigators must be sensitive to specific concerns of stakeholders in psychopharmacology in order to design and execute appropriate clinical trials.},
  issue = {5-6 Suppl A},
  pmcid = {PMC3719483},
  annotation = {00022}
}

@article{kelley_confidence_2007,
  title = {Confidence {{Intervals}} for {{Standardized Effect Sizes}}: {{Theory}}, {{Application}}, and {{Implementation}}},
  shorttitle = {Confidence {{Intervals}} for {{Standardized Effect Sizes}}},
  author = {Kelley, Ken},
  date = {2007},
  journaltitle = {Journal of Statistical Software},
  volume = {20},
  number = {8},
  issn = {1548-7660},
  doi = {10.18637/JSS.V020.I08},
  url = {http://www.jstatsoft.org/v20/i08/},
  urldate = {2018-07-19},
  abstract = {The behavioral, educational, and social sciences are undergoing a paradigmatic shift in methodology, from disciplines that focus on the dichotomous outcome of null hypothesis significance tests to disciplines that report and interpret effect sizes and their corresponding confidence intervals. Due to the arbitrariness of many measurement instruments used in the behavioral, educational, and social sciences, some of the most widely reported effect sizes are standardized. Although forming confidence intervals for standardized effect sizes can be very beneficial, such confidence interval procedures are generally difficult to implement because they depend on noncentral t, F , and χ2 distributions. At present, no main-stream statistical package provides exact confidence intervals for standardized effects without the use of specialized programming scripts. Methods for the Behavioral, Educational, and Social Sciences (MBESS) is an R package that has routines for calculating confidence intervals for noncentral t, F , and χ2 distributions, which are then used in the calculation of exact confidence intervals for standardized effect sizes by using the confidence interval transformation and inversion principles. The present article discusses the way in which confidence intervals are formed for standardized effect sizes and illustrates how such confidence intervals can be easily formed using MBESS in R.},
  langid = {english},
  annotation = {00163}
}

@article{kelley_effect_2012,
  title = {On Effect Size},
  author = {Kelley, Ken and Preacher, Kristopher J.},
  date = {2012},
  journaltitle = {Psychological methods},
  volume = {17},
  number = {2},
  pages = {137--152},
  publisher = {{American Psychological Association}},
  doi = {10.1037/a0028086}
}

@article{kelley_sample_2006,
  title = {Sample Size Planning for the Standardized Mean Difference: Accuracy in Parameter Estimation via Narrow Confidence Intervals.},
  shorttitle = {Sample Size Planning for the Standardized Mean Difference},
  author = {Kelley, Ken and Rausch, Joseph R.},
  date = {2006},
  journaltitle = {Psychological methods},
  volume = {11},
  number = {4},
  pages = {363--385},
  doi = {10.1037},
  url = {http://psycnet.apa.org/journals/met/11/4/363/},
  urldate = {2016-07-27},
  annotation = {00083}
}

@article{kenny_unappreciated_2019,
  title = {The Unappreciated Heterogeneity of Effect Sizes: {{Implications}} for Power, Precision, Planning of Research, and Replication},
  shorttitle = {The Unappreciated Heterogeneity of Effect Sizes},
  author = {Kenny, David A. and Judd, Charles M.},
  date = {2019},
  journaltitle = {Psychological Methods},
  volume = {24},
  number = {5},
  pages = {578--589},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1939-1463(Electronic),1082-989X(Print)},
  doi = {10.1037/met0000209},
  abstract = {Repeated investigations of the same phenomenon typically yield effect sizes that vary more than one would expect from sampling error alone. Such variation is even found in exact replication studies, suggesting that it is not only because of identifiable moderators but also to subtler random variation across studies. Such heterogeneity of effect sizes is typically ignored, with unfortunate consequences. We consider its implications for power analyses, the precision of estimated effects, and the planning of original and replication research. With heterogeneity and an interest in generalizing to a population of studies, the usual power calculations and confidence intervals are likely misleading, and the preference for single definitive large-N studies is misguided. Researchers and methodologists need to recognize that effects are often heterogeneous and plan accordingly. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {Confidence Limits (Statistics),Experimental Replication,Population (Statistics),Prediction,Statistical Power}
}

@article{kerr_harking_1998,
  title = {{{HARKing}}: {{Hypothesizing After}} the {{Results}} Are {{Known}}},
  shorttitle = {{{HARKing}}},
  author = {Kerr, Norbert L.},
  date = {1998-08-01},
  journaltitle = {Personality and Social Psychology Review},
  shortjournal = {Pers Soc Psychol Rev},
  volume = {2},
  number = {3},
  eprint = {15647155},
  eprinttype = {pmid},
  pages = {196--217},
  issn = {1088-8683, 1532-7957},
  doi = {10.1207/s15327957pspr0203_4},
  url = {http://psr.sagepub.com/content/2/3/196},
  urldate = {2016-03-25},
  abstract = {This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as if it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing's costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.},
  langid = {english},
  annotation = {00553}
}

@article{king_point_2011,
  title = {A Point of Minimal Important Difference ({{MID}}): A Critique of Terminology and Methods},
  shorttitle = {A Point of Minimal Important Difference ({{MID}})},
  author = {King, Madeleine T.},
  date = {2011-04-01},
  journaltitle = {Expert Review of Pharmacoeconomics \& Outcomes Research},
  volume = {11},
  number = {2},
  eprint = {21476819},
  eprinttype = {pmid},
  pages = {171--184},
  issn = {1473-7167},
  doi = {10.1586/erp.11.9},
  url = {https://doi.org/10.1586/erp.11.9},
  urldate = {2018-12-21},
  abstract = {The minimal important difference (MID) is a phrase with instant appeal in a field struggling to interpret health-related quality of life and other patient-reported outcomes. The terminology can be confusing, with several terms differing only slightly in definition (e.g., minimal clinically important difference, clinically important difference, minimally detectable difference, the subjectively significant difference), and others that seem similar despite having quite different meanings (minimally detectable difference versus minimum detectable change). Often, nuances of definition are of little consequence in the way that these quantities are estimated and used. Four methods are commonly employed to estimate MIDs: patient rating of change (global transition items); clinical anchors; standard error of measurement; and effect size. These are described and critiqued in this article. There is no universal MID, despite the appeal of the notion. Indeed, for a particular patient-reported outcome instrument or scale, the MID is not an immutable characteristic, but may vary by population and context. At both the group and individual level, the MID may depend on the clinical context and decision at hand, the baseline from which the patient starts, and whether they are improving or deteriorating. Specific estimates of MIDs should therefore not be overinterpreted. For a given health-related quality-of-life scale, all available MID estimates (and their confidence intervals) should be considered, amalgamated into general guidelines and applied judiciously to any particular clinical or research context.},
  keywords = {clinical significance,health-related quality of life,HRQOL,interpretation,MCID,MID,minimal clinically important difference,minimal important difference,patient-reported outcome,PRO},
  annotation = {00223}
}

@article{kirk_practical_1996,
  title = {Practical Significance: {{A}} Concept Whose Time Has Come},
  shorttitle = {Practical Significance},
  author = {Kirk, Roger E.},
  date = {1996},
  journaltitle = {Educational and psychological measurement},
  volume = {56},
  number = {5},
  pages = {746--759},
  publisher = {{Sage Publications Sage CA: Thousand Oaks, CA}}
}

@article{kirk_practical_1996-1,
  title = {Practical {{Significance}}: {{A Concept Whose Time Has Come}}},
  shorttitle = {Practical {{Significance}}},
  author = {Kirk, R. E.},
  date = {1996-10-01},
  journaltitle = {Educational and Psychological Measurement},
  volume = {56},
  number = {5},
  pages = {746--759},
  issn = {0013-1644},
  doi = {10.1177/0013164496056005002},
  url = {http://epm.sagepub.com/cgi/doi/10.1177/0013164496056005002},
  urldate = {2015-11-30},
  langid = {english},
  annotation = {01567}
}

@book{kish_survey_1965,
  title = {Survey {{Sampling}}},
  author = {Kish, Leslie},
  date = {1965},
  publisher = {{Wiley}},
  location = {{New York}}
}

@book{kitcher_advancement_1993,
  title = {The Advancement of Science: Science without Legend, Objectivity without Illusions},
  shorttitle = {The Advancement of Science},
  author = {Kitcher, Philip},
  date = {1993},
  publisher = {{Oxford University Press}},
  location = {{New York}},
  isbn = {978-0-19-504628-1},
  pagetotal = {421},
  keywords = {History,Philosophy,Science}
}

@article{kraft_interpreting_2020,
  title = {Interpreting Effect Sizes of Education Interventions},
  author = {Kraft, Matthew A.},
  date = {2020},
  journaltitle = {Educational Researcher},
  volume = {49},
  number = {4},
  pages = {241--253},
  publisher = {{SAGE Publications Sage CA: Los Angeles, CA}},
  doi = {10.3102/0013189X20912798}
}

@article{kruschke_bayesian_2011,
  title = {Bayesian Assessment of Null Values via Parameter Estimation and Model Comparison},
  author = {Kruschke, John K.},
  date = {2011},
  journaltitle = {Perspectives on Psychological Science},
  volume = {6},
  number = {3},
  pages = {299--312},
  annotation = {00203}
}

@article{kruschke_bayesian_2013,
  title = {Bayesian Estimation Supersedes the t Test.},
  author = {Kruschke, John K.},
  date = {2013},
  journaltitle = {Journal of Experimental Psychology: General},
  volume = {142},
  number = {2},
  pages = {573--603},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/a0029146},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0029146},
  urldate = {2015-12-01},
  langid = {english},
  annotation = {00483}
}

@article{kruschke_bayesian_2017,
  title = {The {{Bayesian New Statistics}}: {{Hypothesis}} Testing, Estimation, Meta-Analysis, and Power Analysis from a {{Bayesian}} Perspective},
  shorttitle = {The {{Bayesian New Statistics}}},
  author = {Kruschke, John and Liddell, Torrin M.},
  date = {2017-02-07},
  journaltitle = {Psychonomic Bulletin \& Review},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-016-1221-4},
  url = {http://link.springer.com/10.3758/s13423-016-1221-4},
  urldate = {2017-02-10},
  langid = {english},
  annotation = {00059}
}

@article{kruschke_rejecting_2018,
  title = {Rejecting or {{Accepting Parameter Values}} in {{Bayesian Estimation}}},
  author = {Kruschke, John K.},
  date = {2018-06-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  shortjournal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {2},
  pages = {270--280},
  issn = {2515-2459},
  doi = {10.1177/2515245918771304},
  url = {https://doi.org/10.1177/2515245918771304},
  urldate = {2019-04-24},
  abstract = {This article explains a decision rule that uses Bayesian posterior distributions as the basis for accepting or rejecting null values of parameters. This decision rule focuses on the range of plausible values indicated by the highest density interval of the posterior distribution and the relation between this range and a region of practical equivalence (ROPE) around the null value. The article also discusses considerations for setting the limits of a ROPE and emphasizes that analogous considerations apply to setting the decision thresholds for p values and Bayes factors.},
  langid = {english}
}

@article{kruschke_rejecting_2018-1,
  title = {Rejecting or {{Accepting Parameter Values}} in {{Bayesian Estimation}}},
  author = {Kruschke, John K.},
  date = {2018-06-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  shortjournal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {2},
  pages = {270--280},
  issn = {2515-2459},
  doi = {10.1177/2515245918771304},
  url = {https://doi.org/10.1177/2515245918771304},
  urldate = {2019-04-24},
  abstract = {This article explains a decision rule that uses Bayesian posterior distributions as the basis for accepting or rejecting null values of parameters. This decision rule focuses on the range of plausible values indicated by the highest density interval of the posterior distribution and the relation between this range and a region of practical equivalence (ROPE) around the null value. The article also discusses considerations for setting the limits of a ROPE and emphasizes that analogous considerations apply to setting the decision thresholds for p values and Bayes factors.},
  langid = {english}
}

@article{kvarven_comparing_2020,
  title = {Comparing Meta-Analyses and Preregistered Multiple-Laboratory Replication Projects},
  author = {Kvarven, Amanda and Strømland, Eirik and Johannesson, Magnus},
  date = {2020-04},
  journaltitle = {Nature Human Behaviour},
  volume = {4},
  number = {4},
  pages = {423--434},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-019-0787-z},
  url = {https://www.nature.com/articles/s41562-019-0787-z},
  urldate = {2021-02-09},
  abstract = {Many researchers rely on meta-analysis to summarize research evidence. However, there is a concern that publication bias and selective reporting may lead to biased meta-analytic effect sizes. We compare the results of meta-analyses to large-scale preregistered replications in psychology carried out at multiple laboratories. The multiple-laboratory replications provide precisely estimated effect sizes that do not suffer from publication bias or selective reporting. We searched the literature and identified 15\,meta-analyses on the same topics as multiple-laboratory replications. We find that meta-analytic effect sizes are significantly different from replication effect sizes for 12 out of the 15\,meta-replication pairs. These differences are systematic and, on average, meta-analytic effect sizes are almost three times as large as replication effect sizes. We also implement three methods of correcting meta-analysis for bias, but these methods do not substantively improve the meta-analytic results.},
  issue = {4},
  langid = {english}
}

@book{lakatos_methodology_1978,
  title = {The Methodology of Scientific Research Programmes: {{Volume}} 1: {{Philosophical}} Papers},
  shorttitle = {The Methodology of Scientific Research Programmes},
  author = {Lakatos, Imre},
  date = {1978},
  volume = {1},
  publisher = {{Cambridge University Press}}
}

@article{lakens_calculating_2013,
  title = {Calculating and Reporting Effect Sizes to Facilitate Cumulative Science: A Practical Primer for t-Tests and {{ANOVAs}}},
  shorttitle = {Calculating and Reporting Effect Sizes to Facilitate Cumulative Science},
  author = {Lakens, Daniël},
  date = {2013},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {4},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2013.00863},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00863/full},
  urldate = {2020-02-03},
  abstract = {Effect sizes are the most important outcome of empirical studies. Most articles on effect sizes highlight their importance to communicate the practical significance of results. For scientists themselves, effect sizes are most useful because they facilitate cumulative science. Effect sizes can be used to determine the sample size for follow-up studies, or examining effects across studies. This article aims to provide a practical primer on how to calculate and report effect sizes for t-tests and ANOVA’s such that effect sizes can be used in a-priori power analyses and meta-analyses. Whereas many articles about effect sizes focus on between-subjects designs and address within-subjects designs only briefly, I provide a detailed overview of the similarities and differences between within- and between-subjects designs. I suggest that some research questions in experimental psychology examine inherently intra-individual effects, which makes effect sizes that incorporate the correlation between measures the best summary of the results. Finally, a supplementary spreadsheet is provided to make it as easy as possible for researchers to incorporate effect size calculations into their workflow.},
  langid = {english},
  keywords = {Cohen's d,effect sizes,eta-squared,power analysis,sample size planning}
}

@article{lakens_equivalence_2017,
  title = {Equivalence {{Tests}}: {{A Practical Primer}} for t {{Tests}}, {{Correlations}}, and {{Meta-Analyses}}},
  shorttitle = {Equivalence {{Tests}}},
  author = {Lakens, Daniël},
  date = {2017},
  journaltitle = {Social Psychological and Personality Science},
  shortjournal = {Social Psychological and Personality Science},
  volume = {8},
  number = {4},
  pages = {355--362},
  issn = {1948-5506},
  doi = {10.1177/1948550617697177},
  url = {http://dx.doi.org/10.1177/1948550617697177},
  urldate = {2017-07-08},
  abstract = {Scientists should be able to provide support for the absence of a meaningful effect. Currently, researchers often incorrectly conclude an effect is absent based a nonsignificant result. A widely recommended approach within a frequentist framework is to test for equivalence. In equivalence tests, such as the two one-sided tests (TOST) procedure discussed in this article, an upper and lower equivalence bound is specified based on the smallest effect size of interest. The TOST procedure can be used to statistically reject the presence of effects large enough to be considered worthwhile. This practical primer with accompanying spreadsheet and R package enables psychologists to easily perform equivalence tests (and power analyses) by setting equivalence bounds based on standardized effect sizes and provides recommendations to prespecify equivalence bounds. Extending your statistical tool kit with equivalence tests is an easy way to improve your statistical and theoretical inferences.},
  langid = {english}
}

@article{lakens_equivalence_2018,
  title = {Equivalence Testing for Psychological Research: {{A}} Tutorial},
  shorttitle = {Equivalence {{Testing}} for {{Psychological Research}}},
  author = {Lakens, Daniël and Scheel, Anne M. and Isager, Peder M.},
  date = {2018},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  shortjournal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {2},
  pages = {259--269},
  issn = {2515-2459},
  doi = {10.1177/2515245918770963},
  url = {https://doi.org/10.1177/2515245918770963},
  urldate = {2018-11-30},
  abstract = {Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests (TOST) procedure to test for equivalence and reject the presence of a smallest effect size of interest (SESOI). The TOST procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the SESOI exists. We explain a range of approaches to determine the SESOI in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects.},
  langid = {english}
}

@article{lakens_improving_2020,
  title = {Improving {{Transparency}}, {{Falsifiability}}, and {{Rigour}} by {{Making Hypothesis Tests Machine Readable}}},
  author = {Lakens, Daniël and DeBruine, Lisa},
  date = {2020-01-27T18:39:51},
  doi = {10.31234/osf.io/5xcda},
  url = {https://psyarxiv.com/5xcda/},
  urldate = {2020-01-31},
  abstract = {Making scientific information machine-readable greatly facilitates its re-use. Many scientific articles have the goal to test a hypothesis, and making the tests of statistical predictions easier to find and access could be very beneficial. We propose an approach that can be used to make hypothesis tests machine readable. We believe there are two benefits to specifying a hypothesis test in a way that a computer can evaluate whether the statistical prediction is corroborated or not. First, hypothesis test will become more transparent, falsifiable, and rigorous. Second, scientists will benefit if information related to hypothesis tests in scientific articles is easily findable and re-usable, for example when performing meta-analyses, during peer review, and when examining meta-scientific research questions. We examine what a machine readable hypothesis test should looks like, and demonstrate the feasibility of machine readable hypothesis tests in a real-life example.}
}

@article{lakens_improving_2020-1,
  title = {Improving {{Inferences About Null Effects With Bayes Factors}} and {{Equivalence Tests}}},
  author = {Lakens, Daniël and McLatchie, Neil and Isager, Peder M. and Scheel, Anne M. and Dienes, Zoltan},
  date = {2020-01-01},
  journaltitle = {The Journals of Gerontology: Series B},
  shortjournal = {J Gerontol B Psychol Sci Soc Sci},
  volume = {75},
  number = {1},
  pages = {45--57},
  publisher = {{Oxford Academic}},
  issn = {1079-5014},
  doi = {10.1093/geronb/gby065},
  url = {https://academic.oup.com/psychsocgerontology/article/75/1/45/5033832},
  urldate = {2020-07-01},
  abstract = {Abstract.  Researchers often conclude an effect is absent when a null-hypothesis significance test yields a nonsignificant p value. However, it is neither logic},
  langid = {english}
}

@article{lakens_justify_2018,
  title = {Justify Your Alpha},
  author = {Lakens, Daniël and Adolfi, Federico G. and Albers, Casper J. and Anvari, Farid and Apps, Matthew A. J. and Argamon, Shlomo E. and Baguley, Thom and Becker, Raymond B. and Benning, Stephen D. and Bradford, Daniel E. and Buchanan, Erin M. and Caldwell, Aaron R. and Calster, Ben and Carlsson, Rickard and Chen, Sau-Chin and Chung, Bryan and Colling, Lincoln J. and Collins, Gary S. and Crook, Zander and Cross, Emily S. and Daniels, Sameera and Danielsson, Henrik and DeBruine, Lisa and Dunleavy, Daniel J. and Earp, Brian D. and Feist, Michele I. and Ferrell, Jason D. and Field, James G. and Fox, Nicholas W. and Friesen, Amanda and Gomes, Caio and Gonzalez-Marquez, Monica and Grange, James A. and Grieve, Andrew P. and Guggenberger, Robert and Grist, James and Harmelen, Anne-Laura and Hasselman, Fred and Hochard, Kevin D. and Hoffarth, Mark R. and Holmes, Nicholas P. and Ingre, Michael and Isager, Peder M. and Isotalus, Hanna K. and Johansson, Christer and Juszczyk, Konrad and Kenny, David A. and Khalil, Ahmed A. and Konat, Barbara and Lao, Junpeng and Larsen, Erik Gahner and Lodder, Gerine M. A. and Lukavský, Jiří and Madan, Christopher R. and Manheim, David and Martin, Stephen R. and Martin, Andrea E. and Mayo, Deborah G. and McCarthy, Randy J. and McConway, Kevin and McFarland, Colin and Nio, Amanda Q. X. and Nilsonne, Gustav and Oliveira, Cilene Lino and Xivry, Jean-Jacques Orban and Parsons, Sam and Pfuhl, Gerit and Quinn, Kimberly A. and Sakon, John J. and Saribay, S. Adil and Schneider, Iris K. and Selvaraju, Manojkumar and Sjoerds, Zsuzsika and Smith, Samuel G. and Smits, Tim and Spies, Jeffrey R. and Sreekumar, Vishnu and Steltenpohl, Crystal N. and Stenhouse, Neil and Świątkowski, Wojciech and Vadillo, Miguel A. and Assen, Marcel A. L. M. and Williams, Matt N. and Williams, Samantha E. and Williams, Donald R. and Yarkoni, Tal and Ziano, Ignazio and Zwaan, Rolf A.},
  date = {2018-02-26},
  journaltitle = {Nature Human Behaviour},
  volume = {2},
  pages = {168--171},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0311-x},
  url = {https://www.nature.com/articles/s41562-018-0311-x},
  urldate = {2018-02-28},
  abstract = {In response to recommendations to redefine statistical significance to P ≤ 0.005, we propose that researchers should transparently report and justify all choices they make when designing a study, including the alpha level.},
  langid = {english}
}

@article{lakens_performing_2014,
  title = {Performing High-Powered Studies Efficiently with Sequential Analyses: {{Sequential}} Analyses},
  shorttitle = {Performing High-Powered Studies Efficiently with Sequential Analyses},
  author = {Lakens, Daniël},
  date = {2014},
  journaltitle = {European Journal of Social Psychology},
  volume = {44},
  number = {7},
  pages = {701--710},
  issn = {00462772},
  doi = {10.1002/ejsp.2023},
  url = {http://doi.wiley.com/10.1002/ejsp.2023},
  urldate = {2015-11-30},
  langid = {english}
}

@article{lakens_reproducibility_2016,
  title = {On the Reproducibility of Meta-Analyses: Six Practical Recommendations},
  shorttitle = {On the Reproducibility of Meta-Analyses},
  author = {Lakens, Daniël and Hilgard, Joe and Staaks, Janneke},
  date = {2016},
  journaltitle = {BMC Psychology},
  shortjournal = {BMC Psychology},
  volume = {4},
  pages = {24},
  issn = {2050-7283},
  doi = {10.1186/s40359-016-0126-3},
  url = {http://dx.doi.org/10.1186/s40359-016-0126-3},
  urldate = {2016-06-07},
  abstract = {Meta-analyses play an important role in cumulative science by combining information across multiple studies and attempting to provide effect size estimates corrected for publication bias. Research on the reproducibility of meta-analyses reveals that errors are common, and the percentage of effect size calculations that cannot be reproduced is much higher than is desirable. Furthermore, the flexibility in inclusion criteria when performing a meta-analysis, combined with the many conflicting conclusions drawn by meta-analyses of the same set of studies performed by different researchers, has led some people to doubt whether meta-analyses can provide objective conclusions.},
  keywords = {Meta-analysis,Open science,Reporting guidelines,Reproducibility}
}

@report{lakens_simulation-based_2019,
  title = {Simulation-{{Based Power-Analysis}} for {{Factorial ANOVA Designs}}},
  author = {Lakens, Daniël and Caldwell, Aaron R.},
  date = {2019-05-28T18:42:32},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/baxsf},
  url = {https://psyarxiv.com/baxsf/},
  urldate = {2020-12-15},
  abstract = {Researchers often rely on analysis of variance (ANOVA) when they report results of experiments. To ensure a study is adequately powered to yield informative results when performing an ANOVA, researchers can perform an a-priori power analysis. However, power analysis for factorial ANOVA designs is often a challenge. Current software solutions do not allow power analyses for complex designs with several within-subject factors. Moreover, power analyses often need partial eta-squared or Cohen's \$f\$ as input, but these effect sizes are not intuitive and do not generalize to different experimental designs. We have created the R package Superpower and online Shiny apps to enable researchers without extensive programming experience to perform simulation-based power analysis for ANOVA designs of up to three within- or between-subject factors. Predicted effects are entered by specifying means, standard deviations, and for within-subject factors the correlations. The simulation provides the statistical power for all ANOVA main effects, interactions, and individual comparisons, and allows researchers to correct for multiple comparisons. The software can plot power across a range of sample sizes, can control error rates for multiple comparisons, and can compute power when the homogeneity or sphericity assumptions are violated. This tutorial will demonstrate how to perform a-priori power analysis to design informative studies for main effects, interactions, and individual comparisons, and highlights important factors that determine the statistical power for factorial ANOVA designs.},
  keywords = {ANOVA,Experimental Design and Sample Surveys,hypothesis test,power analysis,Quantitative Methods,sample size justification,Social and Behavioral Sciences,Statistical Methods}
}

@article{lakens_too_2017,
  title = {Too {{True}} to Be {{Bad}}: {{When Sets}} of {{Studies With Significant}} and {{Nonsignificant Findings Are Probably True}}},
  shorttitle = {Too {{True}} to Be {{Bad}}},
  author = {Lakens, Daniël and Etz, Alexander J.},
  date = {2017},
  journaltitle = {Social Psychological and Personality Science},
  shortjournal = {Social Psychological and Personality Science},
  volume = {8},
  number = {8},
  pages = {875--881},
  issn = {1948-5506},
  doi = {10.1177/1948550617693058},
  url = {https://doi.org/10.1177/1948550617693058},
  urldate = {2018-02-28},
  abstract = {Psychology journals rarely publish nonsignificant results. At the same time, it is often very unlikely (or “too good to be true”) that a set of studies yields exclusively significant results. Here, we use likelihood ratios to explain when sets of studies that contain a mix of significant and nonsignificant results are likely to be true or “too true to be bad.” As we show, mixed results are not only likely to be observed in lines of research but also, when observed, often provide evidence for the alternative hypothesis, given reasonable levels of statistical power and an adequately controlled low Type 1 error rate. Researchers should feel comfortable submitting such lines of research with an internal meta-analysis for publication. A better understanding of probabilities, accompanied by more realistic expectations of what real sets of studies look like, might be an important step in mitigating publication bias in the scientific literature.},
  langid = {english}
}

@article{lakens_value_2020,
  title = {The {{Value}} of {{Preregistration}} for {{Psychological Science}}: {{A Conceptual Analysis}}},
  shorttitle = {The {{Value}} of {{Preregistration}} for {{Psychological Science}}},
  author = {Lakens, Daniël},
  date = {2020},
  journaltitle = {Japanese Psychological Review},
  doi = {10.31234/osf.io/jbh4w},
  url = {https://psyarxiv.com/jbh4w/},
  urldate = {2019-11-19},
  abstract = {For over two centuries researchers have been criticized for using research practices that makes it easier to present data in line with what they wish to be true. With the rise of the internet it has become easier to preregister the theoretical and empirical basis for predictions, the experimental design, the materials, and the analysis code. Whether the practice of preregistration is valuable depends on your philosophy of science. Here, I provide a conceptual analysis of the value of preregistration for psychological science from an error statistical philosophy (Mayo, 2018). Preregistration has the goal to allow others to transparently evaluate the capacity of a test to falsify a prediction, or the severity of a test. Researchers who aim to test predictions with severity should find value in the practice of preregistration. I differentiate the goal of preregistration from positive externalities, discuss how preregistration itself does not make a study better or worse compared to a non-preregistered study, and highlight the importance of evaluating the usefulness of a tool such as preregistration based on an explicit consideration of your philosophy of science.}
}

@article{lan_discrete_1983,
  title = {Discrete {{Sequential Boundaries}} for {{Clinical Trials}}},
  author = {Lan, K. K. Gordon and DeMets, David L.},
  date = {1983-12},
  journaltitle = {Biometrika},
  volume = {70},
  number = {3},
  eprint = {2336502},
  eprinttype = {jstor},
  pages = {659},
  issn = {00063444},
  doi = {10.2307/2336502},
  annotation = {01888}
}

@book{leamer_specification_1978,
  title = {Specification {{Searches}}: {{Ad Hoc Inference}} with {{Nonexperimental Data}}},
  shorttitle = {Specification {{Searches}}},
  author = {Leamer, Edward E.},
  date = {1978-04-24},
  edition = {1 edition},
  publisher = {{Wiley}},
  location = {{New York usw.}},
  abstract = {Offers a radically new approach to inference with nonexperimental data when the statistical model is ambiguously defined. Examines the process of model searching and its implications for inference. Identifies six different varieties of specification searches, discussing the inferential consequences of each in detail.},
  isbn = {978-0-471-01520-8},
  langid = {english},
  pagetotal = {370}
}

@article{lenth_post_2007,
  title = {Post Hoc Power: Tables and Commentary},
  shorttitle = {Post Hoc Power},
  author = {Lenth, Russell V.},
  date = {2007},
  journaltitle = {Iowa City: Department of Statistics and Actuarial Science, University of Iowa},
  url = {https://pdfs.semanticscholar.org/fbfb/cab4b59e54c6a3ed39ba3656f35ef86c5ee3.pdf},
  urldate = {2017-09-30}
}

@article{lenth_practical_2001,
  title = {Some Practical Guidelines for Effective Sample Size Determination},
  author = {Lenth, Russell V.},
  date = {2001},
  journaltitle = {The American Statistician},
  volume = {55},
  number = {3},
  pages = {187--193},
  doi = {10.1198/000313001317098149},
  url = {http://www.tandfonline.com/doi/abs/10.1198/000313001317098149},
  urldate = {2017-09-30}
}

@article{leon_role_2011,
  title = {The {{Role}} and {{Interpretation}} of {{Pilot Studies}} in {{Clinical Research}}},
  author = {Leon, Andrew C. and Davis, Lori L. and Kraemer, Helena C.},
  date = {2011-05},
  journaltitle = {Journal of psychiatric research},
  shortjournal = {J Psychiatr Res},
  volume = {45},
  number = {5},
  eprint = {21035130},
  eprinttype = {pmid},
  pages = {626--629},
  issn = {0022-3956},
  doi = {10.1016/j.jpsychires.2010.10.008},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3081994/},
  urldate = {2016-02-23},
  abstract = {Pilot studies represent a fundamental phase of the research process. The purpose of conducting a pilot study is to examine the feasibility of an approach that is intended to be used in a larger scale study. The roles and limitations of pilot studies are described here using a clinical trial as an example. A pilot study can be used to evaluate the feasibility of recruitment, randomization, retention, assessment procedures, new methods, and implementation of the novel intervention., A pilot study is not a hypothesis testing study. Safety, efficacy and effectiveness are not evaluated in a pilot. Contrary to tradition, a pilot study does not provide a meaningful effect size estimate for planning subsequent studies due to the imprecision inherent in data from small samples. Feasibility results do not necessarily generalize beyond the inclusion and exclusion criteria of the pilot design., A pilot study is a requisite initial step in exploring a novel intervention or an innovative application of an intervention. Pilot results can inform feasibility and identify modifications needed in the design of a larger, ensuing hypothesis testing study. Investigators should be forthright in stating these objectives of a pilot study. Grant reviewers and other stakeholders should expect no more.},
  pmcid = {PMC3081994}
}

@article{leys_how_2019,
  title = {How to {{Classify}}, {{Detect}}, and {{Manage Univariate}} and {{Multivariate Outliers}}, {{With Emphasis}} on {{Pre-Registration}}},
  author = {Leys, Christophe and Delacre, Marie and Mora, Youri L. and Lakens, Daniël and Ley, Christophe},
  date = {2019-04-30},
  journaltitle = {International Review of Social Psychology},
  volume = {32},
  number = {1},
  pages = {5},
  issn = {2397-8570},
  doi = {10.5334/irsp.289},
  url = {http://www.rips-irsp.com/articles/10.5334/irsp.289/},
  urldate = {2019-06-19},
  abstract = {Researchers often lack knowledge about how to deal with outliers when analyzing their data. Even more frequently, researchers do not pre-specify how they plan to manage outliers. In this paper we aim to improve research practices by outlining what you need to know about outliers. We start by providing a functional definition of outliers. We then lay down an appropriate nomenclature/classification of outliers.~This nomenclature is used to understand what kinds of outliers can be encountered and serves as a guideline to make appropriate decisions regarding the conservation, deletion, or recoding of outliers. These decisions might impact the validity of statistical inferences as well as the reproducibility of our experiments. To be able to make informed decisions about outliers you first need proper detection tools. We remind readers why the most common outlier detection methods are problematic and recommend the use of the median absolute deviation to detect univariate outliers, and of the Mahalanobis-MCD distance to detect multivariate outliers. An R package was created that can be used to easily perform these detection tests. Finally, we promote the use of pre-registration to avoid flexibility in data analysis when handling outliers. ~ Publishers note: due to a typesetting error, this paper was originally published with incorrect table numbering, where tables 2, 3, and 4 were incorrectly labelled. This was corrected soon after publication.},
  langid = {english},
  keywords = {Malahanobis distance,median absolute deviation,minimum covariance determinant,outliers,preregistration,robust detection}
}

@article{lindsay_replication_2015,
  title = {Replication in {{Psychological Science}}},
  author = {Lindsay, D. Stephen},
  date = {2015-12-01},
  journaltitle = {Psychological Science},
  shortjournal = {Psychological Science},
  volume = {26},
  number = {12},
  eprint = {26553013},
  eprinttype = {pmid},
  pages = {1827--1832},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797615616374},
  url = {http://pss.sagepub.com/content/26/12/1827},
  urldate = {2016-03-25},
  langid = {english}
}

@article{lovakov_empirically_2017,
  title = {Empirically {{Derived Guidelines}} for {{Interpreting Effect Size}} in {{Social Psychology}}},
  author = {Lovakov, Andrey and Agadullina, Elena},
  date = {2017-11-27},
  journaltitle = {PsyArXiv},
  doi = {10.17605/OSF.IO/2EPC4},
  url = {https://psyarxiv.com/2epc4/},
  urldate = {2017-11-27},
  abstract = {A number of recent research publications have shown that commonly used guidelines for interpreting effect sizes suggested by Cohen (1988) do not fit well with the empirical distribution of those effect sizes, and tend to overestimate them in many research areas. This study proposes empirically derived guidelines for interpreting effect sizes for research in social psychology, based on analysis of the true distributions of the two types of effect size measures widely used in social psychology (correlation coefficient and standardized mean differences). Analysis was carried out on the empirical distribution of 9884 correlation coefficients and 3580 Hedges’ g statistics extracted from studies included in 98 published meta-analyses. The analysis reveals that the 25th, 50th, and 75th percentiles corresponded to correlation coefficients values of 0.12, 0.25, and 0.42 and to Hedges’ g values of 0.15, 0.38, and 0.69, respectively. This suggests that Cohen’s guidelines tend to overestimate medium and large effect sizes. It is recommended that correlation coefficients of 0.10, 0.25, and 0.40 and Hedges’ g of 0.15, 0.40, and 0.70 should be interpreted as small, medium, and large effects for studies in social psychology. The analysis also shows that more than half of all studies lack sufficient sample size to detect a medium effect. This paper reports the sample sizes required to achieve appropriate statistical power for the identification of small, medium, and large effects. This can be used for performing appropriately powered future studies when information about exact effect size is not available.}
}

@article{mahoney_publication_1977,
  title = {Publication Prejudices: {{An}} Experimental Study of Confirmatory Bias in the Peer Review System},
  shorttitle = {Publication Prejudices},
  author = {Mahoney, Michael J.},
  date = {1977-06-01},
  journaltitle = {Cognitive Therapy and Research},
  shortjournal = {Cogn Ther Res},
  volume = {1},
  number = {2},
  pages = {161--175},
  issn = {1573-2819},
  doi = {10.1007/BF01173636},
  url = {https://doi.org/10.1007/BF01173636},
  urldate = {2019-08-03},
  abstract = {Confirmatory bias is the tendency to emphasize and believe experiences which support one's views and to ignore or discredit those which do not. The effects of this tendency have been repeatedly documented in clinical research. However, its ramifications for the behavior of scientists have yet to be adequately explored. For example, although publication is a critical element in determining the contribution and impact of scientific findings, little research attention has been devoted to the variables operative in journal review policies. In the present study, 75 journal reviewers were asked to referee manuscripts which described identical experimental procedures but which reported positive, negative, mixed, or no results. In addition to showing poor interrater agreement, reviewers were strongly biased against manuscripts which reported results contrary to their theoretical perspective. The implications of these findings for epistemology and the peer review system are briefly addressed.},
  langid = {english},
  keywords = {Clinical Research,Cognitive Psychology,Experimental Study,Review System,Theoretical Perspective}
}

@article{marshall_does_2013,
  title = {Does {{Sample Size Matter}} in {{Qualitative Research}}?: {{A Review}} of {{Qualitative Interviews}} in Is {{Research}}},
  shorttitle = {Does {{Sample Size Matter}} in {{Qualitative Research}}?},
  author = {Marshall, Bryan and Cardon, Peter and Poddar, Amit and Fontenot, Renee},
  date = {2013-09-01},
  journaltitle = {Journal of Computer Information Systems},
  volume = {54},
  number = {1},
  pages = {11--22},
  publisher = {{Taylor \& Francis}},
  issn = {0887-4417},
  doi = {10.1080/08874417.2013.11645667},
  url = {https://doi.org/10.1080/08874417.2013.11645667},
  urldate = {2020-12-31},
  abstract = {This study examines 83 IS qualitative studies in leading IS journals for the following purposes: (a) identifying the extent to which IS qualitative studies employ best practices of justifying sample size; (b) identifying optimal ranges of interviews for various types of qualitative research; and (c) identifying the extent to which cultural factors (such as journal of publication, number of authors, world region) impact sample size of interviews. Little or no rigor for justifying sample size was shown for virtually all of the IS studies in this dataset. Furthermore, the number of interviews conducted for qualitative studies is correlated with cultural factors, implying the subjective nature of sample size in qualitative IS studies. Recommendations are provided for minimally acceptable practices of justifying sample size of interviews in qualitative IS studies.},
  keywords = {data saturation,qualitative interviews,qualitative methodology,sample size},
  annotation = {\_eprint: https://doi.org/10.1080/08874417.2013.11645667}
}

@book{maxwell_designing_2017,
  title = {Designing {{Experiments}} and {{Analyzing Data}}: {{A Model Comparison Perspective}}, {{Third Edition}}},
  shorttitle = {Designing {{Experiments}} and {{Analyzing Data}}},
  author = {Maxwell, Scott E. and Delaney, Harold D. and Kelley, Ken},
  date = {2017-08-02},
  edition = {3 edition},
  publisher = {{Routledge}},
  location = {{New York, NY}},
  abstract = {Designing Experiments and Analyzing Data: A Model Comparison Perspective (3rd edition) offers an integrative conceptual framework for understanding experimental design and data analysis. Maxwell, Delaney, and Kelley first apply fundamental principles to simple experimental designs followed by an application of the same principles to more complicated designs. Their integrative conceptual framework better prepares readers to understand the logic behind a general strategy of data analysis that is appropriate for a wide variety of designs, which allows for the introduction of more complex topics that are generally omitted from other books. Numerous pedagogical features further facilitate understanding: examples of published research demonstrate the applicability of each chapter’s content; flowcharts assist in choosing the most appropriate procedure; end-of-chapter lists of important formulas highlight key ideas and assist readers in locating the initial presentation of equations; useful programming code and tips are provided throughout the book and in associated resources available online, and extensive sets of exercises help develop a deeper understanding of the subject. Detailed solutions for some of the exercises and realistic data sets are included on the website (DesigningExperiments.com). The pedagogical approach used throughout the book enables readers to gain an overview of experimental design, from conceptualization of the research question to analysis of the data. The book and its companion website with web apps, tutorials, and detailed code are ideal for students and researchers seeking the optimal way to design their studies and analyze the resulting data.},
  isbn = {978-1-138-89228-6},
  langid = {english},
  pagetotal = {1080},
  annotation = {00000}
}

@incollection{maxwell_ethics_2011,
  title = {Ethics and Sample Size Planning},
  booktitle = {Handbook of Ethics in Quantitative Methodology},
  author = {Maxwell, Scott E. and Kelley, Ken},
  date = {2011},
  pages = {179--204},
  publisher = {{Routledge}}
}

@article{maxwell_sample_2008,
  title = {Sample {{Size Planning}} for {{Statistical Power}} and {{Accuracy}} in {{Parameter Estimation}}},
  author = {Maxwell, Scott E. and Kelley, Ken and Rausch, Joseph R.},
  date = {2008-01},
  journaltitle = {Annual Review of Psychology},
  volume = {59},
  number = {1},
  pages = {537--563},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev.psych.59.103006.093735},
  url = {http://www.annualreviews.org/doi/abs/10.1146/annurev.psych.59.103006.093735},
  urldate = {2015-11-30},
  langid = {english}
}

@book{mayo_statistical_2018,
  title = {Statistical Inference as Severe Testing: How to Get beyond the Statistics Wars},
  shorttitle = {Statistical Inference as Severe Testing},
  author = {Mayo, Deborah G.},
  date = {2018},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}},
  isbn = {978-1-107-05413-4},
  langid = {english},
  keywords = {Deviation (Mathematics),Error analysis (Mathematics),Fallacies (Logic),Inference,Mathematical statistics},
  annotation = {00004}
}

@article{mcgraw_common_1992,
  title = {A Common Language Effect Size Statistic},
  author = {McGraw, Kenneth O. and Wong, S. P.},
  date = {1992},
  journaltitle = {Psychological Bulletin},
  volume = {111},
  number = {2},
  pages = {361--365},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1939-1455(Electronic),0033-2909(Print)},
  doi = {10.1037/0033-2909.111.2.361},
  abstract = {Some of the shortcomings in interpretability and generalizability of the effect size statistics currently available to researchers can be overcome by a statistic that expresses how often a score sampled from one distribution will be greater than a score sampled from another distribution. The statistic, the common language effect size indicator, is easily calculated from sample means and variances (or from proportions in the case of nominal-level data). It can be used for expressing the effect observed in both independent and related sample designs and in both 2-group and n-group designs. Empirical tests show it to be robust to violations of the normality assumption, particularly when the variances in the 2 parent distributions are equal. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Effect Size (Statistical)}
}

@article{mcintosh_power_2020,
  title = {Power Calculations in Single Case Neuropsychology},
  author = {McIntosh, Robert D. and Rittmo, Jonathan Ö},
  date = {2020-09-02T06:47:32},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/fxz49},
  url = {https://psyarxiv.com/fxz49/},
  urldate = {2021-01-07},
  abstract = {Researchers and clinicians in neuropsychology often compare individual patients against healthy control samples, to quantify evidence for cognitive-behavioural deficits and dissociations. Statistical methods for these comparisons have been developed that control Type I (false positive) errors effectively. However, remarkably little attention has been given to the power of these tests. In this practical primer, we describe, in minimally technical terms, the origins and limits of power for case-control comparisons. We argue that power calculations can play useful roles in single-case study design and interpretation, and we make suggestions for optimising power in practice. As well as providing figures, tables and tools for estimating the power of case-control comparisons, we hope to assist researchers in setting realistic expectations for what such tests can achieve in general.},
  keywords = {Behavioral Neuroscience,Clinical Neuroscience,Cognitive Neuroscience,deficit,dissociation,Neuroscience,power,single-case,statistical methods}
}

@article{meehl_appraising_1990,
  title = {Appraising and Amending Theories: {{The}} Strategy of {{Lakatosian}} Defense and Two Principles That Warrant It},
  shorttitle = {Appraising and Amending Theories},
  author = {Meehl, Paul E.},
  date = {1990},
  journaltitle = {Psychological Inquiry},
  volume = {1},
  number = {2},
  pages = {108--141},
  doi = {10.1207/s15327965pli0102_1},
  url = {http://www.tandfonline.com/doi/pdf/10.1207/s15327965pli0102_1},
  urldate = {2016-04-29}
}

@article{meehl_theory-testing_1967,
  title = {Theory-Testing in Psychology and Physics: {{A}} Methodological Paradox},
  shorttitle = {Theory-Testing in Psychology and Physics},
  author = {Meehl, Paul E.},
  date = {1967},
  journaltitle = {Philosophy of science},
  eprint = {186099},
  eprinttype = {jstor},
  pages = {103--115}
}

@article{meyners_equivalence_2012,
  title = {Equivalence Tests – {{A}} Review},
  author = {Meyners, Michael},
  date = {2012-12},
  journaltitle = {Food Quality and Preference},
  volume = {26},
  number = {2},
  pages = {231--245},
  issn = {09503293},
  doi = {10.1016/j.foodqual.2012.05.003},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0950329312000961},
  urldate = {2016-08-06},
  langid = {english}
}

@article{meyvis_increasing_2018,
  title = {Increasing the {{Power}} of {{Your Study}} by {{Increasing}} the {{Effect Size}}},
  author = {Meyvis, Tom and Van Osselaer, Stijn M J},
  date = {2018-02-01},
  journaltitle = {Journal of Consumer Research},
  shortjournal = {Journal of Consumer Research},
  volume = {44},
  number = {5},
  pages = {1157--1173},
  issn = {0093-5301},
  doi = {10.1093/jcr/ucx110},
  url = {https://doi.org/10.1093/jcr/ucx110},
  urldate = {2020-12-30},
  abstract = {As in other social sciences, published findings in consumer research tend to overestimate the size of the effect being investigated, due to both file drawer effects and abuse of researcher degrees of freedom, including opportunistic analysis decisions. Given that most effect sizes are substantially smaller than would be apparent from published research, there has been a widespread call to increase power by increasing sample size. We propose that, aside from increasing sample size, researchers can also increase power by boosting the effect size. If done correctly, removing participants, using covariates, and optimizing experimental designs, stimuli, and measures can boost effect size without inflating researcher degrees of freedom. In fact, careful planning of studies and analyses to maximize effect size is essential to be able to study many psychologically interesting phenomena when massive sample sizes are not feasible.}
}

@article{milgram_maintaining_1978,
  title = {On Maintaining Urban Norms: {{A}} Field Experiment in the Subway},
  shorttitle = {On Maintaining Urban Norms},
  author = {Milgram, Stanley and Sabini, John},
  date = {1978},
  journaltitle = {Advances in environmental psychology},
  volume = {1},
  pages = {31--40}
}

@book{millar_maximum_2011,
  title = {Maximum Likelihood Estimation and Inference: With Examples in {{R}}, {{SAS}}, and {{ADMB}}},
  shorttitle = {Maximum Likelihood Estimation and Inference},
  author = {Millar, R. B.},
  date = {2011},
  series = {Statistics in Practice},
  publisher = {{Wiley}},
  location = {{Chichester, West Sussex}},
  isbn = {978-0-470-09482-2},
  pagetotal = {357},
  keywords = {Chance,Estimation theory,Mathematical models}
}

@article{miller_quest_2019,
  title = {The Quest for an Optimal Alpha},
  author = {Miller, Jeff and Ulrich, Rolf},
  date = {2019-01-02},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {14},
  number = {1},
  pages = {e0208631},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0208631},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0208631},
  urldate = {2019-01-04},
  abstract = {Researchers who analyze data within the framework of null hypothesis significance testing must choose a critical “alpha” level, α, to use as a cutoff for deciding whether a given set of data demonstrates the presence of a particular effect. In most fields, α = 0.05 has traditionally been used as the standard cutoff. Many researchers have recently argued for a change to a more stringent evidence cutoff such as α = 0.01, 0.005, or 0.001, noting that this change would tend to reduce the rate of false positives, which are of growing concern in many research areas. Other researchers oppose this proposed change, however, because it would correspondingly tend to increase the rate of false negatives. We show how a simple statistical model can be used to explore the quantitative tradeoff between reducing false positives and increasing false negatives. In particular, the model shows how the optimal α level depends on numerous characteristics of the research area, and it reveals that although α = 0.05 would indeed be approximately the optimal value in some realistic situations, the optimal α could actually be substantially larger or smaller in other situations. The importance of the model lies in making it clear what characteristics of the research area have to be specified to make a principled argument for using one α level rather than another, and the model thereby provides a blueprint for researchers seeking to justify a particular α level.},
  langid = {english},
  keywords = {Decision theory,Economic growth,Health economics,Medicine and health sciences,Psychology,Publication ethics,Statistical methods,Statistical models},
  annotation = {00000}
}

@article{mitroff_systemic_1974,
  title = {On Systemic Problem Solving and the Error of the Third Kind},
  author = {Mitroff, Ian I. and Featheringham, Tom R.},
  date = {1974-11},
  journaltitle = {Behavioral Science},
  volume = {19},
  number = {6},
  pages = {383--393},
  issn = {00057940, 10991743},
  doi = {10/cjqj8h},
  url = {http://doi.wiley.com/10.1002/bs.3830190605},
  urldate = {2019-01-20},
  langid = {english},
  annotation = {00292}
}

@online{morey_power_2020,
  type = {blog},
  title = {Power and Precision},
  author = {Morey, Richard D.},
  date = {2020-06-12},
  url = {https://medium.com/@richarddmorey/power-and-precision-47f644ddea5e},
  abstract = {Why the push for replacing “power” with “precision” is misguided}
}

@article{morris_using_2019,
  title = {Using Simulation Studies to Evaluate Statistical Methods},
  author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
  date = {2019-05-20},
  journaltitle = {Statistics in Medicine},
  shortjournal = {Statistics in Medicine},
  volume = {38},
  number = {11},
  eprint = {1712.03198},
  eprinttype = {arxiv},
  pages = {2074--2102},
  issn = {0277-6715, 1097-0258},
  doi = {10.1002/sim.8086},
  url = {http://arxiv.org/abs/1712.03198},
  urldate = {2019-08-06},
  abstract = {Simulation studies are computer experiments that involve creating data by pseudorandom sampling. The key strength of simulation studies is the ability to understand the behaviour of statistical methods because some 'truth' (usually some parameter/s of interest) is known from the process of generating the data. This allows us to consider properties of methods, such as bias. While widely used, simulation studies are often poorly designed, analysed and reported. This tutorial outlines the rationale for using simulation studies and offers guidance for design, execution, analysis, reporting and presentation. In particular, this tutorial provides: a structured approach for planning and reporting simulation studies, which involves defining aims, data-generating mechanisms, estimands, methods and performance measures ('ADEMP'); coherent terminology for simulation studies; guidance on coding simulation studies; a critical discussion of key performance measures and their estimation; guidance on structuring tabular and graphical presentation of results; and new graphical presentations. With a view to describing recent practice, we review 100 articles taken from Volume 34 of Statistics in Medicine that included at least one simulation study and identify areas for improvement.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology}
}

@article{morse_significance_1995,
  title = {The {{Significance}} of {{Saturation}}},
  author = {Morse, Janice M.},
  date = {1995-05-01},
  journaltitle = {Qualitative Health Research},
  shortjournal = {Qual Health Res},
  volume = {5},
  number = {2},
  pages = {147--149},
  publisher = {{SAGE Publications Inc}},
  issn = {1049-7323},
  doi = {10.1177/104973239500500201},
  url = {https://doi.org/10.1177/104973239500500201},
  urldate = {2021-01-02},
  langid = {english}
}

@article{moshontz_psychological_2018,
  title = {The {{Psychological Science Accelerator}}: {{Advancing}} Psychology through a Distributed Collaborative Network},
  shorttitle = {The {{Psychological Science Accelerator}}},
  author = {Moshontz, Hannah and Campbell, Lorne and Ebersole, Charles R. and IJzerman, Hans and Urry, Heather L. and Forscher, Patrick S. and Grahe, Jon E. and McCarthy, Randy J. and Musser, Erica D. and Antfolk, Jan},
  date = {2018},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {4},
  pages = {501--515},
  publisher = {{Sage Publications Sage CA: Los Angeles, CA}},
  doi = {10.1177/2515245918797607}
}

@article{mudge_setting_2012,
  title = {Setting an {{Optimal}} α {{That Minimizes Errors}} in {{Null Hypothesis Significance Tests}}},
  author = {Mudge, Joseph F. and Baker, Leanne F. and Edge, Christopher B. and Houlahan, Jeff E.},
  date = {2012-02-28},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {7},
  number = {2},
  pages = {e32734},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0032734},
  url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0032734},
  urldate = {2017-08-20},
  abstract = {Null hypothesis significance testing has been under attack in recent years, partly owing to the arbitrary nature of setting α (the decision-making threshold and probability of Type I error) at a constant value, usually 0.05. If the goal of null hypothesis testing is to present conclusions in which we have the highest possible confidence, then the only logical decision-making threshold is the value that minimizes the probability (or occasionally, cost) of making errors. Setting α to minimize the combination of Type I and Type II error at a critical effect size can easily be accomplished for traditional statistical tests by calculating the α associated with the minimum average of α and β at the critical effect size. This technique also has the flexibility to incorporate prior probabilities of null and alternate hypotheses and/or relative costs of Type I and Type II errors, if known. Using an optimal α results in stronger scientific inferences because it estimates and minimizes both Type I errors and relevant Type II errors for a test. It also results in greater transparency concerning assumptions about relevant effect size(s) and the relative costs of Type I and II errors. By contrast, the use of α = 0.05 results in arbitrary decisions about what effect sizes will likely be considered significant, if real, and results in arbitrary amounts of Type II error for meaningful potential effect sizes. We cannot identify a rationale for continuing to arbitrarily use α = 0.05 for null hypothesis significance tests in any field, when it is possible to determine an optimal α.},
  keywords = {Agricultural soil science,Decision making,Experimental design,Freshwater fish,Gene expression,Lakes,Research errors,Shores}
}

@book{murphy_statistical_2014,
  title = {Statistical Power Analysis: A Simple and General Model for Traditional and Modern Hypothesis Tests},
  shorttitle = {Statistical Power Analysis},
  author = {Murphy, Kevin R. and Myors, Brett and Wolach, Allen H.},
  date = {2014},
  edition = {Fourth edition},
  publisher = {{Routledge, Taylor \& Francis Group}},
  location = {{New York}},
  isbn = {978-1-84872-587-4 978-1-84872-588-1},
  pagetotal = {229},
  keywords = {Statistical hypothesis testing,Statistical power analysis}
}

@article{neyman_inductive_1957,
  title = {"{{Inductive Behavior}}" as a {{Basic Concept}} of {{Philosophy}} of {{Science}}},
  author = {Neyman, Jerzy},
  date = {1957},
  journaltitle = {Revue de l'Institut International de Statistique / Review of the International Statistical Institute},
  volume = {25},
  number = {1/3},
  eprint = {1401671},
  eprinttype = {jstor},
  pages = {7},
  issn = {03731138},
  doi = {10.2307/1401671}
}

@article{neyman_problem_1933,
  title = {On the Problem of the Most Efficient Tests of Statistical Hypotheses.},
  author = {Neyman, Jerzy and Pearson, E. S.},
  date = {1933-01-01},
  journaltitle = {Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences},
  volume = {231},
  number = {694-706},
  pages = {289--337},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.1933.0009},
  url = {http://rsta.royalsocietypublishing.org/content/231/694-706/289},
  urldate = {2016-01-19},
  langid = {english}
}

@book{niiniluoto_critical_1999,
  title = {Critical {{Scientific Realism}}},
  author = {Niiniluoto, Ilkka},
  date = {1999},
  eprint = {Ng_p_3XCHxAC},
  eprinttype = {googlebooks},
  publisher = {{Oxford University Press}},
  abstract = {Ilkka Niiniluoto comes to the rescue of scientific realism, showing that reports of its death have been greatly exaggerated. Philosophical realism holds that the aim of a particular discourse is to make true statements about its subject-matter. Niiniluoto surveys the different varieties ofrealism in ontology, semantics, epistemology, theory construction, and methodology. He then sets out his own original version, and defends it against competing theories in the philosophy of science. Niiniluoto's critical scientific realism is founded upon the notion of truth as correspondencebetween language and reality, and characterizes scientific progress in terms of increasing truthlikeness. This makes it possible not only to take seriously, but also to make precise, the troublesome idea that scientific theories typically are false but nevertheless close to the truth.},
  isbn = {978-0-19-823833-1},
  langid = {english},
  pagetotal = {356},
  keywords = {Philosophy / Metaphysics,Science / Philosophy & Social Aspects}
}

@article{nowok_synthpop_2016,
  title = {Synthpop: {{Bespoke Creation}} of {{Synthetic Data}} in {{R}}},
  shorttitle = {Synthpop},
  author = {Nowok, Beata and Raab, Gillian M. and Dibben, Chris},
  date = {2016-10-28},
  journaltitle = {Journal of Statistical Software},
  volume = {74},
  number = {1},
  pages = {1--26},
  issn = {1548-7660},
  doi = {10.18637/jss.v074.i11},
  url = {https://www.jstatsoft.org/index.php/jss/article/view/v074i11},
  urldate = {2019-08-06},
  langid = {english},
  keywords = {CART,disclosure control,R,synthetic data,UK longitudinal studies}
}

@article{nuijten_prevalence_2015,
  title = {The Prevalence of Statistical Reporting Errors in Psychology (1985–2013)},
  author = {Nuijten, Michèle B. and Hartgerink, Chris H. J. and van Assen, Marcel A. L. M. and Epskamp, Sacha and Wicherts, Jelte M.},
  options = {useprefix=true},
  date = {2015-10-23},
  journaltitle = {Behavior Research Methods},
  issn = {1554-3528},
  doi = {10.3758/s13428-015-0664-2},
  url = {http://link.springer.com/10.3758/s13428-015-0664-2},
  urldate = {2015-11-30},
  langid = {english}
}

@article{obels_analysis_2020,
  title = {Analysis of {{Open Data}} and {{Computational Reproducibility}} in {{Registered Reports}} in {{Psychology}}},
  author = {Obels, Pepijn and Lakens, Daniël and Coles, Nicholas A. and Gottfried, Jaroslav and Green, Seth A.},
  date = {2020-06-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  shortjournal = {Advances in Methods and Practices in Psychological Science},
  volume = {3},
  number = {2},
  pages = {229--237},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245920918872},
  url = {https://doi.org/10.1177/2515245920918872},
  urldate = {2020-07-19},
  abstract = {Ongoing technological developments have made it easier than ever before for scientists to share their data, materials, and analysis code. Sharing data and analysis code makes it easier for other researchers to reuse or check published research. However, these benefits will emerge only if researchers can reproduce the analyses reported in published articles and if data are annotated well enough so that it is clear what all variable and value labels mean. Because most researchers are not trained in computational reproducibility, it is important to evaluate current practices to identify those that can be improved. We examined data and code sharing for Registered Reports published in the psychological literature from 2014 to 2018 and attempted to independently computationally reproduce the main results in each article. Of the 62 articles that met our inclusion criteria, 41 had data available, and 37 had analysis scripts available. Both data and code for 36 of the articles were shared. We could run the scripts for 31 analyses, and we reproduced the main results for 21 articles. Although the percentage of articles for which both data and code were shared (36 out of 62, or 58\%) and the percentage of articles for which main results could be computationally reproduced (21 out of 36, or 58\%) were relatively high compared with the percentages found in other studies, there is clear room for improvement. We provide practical recommendations based on our observations and cite examples of good research practices in the studies whose main results we reproduced.},
  langid = {english}
}

@article{olsson-collentine_heterogeneity_2020,
  title = {Heterogeneity in Direct Replications in Psychology and Its Association with Effect Size},
  author = {Olsson-Collentine, Anton and Wicherts, Jelte M. and van Assen, Marcel A. L. M.},
  options = {useprefix=true},
  date = {2020},
  journaltitle = {Psychological Bulletin},
  volume = {146},
  number = {10},
  pages = {922--940},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1939-1455(Electronic),0033-2909(Print)},
  doi = {10.1037/bul0000294},
  abstract = {We examined the evidence for heterogeneity (of effect sizes) when only minor changes to sample population and settings were made between studies and explored the association between heterogeneity and average effect size in a sample of 68 meta-analyses from 13 preregistered multilab direct replication projects in social and cognitive psychology. Among the many examined effects, examples include the Stroop effect, the “verbal overshadowing” effect, and various priming effects such as “anchoring” effects. We found limited heterogeneity; 48/68 (71\%) meta-analyses had nonsignificant heterogeneity, and most (49/68; 72\%) were most likely to have zero to small heterogeneity. Power to detect small heterogeneity (as defined by Higgins, Thompson, Deeks, \& Altman, 2003) was low for all projects (mean 43\%), but good to excellent for medium and large heterogeneity. Our findings thus show little evidence of widespread heterogeneity in direct replication studies in social and cognitive psychology, suggesting that minor changes in sample population and settings are unlikely to affect research outcomes in these fields of psychology. We also found strong correlations between observed average effect sizes (standardized mean differences and log odds ratios) and heterogeneity in our sample. Our results suggest that heterogeneity and moderation of effects is unlikely for a 0 average true effect size, but increasingly likely for larger average true effect size. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Cognitive Psychology,Experimental Laboratories,Experimental Replication,Homogeneity of Variance,Meta Analysis,Priming,Psychology,Social Psychology,Stroop Effect}
}

@article{orben_crud_2020,
  title = {Crud ({{Re}}){{Defined}}},
  shorttitle = {Crud ({{Re}}){{Defined}}},
  author = {Orben, Amy and Lakens, Daniël},
  date = {2020-06-11},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  publisher = {{SAGE PublicationsSage CA: Los Angeles, CA}},
  doi = {10.1177/2515245920917961},
  url = {https://journals.sagepub.com/doi/10.1177/2515245920917961},
  urldate = {2020-06-12},
  abstract = {The idea that in behavioral research everything correlates with everything else was a niche area of the scientific literature for more than half a century. With...},
  langid = {english}
}

@article{panzarella_denouncing_2020,
  title = {Denouncing the {{Use}} of {{Field-Specific Effect Size Distributions}} to {{Inform Magnitude}}},
  author = {Panzarella, Emily and Beribisky, Nataly and Cribbie, Rob},
  date = {2020-11-12T22:02:22},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/5k64e},
  url = {https://psyarxiv.com/5k64e/},
  urldate = {2020-12-28},
  abstract = {An effect size (ES) provides valuable information regarding the magnitude of effects, with the interpretation of magnitude being the most important. Interpreting ES magnitude requires combining information from the numerical ES value and the context of the research. However, many researchers adopt popular benchmarks such as those proposed by Cohen. More recently, researchers have proposed interpreting ES magnitude relative to the distribution of observed ESs in a specific field, creating unique benchmarks for declaring effects small, medium or large. However, there is no valid rationale whatsoever for this approach. This study was carried out in two parts: 1) We identified articles that proposed the use of field-specific ES distributions to interpret magnitude (primary articles); and 2) We identified articles that cited the primary articles and classified them by year and publication type. The first type consisted of methodological papers. The second type included articles that interpreted ES magnitude using the approach proposed in the primary articles. There has been a steady increase in the number of methodological and substantial articles discussing or adopting the approach of interpreting ES magnitude by considering the distribution of observed ES in that field, even though the approach is devoid of a theoretical framework. It is hoped that this research will restrict the practice of interpreting ES magnitude relative to the distribution of ES values in a field and instead encourage researchers to interpret such by considering the specific context of the study.},
  keywords = {Cohen's d,effect size,effect size distribution,effect size magnitude,Pearson's r,psychology,quantitative methods,Quantitative Methods,Social and Behavioral Sciences,Statistical Methods}
}

@article{parker_sample_2003,
  title = {Sample {{Size}}},
  author = {Parker, Robert A and Berman, Nancy G},
  date = {2003-08-01},
  journaltitle = {The American Statistician},
  shortjournal = {null},
  volume = {57},
  number = {3},
  pages = {166--170},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1198/0003130031919},
  url = {https://amstat.tandfonline.com/doi/abs/10.1198/0003130031919},
  urldate = {2020-12-31},
  abstract = {Conventionally, sample size calculations are viewed as calculations determining the right number of subjects needed for a study. Such calculations follow the classical paradigm: “for a difference X, I need sample size Y.” We argue that the paradigm “for a sample size Y, I get information Z” is more appropriate for many studies and reflects the information needed by scientists when planning a study. This approach applies to both physiological studies and Phase I and II interventional studies. We provide actual examples from our own consulting work to demonstrate this. We conclude that sample size should be viewed not as a unique right number, but rather as a factor needed to assess the utility of a study.}
}

@article{parsons_psychological_2019,
  title = {Psychological {{Science Needs}} a {{Standard Practice}} of {{Reporting}} the {{Reliability}} of {{Cognitive-Behavioral Measurements}}},
  author = {Parsons, Sam and Kruijt, Anne-Wil and Fox, Elaine},
  date = {2019-12-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  shortjournal = {Advances in Methods and Practices in Psychological Science},
  volume = {2},
  number = {4},
  pages = {378--395},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245919879695},
  url = {https://doi.org/10.1177/2515245919879695},
  urldate = {2021-01-03},
  abstract = {Psychological science relies on behavioral measures to assess cognitive processing; however, the field has not yet developed a tradition of routinely examining the reliability of these behavioral measures. Reliable measures are essential to draw robust inferences from statistical analyses, and subpar reliability has severe implications for measures’ validity and interpretation. Without examining and reporting the reliability of measurements used in an analysis, it is nearly impossible to ascertain whether results are robust or have arisen largely from measurement error. In this article, we propose that researchers adopt a standard practice of estimating and reporting the reliability of behavioral assessments of cognitive processing. We illustrate the need for this practice using an example from experimental psychopathology, the dot-probe task, although we argue that reporting reliability is relevant across fields (e.g., social cognition and cognitive psychology). We explore several implications of low measurement reliability and the detrimental impact that failure to assess measurement reliability has on interpretability and comparison of results and therefore research quality. We argue that researchers in the field of cognition need to report measurement reliability as routine practice so that more reliable assessment tools can be developed. To provide some guidance on estimating and reporting reliability, we describe the use of bootstrapped split-half estimation and intraclass correlation coefficients to estimate internal consistency and test-retest reliability, respectively. For future researchers to build upon current results, it is imperative that all researchers provide psychometric information sufficient for estimating the accuracy of inferences and informing further development of cognitive-behavioral assessments.},
  langid = {english},
  keywords = {cognitive-behavioral tasks,estimating and reporting,open materials,psychometrics,reliability}
}

@article{perugini_practical_2018,
  title = {A {{Practical Primer To Power Analysis}} for {{Simple Experimental Designs}}},
  author = {Perugini, Marco and Gallucci, Marcello and Costantini, Giulio},
  date = {2018-07-09},
  journaltitle = {International Review of Social Psychology},
  volume = {31},
  number = {1},
  pages = {20},
  issn = {2397-8570},
  doi = {10.5334/irsp.181},
  url = {http://www.rips-irsp.com/articles/10.5334/irsp.181/},
  urldate = {2019-04-07},
  abstract = {Power analysis is an important tool to use when planning studies. This contribution aims to remind readers what power analysis is, emphasize why it matters, and articulate when and how it should be used. The focus is on applications of power analysis for experimental designs often encountered in psychology, starting from simple two-group independent and paired groups and moving to one-way analysis of variance, factorial designs, contrast analysis, trend analysis, regression analysis, analysis of covariance, and mediation analysis. Special attention is given to the application of power analysis to moderation designs, considering both dichotomous and continuous predictors and moderators. Illustrative practical examples based on G*Power and R packages are provided throughout the article. Annotated code for the examples with R and dedicated computational tools are made freely available at a dedicated web page (https://github.com/mcfanda/primerPowerIRSP). Applications of power analysis for more complex designs are briefly mentioned, and some important general issues related to power analysis are discussed.},
  langid = {english},
  keywords = {effect size,moderation,power analysis,sensitivity analysis,uncertainty}
}

@article{perugini_safeguard_2014,
  title = {Safeguard Power as a Protection against Imprecise Power Estimates},
  author = {Perugini, Marco and Gallucci, Marcello and Costantini, Giulio},
  date = {2014},
  journaltitle = {Perspectives on Psychological Science},
  volume = {9},
  number = {3},
  pages = {319--332},
  doi = {10.1177/1745691614528519},
  url = {http://journals.sagepub.com/doi/abs/10.1177/1745691614528519},
  urldate = {2017-05-10}
}

@article{phillips_statistical_2001,
  title = {Statistical Significance of Sediment Toxicity Test Results: {{Threshold}} Values Derived by the Detectable Significance Approach},
  shorttitle = {Statistical Significance of Sediment Toxicity Test Results},
  author = {Phillips, Bryn M. and Hunt, John W. and Anderson, Brian S. and Puckett, H. Max and Fairey, Russell and Wilson, Craig J. and Tjeerdema, Ron},
  date = {2001},
  journaltitle = {Environmental Toxicology and Chemistry},
  volume = {20},
  number = {2},
  pages = {371--373},
  issn = {1552-8618},
  doi = {10.1002/etc.5620200218},
  url = {https://setac.onlinelibrary.wiley.com/doi/abs/10.1002/etc.5620200218},
  urldate = {2020-12-12},
  abstract = {A number of methods have been employed to determine the statistical significance of sediment toxicity test results. To allow consistency among comparisons, regardless of among-replicate variability, a protocol-specific approach has been used that considers protocol performance over a large number of comparisons. Ninetieth-percentile minimum significant difference (MSD) values were calculated to determine a critical threshold for statistically significant sample toxicity. Significant toxicity threshold values (as a percentage of laboratory control values) are presented for six species and nine endpoints based on data from as many as 720 stations. These threshold values are useful for interpreting sediment toxicity data from large studies and in eliminating cases where statistical significance is assigned in individual cases because among-replicate variability is small.},
  langid = {english},
  keywords = {Minimum significant difference,Sediment toxicity,Statistics},
  annotation = {\_eprint: https://setac.onlinelibrary.wiley.com/doi/pdf/10.1002/etc.5620200218}
}

@article{pickett_questionable_2017,
  title = {Questionable, {{Objectionable}} or {{Criminal}}? {{Public Opinion}} on {{Data Fraud}} and {{Selective Reporting}} in {{Science}}},
  shorttitle = {Questionable, {{Objectionable}} or {{Criminal}}?},
  author = {Pickett, Justin T. and Roche, Sean Patrick},
  date = {2017-03-09},
  journaltitle = {Science and Engineering Ethics},
  shortjournal = {Sci Eng Ethics},
  pages = {1--21},
  issn = {1353-3452, 1471-5546},
  doi = {10.1007/s11948-017-9886-2},
  url = {https://link.springer.com/article/10.1007/s11948-017-9886-2},
  urldate = {2018-01-11},
  abstract = {Data fraud and selective reporting both present serious threats to the credibility of science. However, there remains considerable disagreement among scientists about how best to sanction data fraud, and about the ethicality of selective reporting. The public is arguably the largest stakeholder in the reproducibility of science; research is primarily paid for with public funds, and flawed science threatens the public’s welfare. Members of the public are able to make meaningful judgments about the morality of different behaviors using moral intuitions. Legal scholars emphasize that to maintain legitimacy, social control policies must be developed with some consideration given to the public’s moral intuitions. Although there is a large literature on popular attitudes toward science, there is no existing evidence about public opinion on data fraud or selective reporting. We conducted two studies—a survey experiment with a nationwide convenience sample (N = 821), and a follow-up survey with a representative sample of US adults (N = 964)—to explore community members’ judgments about the morality of data fraud and selective reporting in science. The findings show that community members make a moral distinction between data fraud and selective reporting, but overwhelmingly judge both behaviors to be immoral and deserving of punishment. Community members believe that scientists who commit data fraud or selective reporting should be fired and banned from receiving funding. For data fraud, most Americans support criminal penalties. Results from an ordered logistic regression analysis reveal few demographic and no significant partisan differences in punitiveness toward data fraud.},
  langid = {english}
}

@article{polanin_transparency_2020,
  title = {Transparency and {{Reproducibility}} of {{Meta-Analyses}} in {{Psychology}}: {{A Meta-Review}}},
  shorttitle = {Transparency and {{Reproducibility}} of {{Meta-Analyses}} in {{Psychology}}},
  author = {Polanin, Joshua R. and Hennessy, Emily A. and Tsuji, Sho},
  date = {2020-07-01},
  journaltitle = {Perspectives on Psychological Science},
  shortjournal = {Perspect Psychol Sci},
  volume = {15},
  number = {4},
  pages = {1026--1041},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691620906416},
  url = {https://doi.org/10.1177/1745691620906416},
  urldate = {2020-08-04},
  abstract = {Systematic review and meta-analysis are possible as viable research techniques only through transparent reporting of primary research; thus, one might expect meta-analysts to demonstrate best practice in their reporting of results and have a high degree of transparency leading to reproducibility of their work. This assumption has yet to be fully tested in the psychological sciences. We therefore aimed to assess the transparency and reproducibility of psychological meta-analyses. We conducted a meta-review by sampling 150 studies from Psychological Bulletin to extract information about each review’s transparent and reproducible reporting practices. The results revealed that authors reported on average 55\% of criteria and that transparent reporting practices increased over the three decades studied (b = 1.09, SE = 0.24, t = 4.519, p {$<$} .001). Review authors consistently reported eligibility criteria, effect-size information, and synthesis techniques. Review authors, however, on average, did not report specific search results, screening and extraction procedures, and most importantly, effect-size and moderator information from each individual study. Far fewer studies provided statistical code required for complete analytical replication. We argue that the field of psychology and research synthesis in general should require review authors to report these elements in a transparent and reproducible manner.},
  langid = {english}
}

@book{popper_logic_2002,
  title = {The logic of scientific discovery},
  author = {Popper, Karl R},
  date = {2002},
  publisher = {{Routledge}},
  location = {{London; New York}},
  abstract = {When first published in 1959, this book revolutionized contemporary thinking about science and knowledge. It remains the one of the most widely read books about science to come out of the twentieth century.},
  isbn = {978-0-203-99462-7 978-0-415-27843-0 978-0-415-27844-7},
  langid = {Translated from the German.}
}

@book{proschan_statistical_2006,
  title = {Statistical Monitoring of Clinical Trials: A Unified Approach},
  shorttitle = {Statistical Monitoring of Clinical Trials},
  author = {Proschan, Michael A. and Lan, K. K. Gordan and Wittes, Janet Turk},
  date = {2006},
  series = {Statistics for Biology and Health},
  publisher = {{Springer}},
  location = {{New York, NY}},
  isbn = {978-0-387-30059-7},
  pagetotal = {258},
  keywords = {Bayes Theorem,Clinical Trials,Data Interpretation; Statistical,Drugs,Statistical methods,Statistics,statistics & numerical data,Testing}
}

@article{proschan_two-stage_2005,
  title = {Two-{{Stage Sample Size Re-Estimation Based}} on a {{Nuisance Parameter}}: {{A Review}}},
  shorttitle = {Two-{{Stage Sample Size Re-Estimation Based}} on a {{Nuisance Parameter}}},
  author = {Proschan, Michael A.},
  date = {2005-07-01},
  journaltitle = {Journal of Biopharmaceutical Statistics},
  volume = {15},
  number = {4},
  eprint = {16022163},
  eprinttype = {pmid},
  pages = {559--574},
  publisher = {{Taylor \& Francis}},
  issn = {1054-3406},
  doi = {10.1081/BIP-200062852},
  url = {https://doi.org/10.1081/BIP-200062852},
  urldate = {2021-01-10},
  abstract = {Sample size calculations are important and difficult in clinical trails because they depend on the nuisance parameter and treatment effect. Recently, much attention has been focused on two-stage methods whereby the first stage constitutes an internal pilot study used to estimate parameters and revise the final sample size. This paper reviews two-stage methods based on estimation of nuisance parameters in either a continuous or dichotomous outcome setting.},
  keywords = {Adaptive methods,Ancillary statistic,Blinding,Clinical trials,Conditioning,Continuous outcome,Correlation,Dichotomous outcome,Independence,Internal pilot study,Lumping,Pooling,Power,Restricted design,Unrestricted design},
  annotation = {\_eprint: https://doi.org/10.1081/BIP-200062852}
}

@book{ravetz_scientific_1995,
  title = {Scientific {{Knowledge}} and {{Its Social Problems}}},
  author = {Ravetz, Jerome},
  date = {1995-01-01},
  edition = {Reprint edition},
  publisher = {{Transaction Publishers}},
  location = {{New Brunswick, N.J}},
  abstract = {Science is continually confronted by new and difficult social and ethical problems. Some of these problems have arisen from the transformation of the academic science of the prewar period into the industrialized science of the present. Traditional theories of science are now widely recognized as obsolete. In Scientific Knowledge and Its Social Problems (originally published in 1971), Jerome R. Ravetz analyzes the work of science as the creation and investigation of problems. He demonstrates the role of choice and value judgment, and the inevitability of error, in scientific research. Ravetz’s new introductory essay is a masterful statement of how our understanding of science has evolved over the last two decades.},
  isbn = {978-1-56000-851-4},
  langid = {english},
  pagetotal = {449}
}

@article{rice_heads_1994,
  title = {'{{Heads I}} Win, Tails You Lose': Testing Directional Alternative Hypotheses in Ecological and Evolutionary Research},
  shorttitle = {'{{Heads I}} Win, Tails You Lose'},
  author = {Rice, W. R. and Gaines, S. D.},
  date = {1994-06},
  journaltitle = {Trends in Ecology \& Evolution},
  shortjournal = {Trends Ecol. Evol. (Amst.)},
  volume = {9},
  number = {6},
  eprint = {21236837},
  eprinttype = {pmid},
  pages = {235--237},
  issn = {0169-5347},
  doi = {10.1016/0169-5347(94)90258-5},
  abstract = {Whenever experiments make a priori predictions about the direction of change in some parameter, one-tailed test statistics offer a potentially large gain in power over the corresponding two-tailed test. This gain is rarely used in ecology and evolution because of (1) the belief that one-tailed procedures are unavailable for most statistical tests and (2) an inherent dilemma in one-tailed tests: how do we handle large parameter changes in the unanticipated direction? The first problem is a misconception, whereas the second is easily resolved by recognizing that one- and two-tailed tests are simply extremes in a continuum of testing options.},
  langid = {english},
  annotation = {00231}
}

@article{richard_one_2003,
  title = {One {{Hundred Years}} of {{Social Psychology Quantitatively Described}}.},
  author = {Richard, F. D. and Bond, Charles F. and Stokes-Zoota, Juli J.},
  date = {2003},
  journaltitle = {Review of General Psychology},
  volume = {7},
  number = {4},
  pages = {331--363},
  issn = {1939-1552, 1089-2680},
  doi = {10.1037/1089-2680.7.4.331},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/1089-2680.7.4.331},
  urldate = {2015-11-30},
  langid = {english},
  annotation = {00683}
}

@article{richardson_eta_2011,
  title = {Eta Squared and Partial Eta Squared as Measures of Effect Size in Educational Research},
  author = {Richardson, John T.E.},
  date = {2011-01},
  journaltitle = {Educational Research Review},
  volume = {6},
  number = {2},
  pages = {135--147},
  issn = {1747938X},
  doi = {10.1016/j.edurev.2010.12.001},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S1747938X11000029},
  urldate = {2015-11-30},
  langid = {english},
  annotation = {00564}
}

@article{rijnsoever_i_2017,
  title = {({{I Can}}’t {{Get No}}) {{Saturation}}: {{A}} Simulation and Guidelines for Sample Sizes in Qualitative Research},
  shorttitle = {({{I Can}}’t {{Get No}}) {{Saturation}}},
  author = {van Rijnsoever, Frank J.},
  date = {2017-07-26},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {12},
  number = {7},
  pages = {e0181689},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0181689},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0181689},
  urldate = {2020-08-26},
  abstract = {I explore the sample size in qualitative research that is required to reach theoretical saturation. I conceptualize a population as consisting of sub-populations that contain different types of information sources that hold a number of codes. Theoretical saturation is reached after all the codes in the population have been observed once in the sample. I delineate three different scenarios to sample information sources: “random chance,” which is based on probability sampling, “minimal information,” which yields at least one new code per sampling step, and “maximum information,” which yields the largest number of new codes per sampling step. Next, I use simulations to assess the minimum sample size for each scenario for systematically varying hypothetical populations. I show that theoretical saturation is more dependent on the mean probability of observing codes than on the number of codes in a population. Moreover, the minimal and maximal information scenarios are significantly more efficient than random chance, but yield fewer repetitions per code to validate the findings. I formulate guidelines for purposive sampling and recommend that researchers follow a minimum information scenario.},
  langid = {english},
  keywords = {Computer and information sciences,Data management,Medicine and health sciences,Number theory,Probability theory,Qualitative studies,Simulation and modeling,Social sciences}
}

@article{rogers_using_1993,
  title = {Using Significance Tests to Evaluate Equivalence between Two Experimental Groups.},
  author = {Rogers, James L. and Howard, Kenneth I. and Vessey, John T.},
  date = {1993},
  journaltitle = {Psychological bulletin},
  volume = {113},
  number = {3},
  pages = {553--565},
  doi = {http://dx.doi.org/10.1037/0033-2909.113.3.553},
  url = {http://psycnet.apa.org/journals/bul/113/3/553/},
  urldate = {2016-05-15},
  annotation = {00477}
}

@book{rosenthal_contrasts_2000,
  title = {Contrasts and Effect Sizes in Behavioral Research: A Correlational Approach},
  shorttitle = {Contrasts and Effect Sizes in Behavioral Research},
  author = {Rosenthal, Robert and Rosnow, Ralph L. and Rubin, Donald B.},
  date = {2000},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge, U.K. ; New York}},
  isbn = {978-0-521-65258-2 978-0-521-65980-2},
  langid = {english},
  pagetotal = {212},
  keywords = {Analysis of variance,Psychology,Psychometrics,Social sciences,Statistical methods}
}

@article{rosnow_statistical_1989,
  title = {Statistical Procedures and the Justification of Knowledge in Psychological Science.},
  author = {Rosnow, Ralph L. and Rosenthal, Robert},
  date = {1989},
  journaltitle = {American psychologist},
  volume = {44},
  number = {10},
  pages = {1276},
  doi = {10.1037/0003-066X.44.10.1276},
  annotation = {01007}
}

@article{rouder_bayesian_2009,
  title = {Bayesian t Tests for Accepting and Rejecting the Null Hypothesis},
  author = {Rouder, Jeffrey N. and Speckman, Paul L. and Sun, Dongchu and Morey, Richard D. and Iverson, Geoffrey},
  date = {2009-04},
  journaltitle = {Psychonomic Bulletin \& Review},
  volume = {16},
  number = {2},
  pages = {225--237},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/PBR.16.2.225},
  url = {http://www.springerlink.com/index/10.3758/PBR.16.2.225},
  urldate = {2016-01-11},
  langid = {english},
  annotation = {01475}
}

@article{scheel_why_2020,
  title = {Why {{Hypothesis Testers Should Spend Less Time Testing Hypotheses}}},
  author = {Scheel, Anne M. and Tiokhin, Leonid and Isager, Peder M. and Lakens, Daniël},
  date = {2020-12-16},
  journaltitle = {Perspectives on Psychological Science},
  shortjournal = {Perspect Psychol Sci},
  pages = {1745691620966795},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691620966795},
  url = {https://doi.org/10.1177/1745691620966795},
  urldate = {2020-12-17},
  abstract = {For almost half a century, Paul Meehl educated psychologists about how the mindless use of null-hypothesis significance tests made research on theories in the social sciences basically uninterpretable. In response to the replication crisis, reforms in psychology have focused on formalizing procedures for testing hypotheses. These reforms were necessary and influential. However, as an unexpected consequence, psychological scientists have begun to realize that they may not be ready to test hypotheses. Forcing researchers to prematurely test hypotheses before they have established a sound “derivation chain” between test and theory is counterproductive. Instead, various nonconfirmatory research activities should be used to obtain the inputs necessary to make hypothesis tests informative. Before testing hypotheses, researchers should spend more time forming concepts, developing valid measures, establishing the causal relationships between concepts and the functional form of those relationships, and identifying boundary conditions and auxiliary assumptions. Providing these inputs should be recognized and incentivized as a crucial goal in itself. In this article, we discuss how shifting the focus to nonconfirmatory research can tie together many loose ends of psychology’s reform movement and help us to develop strong, testable theories, as Paul Meehl urged.},
  langid = {english},
  keywords = {exploratory research,hypothesis testing,replication crisis}
}

@article{schimmack_ironic_2012,
  title = {The Ironic Effect of Significant Results on the Credibility of Multiple-Study Articles.},
  author = {Schimmack, Ulrich},
  date = {2012},
  journaltitle = {Psychological Methods},
  volume = {17},
  number = {4},
  pages = {551--566},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/a0029487},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0029487},
  urldate = {2018-05-24},
  abstract = {Cohen (1962) pointed out the importance of statistical power for psychology as a science, but statistical power has not improved. At the same time, the number of studies has increased from a single study to multiple studies within a single article. It has been overlooked that multiple study articles are severely underpowered because power decreases as a function of the number of statistical tests that are being conducted (Maxwell, 2004). The discrepancy between the expected number of significant results and the actual number of significant results in multiple study articles undermine the credibility of the reported results and it is likely that questionable research practices contributed to the reporting of too many significant results (Sterling, 1959). The problem of low power in multiple study articles is illustrated using Bem’s (2011) article on extrasensory perception and Gailliot et al.’s (2007) article on glucose and self-regulation. I conclude with several recommendations that can increase the credibility of scientific evidence in psychological journals. One major recommendation is to pay more attention to the power of studies to produce positive results without the help of questionable research practices and to request that authors justify sample sizes with a priori predictions of effect sizes. It is also important to publish replication studies with non-significant results, if these studies had high power to replicate a published finding.},
  langid = {english},
  annotation = {00000}
}

@article{schnuerch_controlling_2020,
  title = {Controlling Decision Errors with Minimal Costs: {{The}} Sequential Probability Ratio t Test.},
  shorttitle = {Controlling Decision Errors with Minimal Costs},
  author = {Schnuerch, Martin and Erdfelder, Edgar},
  date = {2020},
  journaltitle = {Psychological methods},
  volume = {25},
  number = {2},
  pages = {206--226},
  publisher = {{American Psychological Association}},
  doi = {10.1037/met0000234},
  abstract = {For several years, the public debate in psychological science has been dominated by what is referred to as the reproducibility crisis. This crisis has, inter alia, drawn attention to the need for proper control of statistical decision errors in testing psychological hypotheses. However, conventional methods of error probability control often require fairly large samples. Sequential statistical tests provide an attractive alternative: They can be applied repeatedly during the sampling process and terminate whenever there is sufficient evidence in the data for one of the hypotheses of interest. Thus, sequential tests may substantially reduce the required sample size without compromising predefined error probabilities. Herein, we discuss the most efficient sequential design, the sequential probability ratio test (SPRT), and show how it is easily implemented for a 2-sample t test using standard statistical software. We demonstrate, by means of simulations, that the SPRT not only reliably controls error probabilities but also typically requires substantially smaller samples than standard t tests and other common sequential designs. Moreover, we investigate the robustness of the SPRT against violations of its assumptions. Finally, we illustrate the sequential t test by applying it to an empirical example and provide recommendations on how psychologists can employ it in their own research to benefit from its desirable properties.}
}

@article{schoemann_determining_2017,
  title = {Determining {{Power}} and {{Sample Size}} for {{Simple}} and {{Complex Mediation Models}}},
  author = {Schoemann, Alexander M. and Boulton, Aaron J. and Short, Stephen D.},
  date = {2017-05-01},
  journaltitle = {Social Psychological and Personality Science},
  shortjournal = {Social Psychological and Personality Science},
  volume = {8},
  number = {4},
  pages = {379--386},
  publisher = {{SAGE Publications Inc}},
  issn = {1948-5506},
  doi = {10.1177/1948550617715068},
  url = {https://doi.org/10.1177/1948550617715068},
  urldate = {2020-12-28},
  abstract = {Mediation analyses abound in social and personality psychology. Current recommendations for assessing power and sample size in mediation models include using a Monte Carlo power analysis simulation and testing the indirect effect with a bootstrapped confidence interval. Unfortunately, these methods have rarely been adopted by researchers due to limited software options and the computational time needed. We propose a new method and convenient tools for determining sample size and power in mediation models. We demonstrate our new method through an easy-to-use application that implements the method. These developments will allow researchers to quickly and easily determine power and sample size for simple and complex mediation models.},
  langid = {english},
  keywords = {mediation,power,R,sample size}
}

@article{schoenfeld_procon_2005,
  title = {Pro/Con Clinical Debate: {{It}} Is Acceptable to Stop Large Multicentre Randomized Controlled Trials at Interim Analysis for Futility},
  shorttitle = {Pro/Con Clinical Debate},
  author = {Schoenfeld, David A and Meade, Maureen O},
  date = {2005},
  journaltitle = {Critical Care},
  shortjournal = {Crit Care},
  volume = {9},
  number = {1},
  eprint = {15693981},
  eprinttype = {pmid},
  pages = {34--36},
  issn = {1364-8535},
  doi = {10.1186/cc3013},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1065108/},
  urldate = {2020-08-07},
  abstract = {A few recent, large, well-publicized trials in critical care medicine have been stopped for futility. In the critical care setting, stopping for futility means that independent review committees have elected to stop the trial early – based on predetermined rules – since the likelihood of finding a treatment effect is low. For bedside clinicians the idea of futility in a clinical trial can be confusing. In the present article, two experts in the conduct of clinical trials debate the role of futility-stopping rules.},
  pmcid = {PMC1065108}
}

@article{schonbrodt_at_2013,
  title = {At What Sample Size Do Correlations Stabilize?},
  author = {Schönbrodt, Felix D. and Perugini, Marco},
  date = {2013-10},
  journaltitle = {Journal of Research in Personality},
  volume = {47},
  number = {5},
  pages = {609--612},
  issn = {00926566},
  doi = {10.1016/j.jrp.2013.05.009},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0092656613000858},
  urldate = {2015-11-30},
  langid = {english},
  annotation = {00000}
}

@article{schonbrodt_sequential_2017,
  title = {Sequential Hypothesis Testing with {{Bayes}} Factors: {{Efficiently}} Testing Mean Differences},
  shorttitle = {Sequential Hypothesis Testing with {{Bayes}} Factors},
  author = {Schönbrodt, Felix D. and Wagenmakers, Eric-Jan and Zehetleitner, Michael and Perugini, Marco},
  date = {2017-06},
  journaltitle = {Psychological Methods},
  shortjournal = {Psychol Methods},
  volume = {22},
  number = {2},
  eprint = {26651986},
  eprinttype = {pmid},
  pages = {322--339},
  issn = {1939-1463},
  doi = {10.1037/MET0000061},
  abstract = {Unplanned optional stopping rules have been criticized for inflating Type I error rates under the null hypothesis significance testing (NHST) paradigm. Despite these criticisms, this research practice is not uncommon, probably because it appeals to researcher's intuition to collect more data to push an indecisive result into a decisive region. In this contribution, we investigate the properties of a procedure for Bayesian hypothesis testing that allows optional stopping with unlimited multiple testing, even after each participant. In this procedure, which we call Sequential Bayes Factors (SBFs), Bayes factors are computed until an a priori defined level of evidence is reached. This allows flexible sampling plans and is not dependent upon correct effect size guesses in an a priori power analysis. We investigated the long-term rate of misleading evidence, the average expected sample sizes, and the biasedness of effect size estimates when an SBF design is applied to a test of mean differences between 2 groups. Compared with optimal NHST, the SBF design typically needs 50\% to 70\% smaller samples to reach a conclusion about the presence of an effect, while having the same or lower long-term rate of wrong inference. (PsycINFO Database Record},
  langid = {english},
  keywords = {Bayes Theorem,Data Interpretation; Statistical,Humans,Probability,Research Design,Sample Size},
  annotation = {00070}
}

@article{schulz_sample_2005,
  title = {Sample Size Calculations in Randomised Trials: Mandatory and Mystical},
  shorttitle = {Sample Size Calculations in Randomised Trials},
  author = {Schulz, Kenneth F. and Grimes, David A.},
  date = {2005},
  journaltitle = {The Lancet},
  volume = {365},
  number = {9467},
  pages = {1348--1353},
  doi = {10.1016/S0140-6736(05)61034-3},
  url = {http://www.sciencedirect.com/science/article/pii/S0140673605610343},
  urldate = {2016-03-16}
}

@article{sedlmeier_studies_1989,
  title = {Do Studies of Statistical Power Have an Effect on the Power of Studies?},
  author = {Sedlmeier, Peter and Gigerenzer, Gerd},
  date = {1989},
  journaltitle = {Psychological Bulletin},
  volume = {105},
  number = {2},
  pages = {309--316},
  doi = {10.1037/0033-2909.105.2.309},
  url = {http://psycnet.apa.org/psycinfo/1989-21174-001},
  urldate = {2015-12-11}
}

@article{shmueli_explain_2010,
  ids = {shmueli_explain_2010-1},
  title = {To Explain or to Predict?},
  author = {Shmueli, Galit},
  date = {2010},
  journaltitle = {Statistical science},
  volume = {25},
  number = {3},
  pages = {289--310}
}

@article{simmons_false-positive_2011,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  shorttitle = {False-{{Positive Psychology}}},
  author = {Simmons, J. P. and Nelson, L. D. and Simonsohn, U.},
  date = {2011-11-01},
  journaltitle = {Psychological Science},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797611417632},
  url = {http://pss.sagepub.com/lookup/doi/10.1177/0956797611417632},
  urldate = {2015-11-30},
  langid = {english}
}

@unpublished{simmons_life_2013,
  title = {Life after {{P-Hacking}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  date = {2013-01-17/2013-01-19},
  url = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2205186},
  eventtitle = {Meeting of the {{Society}} for {{Personality}} and {{Social Psychology}}},
  venue = {{New Orleans, LA}}
}

@article{simonsohn_small_2015,
  title = {Small {{Telescopes Detectability}} and the {{Evaluation}} of {{Replication Results}}},
  author = {Simonsohn, Uri},
  date = {2015-05-01},
  journaltitle = {Psychological Science},
  shortjournal = {Psychological Science},
  volume = {26},
  number = {5},
  eprint = {25800521},
  eprinttype = {pmid},
  pages = {559--569},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797614567341},
  url = {http://pss.sagepub.com/content/26/5/559},
  urldate = {2016-08-08},
  abstract = {This article introduces a new approach for evaluating replication results. It combines effect-size estimation with hypothesis testing, assessing the extent to which the replication results are consistent with an effect size big enough to have been detectable in the original study. The approach is demonstrated by examining replications of three well-known findings. Its benefits include the following: (a) differentiating “unsuccessful” replication attempts (i.e., studies yielding p {$>$} .05) that are too noisy from those that actively indicate the effect is undetectably different from zero, (b) “protecting” true findings from underpowered replications, and (c) arriving at intuitively compelling inferences in general and for the revisited replications in particular.},
  langid = {english},
  keywords = {hypothesis testing,open materials,replication,statistical power}
}

@article{skipper_sacredness_1967,
  title = {The {{Sacredness}} of .05: {{A Note}} Concerning the {{Uses}} of {{Statistical Levels}} of {{Significance}} in {{Social Science}}},
  shorttitle = {The {{Sacredness}} of .05},
  author = {Skipper, James K. and Guenther, Anthony L. and Nass, Gilbert},
  date = {1967},
  journaltitle = {The American Sociologist},
  shortjournal = {The American Sociologist},
  volume = {2},
  number = {1},
  eprint = {27701229},
  eprinttype = {jstor},
  pages = {16--18},
  issn = {0003-1232},
  doi = {https://www.jstor.org/stable/27701229}
}

@article{smaldino_natural_2016,
  title = {The Natural Selection of Bad Science},
  author = {Smaldino, Paul E. and McElreath, Richard},
  date = {2016-09-01},
  journaltitle = {Royal Society Open Science},
  volume = {3},
  number = {9},
  pages = {160384},
  issn = {2054-5703},
  doi = {10.1098/rsos.160384},
  url = {http://rsos.royalsocietypublishing.org/content/3/9/160384},
  urldate = {2016-09-25},
  abstract = {Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing—no deliberate cheating nor loafing—by scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more ‘progeny,’ such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.},
  langid = {english}
}

@book{smithson_confidence_2003,
  title = {Confidence Intervals},
  author = {Smithson, Michael},
  date = {2003},
  series = {Sage University Papers. {{Quantitative}} Applications in the Social Sciences},
  number = {no. 07/140},
  publisher = {{Sage Publications}},
  location = {{Thousand Oaks, Calif}},
  isbn = {978-0-7619-2499-9},
  pagetotal = {93},
  keywords = {Confidence intervals,Mathematics,Social sciences,Statistical methods}
}

@book{spiegelhalter_art_2019,
  title = {The {{Art}} of {{Statistics}}: {{How}} to {{Learn}} from {{Data}}},
  shorttitle = {The {{Art}} of {{Statistics}}},
  author = {Spiegelhalter, David},
  date = {2019-09-03},
  edition = {Illustrated edition},
  publisher = {{Basic Books}},
  location = {{New York}},
  abstract = {The definitive guide to statistical thinkingStatistics are everywhere, as integral to science as they are to business, and in the popular media hundreds of times a day. In this age of big data, a basic grasp of statistical literacy is more important than ever if we want to separate the fact from the fiction, the ostentatious embellishments from the raw evidence -- and even more so if we hope to participate in the future, rather than being simple bystanders.In The Art of Statistics, world-renowned statistician David Spiegelhalter shows readers how to derive knowledge from raw data by focusing on the concepts and connections behind the math. Drawing on real world examples to introduce complex issues, he shows us how statistics can help us determine the luckiest passenger on the Titanic, whether a notorious serial killer could have been caught earlier, and if screening for ovarian cancer is beneficial. The Art of Statistics not only shows us how mathematicians have used statistical science to solve these problems -- it teaches us how we too can think like statisticians. We learn how to clarify our questions, assumptions, and expectations when approaching a problem, and -- perhaps even more importantly -- we learn how to responsibly interpret the answers we receive.Combining the incomparable insight of an expert with the playful enthusiasm of an aficionado, The Art of Statistics is the definitive guide to stats that every modern person needs.},
  isbn = {978-1-5416-1851-0},
  langid = {english},
  pagetotal = {448}
}

@article{spiegelhalter_monitoring_1986,
  title = {Monitoring Clinical Trials: Conditional or Predictive Power?},
  shorttitle = {Monitoring Clinical Trials},
  author = {Spiegelhalter, David J. and Freedman, Laurence S. and Blackburn, Patrick R.},
  date = {1986},
  journaltitle = {Controlled clinical trials},
  volume = {7},
  number = {1},
  pages = {8--17},
  publisher = {{Elsevier}},
  doi = {10.1016/0197-2456(86)90003-6}
}

@article{steiger_beyond_2004,
  title = {Beyond the {{F Test}}: {{Effect Size Confidence Intervals}} and {{Tests}} of {{Close Fit}} in the {{Analysis}} of {{Variance}} and {{Contrast Analysis}}.},
  shorttitle = {Beyond the {{F Test}}},
  author = {Steiger, James H.},
  date = {2004},
  journaltitle = {Psychological Methods},
  volume = {9},
  number = {2},
  pages = {164--182},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.9.2.164},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/1082-989X.9.2.164},
  urldate = {2016-01-11},
  langid = {english}
}

@article{taylor_bias_1996,
  title = {Bias in Linear Model Power and Sample Size Calculation Due to Estimating Noncentrality},
  author = {Taylor, Douglas J. and Muller, Keith E.},
  date = {1996},
  journaltitle = {Communications in Statistics-Theory and Methods},
  volume = {25},
  number = {7},
  pages = {1595--1610},
  doi = {10.1080/03610929608831787},
  url = {http://www.tandfonline.com/doi/abs/10.1080/03610929608831787},
  urldate = {2017-05-08}
}

@article{teare_sample_2014,
  title = {Sample Size Requirements to Estimate Key Design Parameters from External Pilot Randomised Controlled Trials: A Simulation Study},
  shorttitle = {Sample Size Requirements to Estimate Key Design Parameters from External Pilot Randomised Controlled Trials},
  author = {Teare, M. Dawn and Dimairo, Munyaradzi and Shephard, Neil and Hayman, Alex and Whitehead, Amy and Walters, Stephen J.},
  date = {2014-07-03},
  journaltitle = {Trials},
  shortjournal = {Trials},
  volume = {15},
  number = {1},
  pages = {264},
  issn = {1745-6215},
  doi = {10.1186/1745-6215-15-264},
  url = {https://doi.org/10.1186/1745-6215-15-264},
  urldate = {2020-12-17},
  abstract = {External pilot or feasibility studies can be used to estimate key unknown parameters to inform the design of the definitive randomised controlled trial (RCT). However, there is little consensus on how large pilot studies need to be, and some suggest inflating estimates to adjust for the lack of precision when planning the definitive RCT.},
  langid = {english}
}

@online{ter_schure_accumulation_2019,
  title = {Accumulation {{Bias}} in {{Meta-Analysis}}: {{The Need}} to {{Consider Time}} in {{Error Control}}},
  shorttitle = {Accumulation {{Bias}} in {{Meta-Analysis}}},
  author = {ter Schure, Judith and Grünwald, Peter D.},
  options = {useprefix=true},
  date = {2019-05-31},
  eprint = {1905.13494},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  url = {http://arxiv.org/abs/1905.13494},
  urldate = {2019-07-13},
  abstract = {Studies accumulate over time and meta-analyses are mainly retrospective. These two characteristics introduce dependencies between the analysis time, at which a series of studies is up for meta-analysis, and results within the series. Dependencies introduce bias --- Accumulation Bias --- and invalidate the sampling distribution assumed for p-value tests, thus inflating type-I errors. But dependencies are also inevitable, since for science to accumulate efficiently, new research needs to be informed by past results. Here, we investigate various ways in which time influences error control in meta-analysis testing. We introduce an Accumulation Bias Framework that allows us to model a wide variety of practically occurring dependencies, including study series accumulation, meta-analysis timing, and approaches to multiple testing in living systematic reviews. The strength of this framework is that it shows how all dependencies affect p-value-based tests in a similar manner. This leads to two main conclusions. First, Accumulation Bias is inevitable, and even if it can be approximated and accounted for, no valid p-value tests can be constructed. Second, tests based on likelihood ratios withstand Accumulation Bias: they provide bounds on error probabilities that remain valid despite the bias. We leave the reader with a choice between two proposals to consider time in error control: either treat individual (primary) studies and meta-analyses as two separate worlds --- each with their own timing --- or integrate individual studies in the meta-analysis world. Taking up likelihood ratios in either approach allows for valid tests that relate well to the accumulating nature of scientific knowledge. Likelihood ratios can be interpreted as betting profits, earned in previous studies and invested in new ones, while the meta-analyst is allowed to cash out at any time and advise against future studies.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology}
}

@book{thompson_sampling_2012,
  title = {Sampling},
  author = {Thompson, Steven K.},
  date = {2012},
  series = {Wiley Series in Probability and Statistics},
  edition = {3rd ed},
  publisher = {{Wiley}},
  location = {{Hoboken, N.J}},
  abstract = {"The Third Edition retains the general organization of the prior two editions, but it incorporates new material throughout the text. The book is organized into six parts: Part I covers basic sampling from simple random sampling to unequal probability sampling; Part II treats the use of auxiliary data with ratio and regression estimation and looks at the ideas of sufficient data, model, and design in practical sampling; Part III covers major useful designs such as stratified, cluster and systematic, multistage, and double and network sampling; Part IV examines detectability methods for elusive populations, and basic problems in detectability, visibility, and catchability are discussed; Part V concerns spatial sampling with the prediction methods of geostatistics, considerations of efficient spatial designs, and comparisons of different observational methods including plot shapes and detection aspects; and Part VI introduces adaptive sampling designs in which the sampling procedure depends on what is observed during the survey. For this new edition, the author has focused on thoroughly updating the book with a special emphasis on the first 14 chapters since these topics are invariably covered in basic sampling courses. The author has also implemented new approaches to explain the various techniques in the book, and as a result, new examples and explanations have been added throughout. In an effort to improve the presentation and visualization of the book, new figures as well as replacement figures for previously existing figures have been added. This book has continuously stood out from other sampling texts since the figures evoke the idea of each sampling design. The new figures will help readers to better visualize and understand the underlying concepts such as the different sampling strategies"--},
  isbn = {978-0-470-40231-3},
  langid = {english},
  pagetotal = {436},
  keywords = {Sampling (Statistics)},
  annotation = {OCLC: ocn746489136}
}

@article{tran_predicting_2017,
  title = {Predicting Data Saturation in Qualitative Surveys with Mathematical Models from Ecological Research},
  author = {Tran, Viet-Thi and Porcher, Raphael and Tran, Viet-Chi and Ravaud, Philippe},
  date = {2017-02-01},
  journaltitle = {Journal of Clinical Epidemiology},
  shortjournal = {Journal of Clinical Epidemiology},
  volume = {82},
  eprint = {27789316},
  eprinttype = {pmid},
  pages = {71-78.e2},
  publisher = {{Elsevier}},
  issn = {0895-4356, 1878-5921},
  doi = {10.1016/j.jclinepi.2016.10.001},
  url = {https://www.jclinepi.com/article/S0895-4356(16)30543-1/abstract},
  urldate = {2020-08-26},
  abstract = {{$<$}h2{$>$}Abstract{$<$}/h2{$><$}h3{$>$}Objective{$<$}/h3{$><$}p{$>$}Sample size in surveys with open-ended questions relies on the principle of data saturation. Determining the point of data saturation is complex because researchers have information on only what they have found. The decision to stop data collection is solely dictated by the judgment and experience of researchers. In this article, we present how mathematical modeling may be used to describe and extrapolate the accumulation of themes during a study to help researchers determine the point of data saturation.{$<$}/p{$><$}h3{$>$}Study Design and Setting{$<$}/h3{$><$}p{$>$}The model considers a latent distribution of the probability of elicitation of all themes and infers the accumulation of themes as arising from a mixture of zero-truncated binomial distributions. We illustrate how the model could be used with data from a survey with open-ended questions on the burden of treatment involving 1,053 participants from 34 different countries and with various conditions. The performance of the model in predicting the number of themes to be found with the inclusion of new participants was investigated by Monte Carlo simulations. Then, we tested how the slope of the expected theme accumulation curve could be used as a stopping criterion for data collection in surveys with open-ended questions.{$<$}/p{$><$}h3{$>$}Results{$<$}/h3{$><$}p{$>$}By doubling the sample size after the inclusion of initial samples of 25 to 200 participants, the model reliably predicted the number of themes to be found. Mean estimation error ranged from 3\% to 1\% with simulated data and was {$<$}2\% with data from the study of the burden of treatment. Sequentially calculating the slope of the expected theme accumulation curve for every five new participants included was a feasible approach to balance the benefits of including these new participants in the study. In our simulations, a stopping criterion based on a value of 0.05 for this slope allowed for identifying 97.5\% of the themes while limiting the inclusion of participants eliciting nothing new in the study.{$<$}/p{$><$}h3{$>$}Conclusion{$<$}/h3{$><$}p{$>$}Mathematical models adapted from ecological research can accurately predict the point of data saturation in surveys with open-ended questions.{$<$}/p{$>$}},
  langid = {english}
}

@book{tukey_exploratory_1977,
  title = {Exploratory {{Data Analysis}}},
  author = {Tukey, John W.},
  date = {1977},
  edition = {1 edition},
  publisher = {{Pearson}},
  location = {{Reading, Mass}},
  abstract = {The approach in this introductory book is that of informal study of the data. Methods range from plotting picture-drawing techniques to rather elaborate numerical summaries. Several of the methods are the original creations of the author, and all can be carried out either with pencil or aided by hand-held calculator.},
  isbn = {978-0-201-07616-5},
  langid = {english},
  pagetotal = {712}
}

@article{tukey_future_1962,
  ids = {tukey_future_1962-1},
  title = {The {{Future}} of {{Data Analysis}}},
  author = {Tukey, John W.},
  date = {1962-03},
  journaltitle = {The Annals of Mathematical Statistics},
  shortjournal = {Ann. Math. Statist.},
  volume = {33},
  number = {1},
  pages = {1--67},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177704711},
  url = {https://projecteuclid.org/euclid.aoms/1177704711},
  urldate = {2019-10-02},
  abstract = {Project Euclid - mathematics and statistics online},
  langid = {english},
  mrnumber = {MR133937},
  zmnumber = {0107.36401}
}

@article{tversky_features_1977,
  title = {Features of Similarity},
  author = {Tversky, Amos},
  date = {1977},
  journaltitle = {Psychological review},
  volume = {84},
  number = {4},
  pages = {327--352},
  doi = {10.1037/0033-295X.84.4.327},
  url = {http://psycnet.apa.org/journals/rev/84/4/327/},
  urldate = {2017-06-18}
}

@article{valentine_how_2010,
  title = {How {{Many Studies Do You Need}}?: {{A Primer}} on {{Statistical Power}} for {{Meta-Analysis}}},
  shorttitle = {How {{Many Studies Do You Need}}?},
  author = {Valentine, Jeffrey C. and Pigott, Therese D. and Rothstein, Hannah R.},
  date = {2010-04-01},
  journaltitle = {Journal of Educational and Behavioral Statistics},
  shortjournal = {Journal of Educational and Behavioral Statistics},
  volume = {35},
  number = {2},
  pages = {215--247},
  publisher = {{American Educational Research Association}},
  issn = {1076-9986},
  doi = {10.3102/1076998609346961},
  url = {https://doi.org/10.3102/1076998609346961},
  urldate = {2020-12-28},
  abstract = {In this article, the authors outline methods for using fixed and random effects power analysis in the context of meta-analysis. Like statistical power analysis for primary studies, power analysis for meta-analysis can be done either prospectively or retrospectively and requires assumptions about parameters that are unknown. The authors provide some suggestions for thinking about these parameters, in particular for the random effects variance component. The authors also show how the typically uninformative retrospective power analysis can be made more informative. The authors then discuss the value of confidence intervals, show how they could be used in addition to or instead of retrospective power analysis, and also demonstrate that confidence intervals can convey information more effectively in some situations than power analyses alone. Finally, the authors take up the question “How many studies do you need to do a meta-analysis?” and show that, given the need for a conclusion, the answer is “two studies,” because all other synthesis techniques are less transparent and/or are less likely to be valid. For systematic reviewers who choose not to conduct a quantitative synthesis, the authors provide suggestions for both highlighting the current limitations in the research base and for displaying the characteristics and results of studies that were found to meet inclusion criteria.},
  langid = {english},
  keywords = {meta-analysis,research methodology,statistics}
}

@article{van_de_schoot_analyzing_2015,
  title = {Analyzing Small Data Sets Using {{Bayesian}} Estimation: The Case of Posttraumatic Stress Symptoms Following Mechanical Ventilation in Burn Survivors},
  shorttitle = {Analyzing Small Data Sets Using {{Bayesian}} Estimation},
  author = {van de Schoot, Rens and Broere, Joris J. and Perryck, Koen H. and Zondervan-Zwijnenburg, Mariëlle and van Loey, Nancy E.},
  options = {useprefix=true},
  date = {2015-03-11},
  journaltitle = {European Journal of Psychotraumatology},
  shortjournal = {Eur J Psychotraumatol},
  volume = {6},
  eprint = {25765534},
  eprinttype = {pmid},
  issn = {2000-8198},
  doi = {10.3402/ejpt.v6.25216},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4357639/},
  urldate = {2019-08-03},
  abstract = {Background The analysis of small data sets in longitudinal studies can lead to power issues and often suffers from biased parameter values. These issues can be solved by using Bayesian estimation in conjunction with informative prior distributions. By means of a simulation study and an empirical example concerning posttraumatic stress symptoms (PTSS) following mechanical ventilation in burn survivors, we demonstrate the advantages and potential pitfalls of using Bayesian estimation. Methods First, we show how to specify prior distributions and by means of a sensitivity analysis we demonstrate how to check the exact influence of the prior (mis-) specification. Thereafter, we show by means of a simulation the situations in which the Bayesian approach outperforms the default, maximum likelihood and approach. Finally, we re-analyze empirical data on burn survivors which provided preliminary evidence of an aversive influence of a period of mechanical ventilation on the course of PTSS following burns. Results Not suprisingly, maximum likelihood estimation showed insufficient coverage as well as power with very small samples. Only when Bayesian analysis, in conjunction with informative priors, was used power increased to acceptable levels. As expected, we showed that the smaller the sample size the more the results rely on the prior specification. Conclusion We show that two issues often encountered during analysis of small samples, power and biased parameters, can be solved by including prior information into Bayesian analysis. We argue that the use of informative priors should always be reported together with a sensitivity analysis.},
  pmcid = {PMC4357639}
}

@book{van_fraassen_scientific_1980,
  title = {The Scientific Image},
  author = {Van Fraassen, Bas C.},
  date = {1980},
  series = {Clarendon Library of Logic and Philosophy},
  publisher = {{Clarendon Press ; Oxford University Press}},
  location = {{Oxford : New York}},
  isbn = {978-0-19-824424-0 978-0-19-824427-1},
  pagetotal = {235},
  keywords = {Philosophy,Science}
}

@article{van_t_veer_pre-registration_2016,
  title = {Pre-Registration in Social Psychology—{{A}} Discussion and Suggested Template},
  author = {van 't Veer, Anna Elisabeth and Giner-Sorolla, Roger},
  options = {useprefix=true},
  date = {2016-11-01},
  journaltitle = {Journal of Experimental Social Psychology},
  shortjournal = {Journal of Experimental Social Psychology},
  series = {Special {{Issue}}: {{Confirmatory}}},
  volume = {67},
  pages = {2--12},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2016.03.004},
  url = {http://www.sciencedirect.com/science/article/pii/S0022103116301925},
  urldate = {2018-09-26},
  abstract = {Pre-registration of studies before they are conducted has recently become more feasible for researchers, and is encouraged by an increasing number of journals. However, because the practice of pre-registration is relatively new to psychological science, specific guidelines for the content of registrations are still in a formative stage. After giving a brief history of pre-registration in medical and psychological research, we outline two different models that can be applied—reviewed and unreviewed pre-registration—and discuss the advantages of each model to science as a whole and to the individual scientist, as well as some of their drawbacks and limitations. Finally, we present and justify a proposed standard template that can facilitate pre-registration. Researchers can use the template before and during the editorial process to meet article requirements and enhance the robustness of their scholarly efforts.},
  keywords = {Pre-registration,Research methods,Reviewed pre-registration (RPR),Solid science,Unreviewed pre-registration (UPR)},
  annotation = {00000}
}

@article{vazire_quality_2017,
  title = {Quality {{Uncertainty Erodes Trust}} in {{Science}}},
  author = {Vazire, Simine},
  date = {2017-02-28},
  journaltitle = {Collabra: Psychology},
  volume = {3},
  number = {1},
  pages = {1},
  issn = {2474-7394},
  doi = {10.1525/collabra.74},
  url = {http://www.collabra.org/articles/10.1525/collabra.74/},
  urldate = {2019-12-14},
  abstract = {Article: Quality Uncertainty Erodes Trust in Science},
  langid = {english}
}

@article{viechtbauer_conducting_2010,
  title = {Conducting Meta-Analyses in {{R}} with the Metafor Package},
  author = {Viechtbauer, Wolfgang},
  date = {2010},
  journaltitle = {J Stat Softw},
  volume = {36},
  number = {3},
  pages = {1--48},
  doi = {http://dx.doi.org/10.18637/jss.v036.i03},
  url = {http://brieger.esalq.usp.br/CRAN/web/packages/metafor/vignettes/metafor.pdf},
  urldate = {2016-08-07}
}

@article{viechtbauer_simple_2015,
  title = {A Simple Formula for the Calculation of Sample Size in Pilot Studies},
  author = {Viechtbauer, Wolfgang and Smits, Luc and Kotz, Daniel and Budé, Luc and Spigt, Mark and Serroyen, Jan and Crutzen, Rik},
  date = {2015-11},
  journaltitle = {Journal of Clinical Epidemiology},
  shortjournal = {J Clin Epidemiol},
  volume = {68},
  number = {11},
  eprint = {26146089},
  eprinttype = {pmid},
  pages = {1375--1379},
  issn = {1878-5921},
  doi = {10.1016/j.jclinepi.2015.04.014},
  abstract = {One of the goals of a pilot study is to identify unforeseen problems, such as ambiguous inclusion or exclusion criteria or misinterpretations of questionnaire items. Although sample size calculation methods for pilot studies have been proposed, none of them are directed at the goal of problem detection. In this article, we present a simple formula to calculate the sample size needed to be able to identify, with a chosen level of confidence, problems that may arise with a given probability. If a problem exists with 5\% probability in a potential study participant, the problem will almost certainly be identified (with 95\% confidence) in a pilot study including 59 participants.},
  langid = {english},
  keywords = {Humans,Mathematical Concepts,Pilot Projects,Pilot study,Problem detection,Rule of three,Sample size,Sample Size,Unforeseen problems}
}

@article{wacholder_assessing_2004,
  title = {Assessing the {{Probability That}} a {{Positive Report}} Is {{False}}: {{An Approach}} for {{Molecular Epidemiology Studies}}},
  shorttitle = {Assessing the {{Probability That}} a {{Positive Report}} Is {{False}}},
  author = {Wacholder, S. and Chanock, S. and Garcia-Closas, M. and El ghormli, L. and Rothman, N.},
  date = {2004-03-17},
  journaltitle = {JNCI Journal of the National Cancer Institute},
  volume = {96},
  number = {6},
  pages = {434--442},
  issn = {0027-8874, 1460-2105},
  doi = {10.1093/jnci/djh075},
  url = {http://jnci.oxfordjournals.org/cgi/doi/10.1093/jnci/djh075},
  urldate = {2015-11-30},
  langid = {english}
}

@article{wald_sequential_1945,
  title = {Sequential Tests of Statistical Hypotheses},
  author = {Wald, Abraham},
  date = {1945},
  journaltitle = {The Annals of Mathematical Statistics},
  volume = {16},
  number = {2},
  eprint = {2235829},
  eprinttype = {jstor},
  pages = {117--186},
  doi = {https://www.jstor.org/stable/2240273}
}

@article{wasserstein_moving_2019,
  title = {Moving to a {{World Beyond}} “p {$<$} 0.05”},
  author = {Wasserstein, Ronald L. and Schirm, Allen L. and Lazar, Nicole A.},
  date = {2019-03-29},
  journaltitle = {The American Statistician},
  volume = {73},
  pages = {1--19},
  issn = {0003-1305},
  doi = {10.1080/00031305.2019.1583913},
  url = {https://doi.org/10.1080/00031305.2019.1583913},
  urldate = {2019-08-26},
  issue = {sup1}
}

@book{wassmer_group_2016,
  title = {Group {{Sequential}} and {{Confirmatory Adaptive Designs}} in {{Clinical Trials}}},
  author = {Wassmer, Gernot and Brannath, Werner},
  date = {2016},
  series = {Springer {{Series}} in {{Pharmaceutical Statistics}}},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-32562-0},
  url = {http://link.springer.com/10.1007/978-3-319-32562-0},
  urldate = {2020-03-16},
  isbn = {978-3-319-32560-6 978-3-319-32562-0},
  langid = {english}
}

@software{wassmer_rpact_2019,
  title = {Rpact: {{Confirmatory}} Adaptive Clinical Trial Design and Analysis},
  author = {Wassmer, Gernot and Pahlke, Friedrich},
  date = {2019},
  url = {Retrieved from https://CRAN.R-project.org/package=rpact},
  version = {2.06}
}

@article{westberg_combining_1985,
  title = {Combining {{Independent Statistical Tests}}},
  author = {Westberg, Margareta},
  date = {1985},
  journaltitle = {Journal of the Royal Statistical Society. Series D (The Statistician)},
  volume = {34},
  number = {3},
  eprint = {2987655},
  eprinttype = {jstor},
  pages = {287--296},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0039-0526},
  doi = {10.2307/2987655},
  abstract = {In the present study two well-known combination methods, Fisher's and Tippett's, are compared according to their power. The calculations are made for normally and chi-square distributed test statistics. None of the two procedures is uniformly better than the other according to the power but sometimes the power curves cross each other. The calculated power-graphs give guidelines for when to use Fisher's method and when to use Tippett's.}
}

@article{westfall_statistical_2014,
  title = {Statistical Power and Optimal Design in Experiments in Which Samples of Participants Respond to Samples of Stimuli},
  author = {Westfall, Jacob and Kenny, David A. and Judd, Charles M.},
  date = {2014},
  journaltitle = {Journal of Experimental Psychology: General},
  volume = {143},
  number = {5},
  pages = {2020--2045},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1939-2222(Electronic),0096-3445(Print)},
  doi = {10.1037/xge0000014},
  abstract = {Researchers designing experiments in which a sample of participants responds to a sample of stimuli are faced with difficult questions about optimal study design. The conventional procedures of statistical power analysis fail to provide appropriate answers to these questions because they are based on statistical models in which stimuli are not assumed to be a source of random variation in the data, models that are inappropriate for experiments involving crossed random factors of participants and stimuli. In this article, we present new methods of power analysis for designs with crossed random factors, and we give detailed, practical guidance to psychology researchers planning experiments in which a sample of participants responds to a sample of stimuli. We extensively examine 5 commonly used experimental designs, describe how to estimate statistical power in each, and provide power analysis results based on a reasonable set of default parameter values. We then develop general conclusions and formulate rules of thumb concerning the optimal design of experiments in which a sample of participants responds to a sample of stimuli. We show that in crossed designs, statistical power typically does not approach unity as the number of participants goes to infinity but instead approaches a maximum attainable power value that is possibly small, depending on the stimulus sample. We also consider the statistical merits of designs involving multiple stimulus blocks. Finally, we provide a simple and flexible Web-based power application to aid researchers in planning studies with samples of stimuli. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Experimental Design,Sampling (Experimental),Statistical Power,Stimulus Parameters}
}

@article{westfall_statistical_2014-1,
  title = {Statistical Power and Optimal Design in Experiments in Which Samples of Participants Respond to Samples of Stimuli.},
  author = {Westfall, Jacob and Kenny, David A. and Judd, Charles M.},
  date = {2014},
  journaltitle = {Journal of Experimental Psychology: General},
  volume = {143},
  number = {5},
  pages = {2020}
}

@article{wicherts_degrees_2016,
  title = {Degrees of {{Freedom}} in {{Planning}}, {{Running}}, {{Analyzing}}, and {{Reporting Psychological Studies}}: {{A Checklist}} to {{Avoid}} p-{{Hacking}}},
  shorttitle = {Degrees of {{Freedom}} in {{Planning}}, {{Running}}, {{Analyzing}}, and {{Reporting Psychological Studies}}},
  author = {Wicherts, Jelte M. and Veldkamp, Coosje L. S. and Augusteijn, Hilde E. M. and Bakker, Marjan and Aert, Van and M, Robbie C. and Assen, Van and M, Marcel A. L.},
  date = {2016},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {7},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2016.01832},
  url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2016.01832/abstract},
  urldate = {2017-03-21},
  abstract = {The designing, collecting, analyzing, and reporting of psychological studies entail many choices that are often arbitrary. The opportunistic use of these so-called researcher degrees of freedom aimed at obtaining statistically significant results is problematic because it enhances the chances of false positive results and may inflate effect size estimates. In this review article, we present an extensive list of 34 degrees of freedom that researchers have in formulating hypotheses, and in designing, running, analyzing, and reporting of psychological research. The list can be used in research methods education, and as a checklist to assess the quality of preregistrations and to determine the potential for bias due to (arbitrary) choices in unregistered studies.},
  langid = {english},
  keywords = {BIAS,Experimental design (study designs),p-hacking,questionable research practices,Research methods education,significance chasing,significance testing}
}

@article{wigboldus_encourage_2016,
  title = {Encourage {{Playing}} with {{Data}} and {{Discourage Questionable Reporting Practices}}},
  author = {Wigboldus, Daniel H. J. and Dotsch, Ron},
  date = {2016-03-01},
  journaltitle = {Psychometrika},
  shortjournal = {Psychometrika},
  volume = {81},
  number = {1},
  pages = {27--32},
  issn = {1860-0980},
  doi = {10.1007/s11336-015-9445-1},
  url = {https://doi.org/10.1007/s11336-015-9445-1},
  urldate = {2019-08-03},
  langid = {english},
  keywords = {Confirmatory Analysis,Data Analysis Phase,Data Analysis Plan,Reporting Practice,Research Practice}
}

@article{williams_impact_1995,
  title = {Impact of {{Measurement Error}} on {{Statistical Power}}: {{Review}} of an {{Old Paradox}}},
  shorttitle = {Impact of {{Measurement Error}} on {{Statistical Power}}},
  author = {Williams, Richard H. and Zimmerman, Donald W. and Zumbo, Bruno D.},
  date = {1995-07-01},
  journaltitle = {The Journal of Experimental Education},
  volume = {63},
  number = {4},
  pages = {363--370},
  publisher = {{Routledge}},
  issn = {0022-0973},
  doi = {10.1080/00220973.1995.9943470},
  url = {https://doi.org/10.1080/00220973.1995.9943470},
  urldate = {2020-12-28},
  abstract = {The relation between test reliability and statistical power has been a controversial issue, perhaps due in part to a 1975 publication in the Psychological Bulletin by Overall and Woodward, “Unreliability of Difference Scores: A Paradox for the Measurement of Change”, in which they demonstrated that a Student t test based on pretest-posttest differences can attain its greatest power when the difference score reliability is zero. In the present article, the authors attempt to explain this paradox by demonstrating in several ways that power is not a mathematical function of reliability unless either true score variance or error score variance is constant.},
  annotation = {\_eprint: https://doi.org/10.1080/00220973.1995.9943470}
}

@article{wilson_practical_2015,
  title = {A {{Practical Guide}} to {{Value}} of {{Information Analysis}}},
  author = {Wilson, Edward C. F.},
  date = {2015-02-01},
  journaltitle = {PharmacoEconomics},
  shortjournal = {PharmacoEconomics},
  volume = {33},
  number = {2},
  pages = {105--121},
  issn = {1179-2027},
  doi = {10.1007/s40273-014-0219-x},
  url = {https://doi.org/10.1007/s40273-014-0219-x},
  urldate = {2020-11-10},
  abstract = {Value of information analysis is a quantitative method to estimate the return on investment in proposed research projects. It can be used in a number of ways. Funders of research may find it useful to rank projects in terms of the expected return on investment from a variety of competing projects. Alternatively, trialists can use the principles to identify the efficient sample size of a proposed study as an alternative to traditional power calculations, and finally, a value of information analysis can be conducted alongside an economic evaluation as a quantitative adjunct to the ‘future research’ or ‘next steps’ section of a study write up. The purpose of this paper is to present a brief introduction to the methods, a step-by-step guide to calculation and a discussion of issues that arise in their application to healthcare decision making. Worked examples are provided in the accompanying online appendices as Microsoft Excel spreadsheets.},
  langid = {english}
}

@article{wilson_vanvoorhis_understanding_2007,
  title = {Understanding Power and Rules of Thumb for Determining Sample Sizes},
  author = {Wilson VanVoorhis, Carmen R. and Morgan, Betsy L.},
  date = {2007},
  journaltitle = {Tutorials in quantitative methods for psychology},
  volume = {3},
  number = {2},
  pages = {43--50},
  doi = {10.20982/tqmp.03.2.p043}
}

@book{winer_statistical_1962,
  title = {Statistical Principles in Experimental Design},
  author = {Winer, B. J},
  date = {1962},
  publisher = {{New York : McGraw-Hill}},
  url = {https://trove.nla.gov.au/version/39914160},
  urldate = {2019-06-03},
  abstract = {"Written primarily for students and research workers in the area of the behavioral sciences, this book is meant to provide a text and comprehensive reference source on statistical principles underlying experimental design. Particular emphasis is given to those designs that are likely to prove useful in research in the behavioral sciences. The book primarily emphasizes the logical basis of principles underlying designs for experiments rather than mathematical derivations associated with relevant sampling distributions. The topics selected for inclusion are those covered in courses taught by the author during the past several years. Students in these courses have widely varying backgrounds in mathematics and come primarily from the fields of psychology, education, economics, sociology, and industrial engineering. It has been the intention of the author to keep the book at a readability level appropriate for students having a mathematical background equivalent to freshman college algebra. From experience with those sections of the book which have been used as text material in dittoed form, there is evidence to indicate that, in large measure, the desired readability level has been attained. Admittedly, however, there are some sections in the book where this readability goal has not been achieved. The first course in design, as taught by the author, has as a prerequisite a basic course in statistical inference. The contents of Chaps. 1 and 2 review the highlights of what is included in the prerequisite material. These chapters are not meant to provide the reader with a first exposure to these topics. They are intended to provide a review of terminology and notation for the concepts which are more fully developed in later chapters. By no means is all the material included in the book covered in a one semester course. In a course of this length, the author has included Chaps. 3, 4, parts of 5, 6, parts of 7, parts of 10, and parts of 11. Chapters 8 through 11 were written to be somewhat independent of each other. Hence one may read, with understanding, in these chapters without undue reference to material in the others. In general, the discussion of principles, interpretations of illustrative examples, and computational procedures are included in successive sections within the same chapter. However, to facilitate the use of the book as a reference source, this procedure is not followed in Chaps. 5 and 6. Basic principles associated with a large class of designs for factorial experiments are discussed in Chap. 5. Detailed illustrative examples of these designs are presented in Chap. 6. For teaching purposes, the author includes relevant material from Chap. 6 with the corresponding material in Chap. 5. Selected topics from Chaps. 7 through 11 have formed the basis for a second course in experimental design. Relatively complete tables for sampling distributions of statistics used in the analysis of experimental designs are included in the Appendix. Ample references to source materials having mathematical proofs for the principles stated in the text are provided"--Preface. (PsycINFO Database Record (c) 2008 APA, all rights reserved).  x, 672 p. : ill. ; 24 cm. Social sciences -- Statistical methods. Psychology -- Statistical methods. Research. Plan d'expérience. Experimentelle Psychologie. Statistik. Versuchsplanung. Psychology -- Research. Social sciences -- Research. Experimenteel ontwerp. Variantieanalyse. Experimental design. Statistics as Topic.},
  langid = {english}
}

@article{wittes_role_1990,
  title = {The Role of Internal Pilot Studies in Increasing the Efficiency of Clinical Trials},
  author = {Wittes, Janet and Brittain, Erica},
  date = {1990},
  journaltitle = {Statistics in Medicine},
  volume = {9},
  number = {1-2},
  pages = {65--72},
  issn = {1097-0258},
  doi = {10.1002/sim.4780090113},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4780090113},
  urldate = {2020-12-31},
  abstract = {Investigators often design clinical trials without knowing precisely the values of such necessary parameters as the variances or the event rates in the control group. In order to determine reasonable values for such parameters, they may design a small pilot study external to the main trial. In this paper we propose designs, which we term internal pilot studies, that designate a portion of the main trial as a pilot phase. At the end of the internal pilot study, the investigators recompute preselected parameters and recalculate required sample size. The study then proceeds with the modifications dictated by the internal pilot. Final analyses of the results incorporate all data, disregarding the fact that part of the data came from a pilot phase. As one example of this type of design, we consider a study to compare two normally distributed means. By simulation, we show a numerical example for which the effect of the procedure on the α-level is negligible, but the potential gain in power considerable. We urge considering a similar approach for a number of types of endpoints.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.4780090113}
}

@article{yuan_post_2005,
  title = {On the {{Post Hoc Power}} in {{Testing Mean Differences}}},
  author = {Yuan, Ke-Hai and Maxwell, Scott},
  date = {2005-06-01},
  journaltitle = {Journal of Educational and Behavioral Statistics},
  shortjournal = {Journal of Educational and Behavioral Statistics},
  volume = {30},
  number = {2},
  pages = {141--167},
  publisher = {{American Educational Research Association}},
  issn = {1076-9986},
  doi = {10.3102/10769986030002141},
  url = {https://doi.org/10.3102/10769986030002141},
  urldate = {2020-07-14},
  abstract = {Retrospective or post hoc power analysis is recommended by reviewers and editors of many journals. Little literature has been found that gave a serious study of the post hoc power. When the sample size is large, the observed effect size is a good estimator of the true effect size. One would hope that the post hoc power is also a good estimator of the true power. This article studies whether such a power estimator provides valuable information about the true power., Using analytical, numerical, and Monte Carlo approaches, our results show that the estimated power does not provide useful information when the true power is small. It is almost always a biased estimator of the true power. The bias can be negative or positive. Large sample size alone does not guarantee the post hoc power to be a good estimator of the true power. Actually, when the population variance is known, the cumulative distribution function of the post hoc power is solely a function of the population power. This distribution is uniform when the true power equals 0.5 and highly skewed when the true power is near 0 or 1. When the population variance is unknown, the post hoc power behaves essentially the same as when the variance is known.},
  langid = {english}
}

@article{zumbo_note_1998,
  title = {A {{Note}} on {{Misconceptions Concerning Prospective}} and {{Retrospective Power}}},
  author = {Zumbo, Bruno D. and Hubley, Anita M.},
  date = {1998},
  journaltitle = {Journal of the Royal Statistical Society: Series D (The Statistician)},
  volume = {47},
  number = {2},
  pages = {385--388},
  issn = {1467-9884},
  doi = {10.1111/1467-9884.00139},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-9884.00139},
  urldate = {2022-01-21},
  abstract = {This paper addresses some misconceptions concerning prospective and retrospective power when journal editors, reviewers and readers want to know the observed power of statistical tests within a completed study. In addition, a practical solution to both the problem of computing the power after a study has been conducted and the critical evaluation of null findings is suggested},
  langid = {english},
  keywords = {Effect size,Research design,Statistical power,Statistics},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9884.00139}
}


